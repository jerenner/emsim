unet:
  encoder:
    layers: [2, 2, 2, 2]
    channels: [32, 64, 128, 256]
    drop_path_rate: 0.0
  decoder:
    layers: [2, 2, 2, 2]
    channels: [256, 128, 64, 32]
    drop_path_rate: 0.0
  stem_channels: 16
  act_layer: "relu"
  norm_layer: "batchnorm1d"
  convert_sync_batch_norm: true

transformer:
  d_model: 256
  n_heads: 8
  dim_feedforward: 1024
  n_deformable_points: 4
  dropout: 0.1
  activation_fn: "gelu"
  encoder_layers: 6
  decoder_layers: 6
  level_filter_ratio: [0.25, 0.5, 1.0, 1.0]
  layer_filter_ratio: [1.0, 0.8, 0.6, 0.6, 0.4, 0.2]
  max_tokens: 10000
  query_embeddings: 200
  decoder_detach_updated_positions: true
  decoder_look_forward_twice: true

criterion:
  aux_loss:
    use_aux_loss: true
    use_final_matches: false
    aux_loss_weight: 1.0
  no_electron_weight: 1.0
  matcher:
    cost_coef_class: 1.0
    cost_coef_mask: 3.0
    cost_coef_dice: 5.0
    cost_coef_dist: 1.0
    cost_coef_nll: 0.0
    cost_coef_likelihood: 0.0
  loss_coef_class: 10.0
  loss_coef_mask_bce: 3.0
  loss_coef_mask_dice: 5.0
  loss_coef_incidence_nll: 0.1
  loss_coef_incidence_likelihood: 10.0
  loss_coef_incidence_huber: 100.0
  detach_likelihood_mean: false
  salience:
    alpha: 0.25
    gamma: 2.0

dataset:
  directory: "./data"
  # pixels_file: "pixelated_5um_tracks_thinned_4um_back_20k_300keV.txt"
  pixels_file: "pixelated_5um_tracks_thinned_4um_back_1M_300keV.txt"
  num_workers: 6
  train_percentage: 0.95
  events_per_image_range: [100, 200]
  noise_std: 1.0
  pixel_patch_size: 7
  n_sigma_sparsify: 3
  max_pixels_to_keep: 50000

lr: 0.0001
max_grad_norm: 0.1
batch_size: 1
num_steps: 10000
warmup_percentage: 0.1
eval_steps: 1000
seed: 1234
print_interval: 10
resume_file: null
tensorboard_name: null
log_tensorboard: true
clear_cache_interval: 1000
log_level: "INFO"
compile: false

ddp:
  nodes: 2
  devices: 2
  find_unused_parameters: false