{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import training as tr\n",
    "import emnet\n",
    "import emsim_utils\n",
    "\n",
    "import struct\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "#import scipy.optimize as optimize\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/1035340/reading-binary-file-and-looping-over-each-byte\n",
    "def bytes_from_file(filename, chunksize=2):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunksize)\n",
    "            if chunk:\n",
    "                yield struct.unpack('<H', chunk)[0]\n",
    "            else:\n",
    "                break    \n",
    "\n",
    "\n",
    "# Read the specified number of images from the given file.\n",
    "# Each \"image\" is the difference between two 512x512 \"pre\"-images\n",
    "#  read in succession. Each image is median subtracted.\n",
    "def read_images(datfile,nimages):\n",
    "    \n",
    "    # Create a reader.\n",
    "    freader = iter(bytes_from_file(datfile))\n",
    "\n",
    "    # Read 4-byte header.\n",
    "    for i in range(2):\n",
    "        next(freader)\n",
    "\n",
    "    # Read the images.\n",
    "    imgs = []\n",
    "    for ni in range(nimages):\n",
    "\n",
    "        img1 = []\n",
    "        for i in range(512*512):\n",
    "            img1.append(next(freader))\n",
    "\n",
    "        img2 = []\n",
    "        for i in range(512*512):\n",
    "            img2.append(next(freader))\n",
    "\n",
    "        imgs.append(np.array(img2).reshape([512,512]) - np.array(img1).reshape([512,512]))\n",
    "\n",
    "    # Return the final image array in numpy format.\n",
    "    imgs = np.array(imgs)\n",
    "    imgs = imgs - np.median(imgs,axis=0)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "th = 100\n",
    "all_img_proc, all_img_th_proc = [], []\n",
    "datfiles = glob(\"/home/jrenner/local/data/4dstem/scan_0000001768/*.dat\")\n",
    "for datfile in datfiles:\n",
    "    print(\"Reading datafile\",datfile)\n",
    "    \n",
    "    # Read in all images from the data file.\n",
    "    imgs = read_images(datfile,2048)\n",
    "    \n",
    "    # Average over all images to get the \"processed\" image.\n",
    "    img_proc = np.average(imgs,axis=0)\n",
    "    all_img_proc.append(img_proc)\n",
    "    \n",
    "    # Threshold the images to get individual counts.\n",
    "    imgs_th = np.zeros(imgs.shape)\n",
    "    imgs_th[imgs > th] = 1\n",
    "    \n",
    "    # Sum the thresholded images.\n",
    "    img_th_proc = np.sum(imgs_th,axis=0)\n",
    "    all_img_th_proc.append(img_th_proc)\n",
    "\n",
    "all_img_proc = np.array(all_img_proc)\n",
    "all_img_th_proc = np.array(all_img_th_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"aall_img_arrays.npz\",all_img_proc=all_img_proc,all_img_th_proc=all_img_th_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the arrays from the file\n",
    "f_img_arrays = np.load(\"img_arrays_0_left.npz\")\n",
    "all_img_proc = f_img_arrays['all_img_proc']\n",
    "all_img_th_proc = f_img_arrays['all_img_th_proc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(8.0)\n",
    "fig.set_figwidth(20.0)\n",
    "for iplt in range(len(all_img_th_proc)):\n",
    "    iaxis = int(iplt / 8)\n",
    "    jaxis = iplt % 8\n",
    "    axs[iaxis,jaxis].imshow(all_img_th_proc[iplt],vmax=100)\n",
    "\n",
    "# plt.imshow(all_img_proc[11])\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the summed, thresholded image from all datasets.\n",
    "img_th_all = np.sum(all_img_th_proc,axis=0)\n",
    "plt.imshow(img_th_all,vmax=1000)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the numbers of counts in each pixel.\n",
    "plt.hist(all_img_th_proc.flatten(),bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram individual image elements to determine threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a histogram of all image elements.\n",
    "plt.hist(imgs.flatten(),bins=100,range=[0,400])\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a thresholded image.\n",
    "th = 100\n",
    "imgs_th = np.zeros(imgs.shape)\n",
    "imgs_th[imgs > th] = 1#imgs[imgs > th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the thresholded image.\n",
    "img_th_proc = np.sum(imgs_th,axis=0)\n",
    "plt.imshow(img_th_proc,vmax=10)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the edge of the thresholded image for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0))) + b\n",
    "    return (y)\n",
    "\n",
    "# Fit an s-curve to the specified row in img,\n",
    "#  within a range of +/- rng centered upon x0.\n",
    "def fit_scurve(img,nrow,x0,rng=50):\n",
    "    \n",
    "    # Choose the row, selecting elements (x0-rng):(x0+rng)\n",
    "    xmin = max(0,x0-rng)\n",
    "    xmax = min(img.shape[1],x0+rng)\n",
    "    ydata = img[nrow,xmin:xmax]\n",
    "    xdata = np.arange(xmin,xmax)\n",
    "\n",
    "    # Set an initial guess and perform the fit.\n",
    "    p0 = [np.mean(ydata[ydata > np.mean(ydata)]), x0, 1, min(ydata)]\n",
    "    popt, pcov = curve_fit(sigmoid, xdata, ydata, p0, method='dogbox') #, bounds=((-max(ydata), x0-20, -np.inf, 0), (max(ydata), x0+20, np.inf, 200)))\n",
    "    \n",
    "    return popt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_guess = 357\n",
    "x_edges = []\n",
    "for row in range(img_th_all.shape[0]):\n",
    "    \n",
    "    # Perform the fit.\n",
    "    try:\n",
    "        L,x0,k,b = fit_scurve(img_th_all,row,x_guess)\n",
    "    except:\n",
    "        print(\"** Fit failed for row\",row)\n",
    "        L,k,b = None,None,None\n",
    "        x0 = x_guess\n",
    "    \n",
    "    x_edge = int(np.round(x0))\n",
    "    x_edges.append(x_edge)\n",
    "    x_guess = x_edge\n",
    "    print(\"-- Fit row\",row,\": L=\",L,\"x0=\",x0,\"k=\",k,\"b=\",b)\n",
    "x_edges = np.array(x_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_edge = np.zeros([512,512]) #img_th_all.shape)\n",
    "for row,iedge in zip(range(img_edge.shape[0]),x_edges):\n",
    "    img_edge[row,iedge] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.imshow(img_th_all,vmax=1000)\n",
    "plt.colorbar()\n",
    "#plt.imshow(img_edge,alpha=0.1)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.imshow(img_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test a single fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "x_guess = 360\n",
    "rng = 50\n",
    "\n",
    "popt = fit_scurve(img_th_all,row,x_guess)\n",
    "\n",
    "xmin = max(0,x_guess-rng)\n",
    "xmax = min(img_th_all.shape[1],x_guess+rng)\n",
    "ydata = img_th_all[row,xmin:xmax]\n",
    "xdata = np.arange(xmin,xmax)\n",
    "\n",
    "x = np.linspace(xdata[0], xdata[-1], 1000)\n",
    "y = sigmoid(x, *popt)\n",
    "\n",
    "plt.plot(xdata, ydata, label='data')\n",
    "plt.plot(x,y, label='fit')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Image column\")\n",
    "plt.ylabel(\"Counts\")\n",
    "\n",
    "print(\"-- Fit row\",row,\": popt =\",popt)\n",
    "print(np.mean(ydata[ydata > np.mean(ydata)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all arrays.\n",
    "np.savez(\"img_arrays_0_right.npz\",all_img_proc=all_img_proc,all_img_th_proc=all_img_th_proc,x_edges=x_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select the images using the edge array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_img_arrays = np.load(\"img_arrays_0_left.npz\")\n",
    "x_edges = f_img_arrays['x_edges']\n",
    "side = 1 # 0 = right, 1 = left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines whether the image is a valid event.\n",
    "def check_subimage_th(subimg):\n",
    "    valid = False\n",
    "    \n",
    "    # Ensure the maximum pixel index is within some number of pixels of the borders.\n",
    "    pixel_padding = 1\n",
    "    max_index = np.unravel_index(subimg.argmax(),subimg.shape)\n",
    "    valid = (max_index[0] > pixel_padding) and (max_index[0] < subimg.shape[0]-pixel_padding-1) and (max_index[1] > pixel_padding) and (max_index[1] < subimg.shape[1]-pixel_padding-1)\n",
    "    \n",
    "    # Ensure there is at least 1 nonzero pixel.\n",
    "    valid = valid and (len(subimg[subimg > 0]) > 0) \n",
    "    \n",
    "    return valid\n",
    "\n",
    "def choose_subimages(img, img_th, available_edges, debug=False):\n",
    "\n",
    "    valid_subimages, valid_subimages_c, valid_subimages_th = [], [], []\n",
    "    line_m, line_b = [], []\n",
    "\n",
    "    # Loop until no available edges remain.\n",
    "    done = (available_edges == -1).all()\n",
    "    while(not done):\n",
    "\n",
    "        # Choose a random location in the x-edges array.\n",
    "        rmid = np.random.randint(rmax)\n",
    "\n",
    "        # Make sure the current edge is available.\n",
    "        if(available_edges[rmid] < 0):\n",
    "            continue\n",
    "\n",
    "        # Select a window equal to the subimage size in length.\n",
    "        rstart = rmid - subimg_halfsize\n",
    "        rend   = rmid + subimg_halfsize + 1\n",
    "        if(rstart < rmin or rend >= rmax):\n",
    "            if(rstart < rmin): rstart = rmin\n",
    "            if(rend >= rmax):  rend = rmax\n",
    "            available_edges[rstart:rend] = -1\n",
    "            \n",
    "            done = (available_edges == -1).all()\n",
    "            continue\n",
    "\n",
    "        # Ensure all edges for this image have not yet been used.\n",
    "        aedges = available_edges[rstart:rend]\n",
    "        if((aedges > 0).all()):\n",
    "\n",
    "            # Construct the subimage.\n",
    "            xmid = x_edges[rmid]\n",
    "            cstart = xmid - subimg_halfsize\n",
    "            cend   = xmid + subimg_halfsize + 1\n",
    "\n",
    "            # Only continue with a subimage that fits properly in the full image.\n",
    "            if(cstart >= cmin and cend < cmax):\n",
    "\n",
    "                # Create the subimage.\n",
    "                subimg    = img[rstart:rend,cstart:cend]\n",
    "                subimg_th = img_th[rstart:rend,cstart:cend]\n",
    "                \n",
    "                # Compute the boundaries for the subimage centered on the max pixel.\n",
    "                arg_max = np.unravel_index(np.argmax(subimg),subimg.shape)\n",
    "                delta = int((subimg.shape[0]-1)/2)  # the extent of the event from the center pixel\n",
    "                ileft = rstart + (arg_max[0] - delta)\n",
    "                jleft = cstart + (arg_max[1] - delta)\n",
    "                iright = rend + (arg_max[0] - delta)\n",
    "                jright = cend + (arg_max[1] - delta)\n",
    "\n",
    "                # Check that the (thresholded) subimage is valid.\n",
    "                if(check_subimage_th(subimg_th)):\n",
    "\n",
    "                    # Compute the slopes and intercepts.\n",
    "                    x1 = (x_edges[rstart] - cstart) + 0.5\n",
    "                    x2 = (x_edges[rend] - cstart) + 0.5\n",
    "                    if(x2 == x1): \n",
    "                        if(side == 0): x2 = x1-1\n",
    "                        else: x2 = x1+1\n",
    "                    y1 = 0.5\n",
    "                    y2 = (rend - rstart) - 0.5\n",
    "                    m = (y2 - y1) / (x2 - x1)\n",
    "                    b = y2 - m*x2\n",
    "                    if(debug): print(\"xmid_rel = {}, cstart = {}, cend = {}\".format(xmid-cstart,cstart,cend))\n",
    "                    if(debug): print(\"(x1,y1) = ({},{}); (x2,y2) = ({},{})\".format(x1,y1,x2,y2))\n",
    "                    \n",
    "                    # Only save the subimage if the slope is negative for side = 0 or positive for side = 1.\n",
    "                    # (Note, because y increases downward, the notion of \"negative\" and \"positive\" is flipped.)\n",
    "                    if(debug): print(\"side = {}, m = {}\".format(side,m))\n",
    "                    if((side == 0 and m < 0) or (side == 1 and m > 0)):\n",
    "\n",
    "                        # Save the subimage if the centered subimage is also valid.\n",
    "                        if(ileft >= rmin and iright < rmax and jleft >= cmin and jright < cmax):\n",
    "                            if(debug): print(\"Centered subimage is valid.\")\n",
    "                            subimg_c = img[ileft:iright,jleft:jright]\n",
    "                            arg_max_c = np.unravel_index(np.argmax(subimg_c),subimg_c.shape)\n",
    "\n",
    "                            # Don't save the image if the center pixel is no longer the maximum.\n",
    "                            if(arg_max_c[0] == delta and arg_max_c[1] == delta):\n",
    "                                if(debug): print(\"Center pixel is no longer maximum: ADDING IMAGE\")\n",
    "                                valid_subimages.append(subimg)\n",
    "                                valid_subimages_c.append(subimg_c)\n",
    "                                valid_subimages_th.append(subimg_th)\n",
    "                                line_m.append(m)\n",
    "                                line_b.append(b)\n",
    "\n",
    "                    # Remove all edges for this subimage from availability.\n",
    "                    available_edges[rstart:rend] = -1\n",
    "\n",
    "                # If the image is not valid, do nothing, but continue the loop.\n",
    "\n",
    "            else:\n",
    "                available_edges[rstart:rend] = -1\n",
    "\n",
    "        # If not all edges are available, \n",
    "        else:\n",
    "            available_edges[rstart:rend] = -1\n",
    "\n",
    "        # Set the terminating condition.\n",
    "        done = (available_edges == -1).all()\n",
    "        #if(debug): print(\"Number of available edges:\",np.sum(available_edges == 1),\"/\",len(available_edges))\n",
    "    \n",
    "    # Return the arrays.\n",
    "    valid_subimages = np.array(valid_subimages)\n",
    "    valid_subimages_c = np.array(valid_subimages_c)\n",
    "    valid_subimages_th = np.array(valid_subimages_th)\n",
    "    line_m = np.array(line_m)\n",
    "    line_b = np.array(line_b)\n",
    "    \n",
    "    return valid_subimages, valid_subimages_c, valid_subimages_th, line_m, line_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "th = 100\n",
    "subimg_halfsize = 5\n",
    "rmin = 0\n",
    "rmax = len(x_edges)\n",
    "cmin = 0\n",
    "cmax = len(x_edges) # assume image is square\n",
    "\n",
    "valid_subimages, valid_subimages_c, valid_subimages_th = None, None, None\n",
    "line_m, line_b = None, None\n",
    "\n",
    "datfiles = glob(\"/home/jrenner/local/data/4dstem/scan_0000001768/*.dat\")\n",
    "for datfile in datfiles[0:10]:\n",
    "    print(\"Reading datafile\",datfile)\n",
    "    \n",
    "    # Read in all images from the data file.\n",
    "    imgs = read_images(datfile,2048)\n",
    "    \n",
    "    # Threshold the images to get individual counts\n",
    "    imgs_th = np.zeros(imgs.shape)\n",
    "    imgs_th[imgs > th] = 1\n",
    "    \n",
    "    # Get all subimages from each image.\n",
    "    for iimg in range(len(imgs)):\n",
    "        if(iimg % 200 == 0): print(\"--> Processing image\",iimg,\"of\",len(imgs))\n",
    "        \n",
    "        # Get this image and its thresholded version.\n",
    "        img = imgs[iimg]\n",
    "        img_th = imgs_th[iimg]\n",
    "        \n",
    "        # Create a new array of available edges.\n",
    "        available_edges = np.ones(len(x_edges))\n",
    "        \n",
    "        # Get the subimages.\n",
    "        t_valid_subimages, t_valid_subimages_c, t_valid_subimages_th, t_line_m, t_line_b = choose_subimages(img, img_th, available_edges, debug = False)\n",
    "        \n",
    "        if(valid_subimages is None): valid_subimages = t_valid_subimages\n",
    "        else: valid_subimages = np.append(valid_subimages, t_valid_subimages, axis=0)\n",
    "        if(valid_subimages_c is None): valid_subimages_c = t_valid_subimages_c\n",
    "        else: valid_subimages_c = np.append(valid_subimages_c, t_valid_subimages_c, axis=0)\n",
    "        if(valid_subimages_th is None): valid_subimages_th = t_valid_subimages_th\n",
    "        else: valid_subimages_th = np.append(valid_subimages_th, t_valid_subimages_th, axis=0)\n",
    "        if(line_m is None): line_m = t_line_m\n",
    "        else: line_m = np.append(line_m, t_line_m, axis=0)\n",
    "        if(line_b is None): line_b = t_line_b\n",
    "        else: line_b = np.append(line_b, t_line_b, axis=0)\n",
    "        \n",
    "    del imgs\n",
    "    del imgs_th\n",
    "    gc.collect()\n",
    "\n",
    "# Convert to numpy arrays.\n",
    "# valid_subimages = np.array(valid_subimages)\n",
    "# valid_subimages_th = np.array(valid_subimages_th)\n",
    "# line_m = np.array(line_m)\n",
    "# line_b = np.array(line_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_subimages_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"subimgs_170k_left.npz\",valid_subimages=valid_subimages,valid_subimages_c=valid_subimages_c,valid_subimages_th=valid_subimages_th,line_m=line_m,line_b=line_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subimages\n",
    "fn = np.load(\"subimgs_173k_right.npz\")\n",
    "valid_subimages = fn['valid_subimages']\n",
    "valid_subimages_c = fn['valid_subimages_c']\n",
    "valid_subimages_th = fn['valid_subimages_th']\n",
    "line_m = fn['line_m']\n",
    "line_b = fn['line_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isub = 15\n",
    "nrows = ncols = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot subimages.\n",
    "mm = line_m[isub]\n",
    "bb = line_b[isub]\n",
    "xfit = np.arange(0,ncols-1,1e-4)\n",
    "yfit = mm*xfit + bb\n",
    "\n",
    "plt.imshow(valid_subimages[isub])\n",
    "plt.plot(xfit[(yfit >= 0) & (yfit < nrows-1)],yfit[(yfit >= 0) & (yfit < nrows-1)],linewidth=2,color='red')\n",
    "print(\"Slope is\",mm,\"and intercept is\",bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(valid_subimages_th[isub])\n",
    "#plt.imshow(np.sum(valid_subimages_th,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_subimages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/jrenner/local/jerenner/emsim/models'\n",
    "lrate       = 1e-3   # Learning rate to use in the training.\n",
    "load_model  = True   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 200    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/realdata/model_150k_symmetrized.pt\".format(modeldir)\n",
    "\n",
    "# Create the dataset.\n",
    "dataset_train = tr.RealFrameDataset(\"/home/jrenner/local/jerenner/emsim/notebooks/subimgs_170k_left.npz\",\n",
    "                                    \"/home/jrenner/local/jerenner/emsim/notebooks/subimgs_173k_right.npz\",\n",
    "                                    0,150000)\n",
    "dataset_val = tr.RealFrameDataset(\"/home/jrenner/local/jerenner/emsim/notebooks/subimgs_170k_left.npz\",\n",
    "                                  \"/home/jrenner/local/jerenner/emsim/notebooks/subimgs_173k_right.npz\",\n",
    "                                  150000,20000)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=1000, shuffle=False, collate_fn=tr.my_collate_reg_line_realdata, num_workers=1)\n",
    "\n",
    "# Define the model.\n",
    "model = emnet.basicCNN_reg()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=lrate, weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    for m in model.modules():\n",
    "        for child in m.children():\n",
    "            if type(child) == nn.BatchNorm2d:\n",
    "                child.track_running_stats = False\n",
    "                child.running_mean = None\n",
    "                child.running_var = None\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    train_loss = tr.train_regression_line(model, epoch, train_loader, optimizer, 1000)\n",
    "    scheduler.step(train_loss)\n",
    "    #if(epoch % 50 == 0):\n",
    "    torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at individual events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light side is 0\n",
      "Arg max is [8 8]\n",
      "Line drawn: m = 10.0, b = -44.5\n",
      "Pred point ( 7.660212159156799 , 8.574605889618397 )\n",
      "Pred vector ( -0.83978784 , 0.07460589 )\n",
      "Predicted distance to the line: 2.341075311668933\n",
      "TEST LOSS\n",
      "-- Test vector loss: tensor(0.0185, device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "-- Test distance loss: tensor(0.3102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "COMPARE LOSS\n",
      "-- Distance is: 2.341075311668933\n",
      "-- Compare vector loss: 0.7108096563938373\n",
      "-- Compare distance loss: 0.3102001155201245\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at individual events\n",
    "evt_plt = 120\n",
    "evt_item = dataset_val[evt_plt]\n",
    "evt_frame_cmax = np.array(evt_item[0])\n",
    "evt_frame      = np.array(evt_item[1])\n",
    "evt_argmax     = np.array(evt_item[2])\n",
    "evt_err        = np.array(evt_item[3])/emnet.PIXEL_SIZE\n",
    "evt_lside      = np.array(evt_item[4])\n",
    "evt_line_m     = evt_item[5]\n",
    "evt_line_b     = evt_item[6]\n",
    "print(\"Light side is\",evt_lside)\n",
    "print(\"Arg max is\",evt_argmax)\n",
    "\n",
    "# Information for drawing the line.\n",
    "nrows = evt_frame.shape[0]\n",
    "ncols = evt_frame.shape[1]\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "print(\"Line drawn: m = {}, b = {}\".format(evt_line_m,evt_line_b))\n",
    "\n",
    "# Run the model and compute the reconstructed point.\n",
    "data = torch.tensor(evt_frame_cmax).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "output_score = model(data)\n",
    "pred_err = np.array(output_score.cpu().detach().numpy()).squeeze()\n",
    "row_pred = pred_err[1] + evt_argmax[0] + 0.5\n",
    "col_pred = pred_err[0] + evt_argmax[1] + 0.5\n",
    "print(\"Pred point (\",col_pred,\",\",row_pred,\")\")\n",
    "print(\"Pred vector (\",pred_err[0],\",\",pred_err[1],\")\")\n",
    "\n",
    "# Compute the distance from the reconstructed point to the line.\n",
    "dist_pred = (evt_line_m*col_pred - row_pred + evt_line_b) / (evt_line_m**2 + 1)**0.5\n",
    "print(\"Predicted distance to the line:\",dist_pred)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_frame_cmax,extent=[0,11,11,0])\n",
    "#ax1.plot([evt_err[0]+5.5],[evt_err[1]+5.5],color='red',marker='*',markersize=10)\n",
    "ax1.plot([pred_err[0]+5.5],[pred_err[1]+5.5],color='green',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event, seen by the NN\\n(centered on max pixel)\")\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "plt.imshow(evt_frame,extent=[0,11,11,0])\n",
    "#ax2.plot([col_true],[row_true],color='red',marker='*',markersize=10)\n",
    "ax2.plot([col_pred],[row_pred],color='green',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event with line\")\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = evt_line_m*xfit + evt_line_b\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)],linewidth=2,color='red')\n",
    "\n",
    "# Compute a test loss\n",
    "batch_size = 1\n",
    "indices = np.indices((emnet.EVT_SIZE,emnet.EVT_SIZE))\n",
    "row_coords = torch.tensor(indices[0] + 0.5 - ((emnet.EVT_SIZE-1)/2 + 0.5)).repeat([batch_size,1,1]).cuda()\n",
    "col_coords = torch.tensor(indices[1] + 0.5 - ((emnet.EVT_SIZE-1)/2 + 0.5)).repeat([batch_size,1,1]).cuda()\n",
    "print(\"TEST LOSS\")\n",
    "#loss_vec, loss_dist = tr.loss_reg_edge(torch.tensor([[evt_err[0],evt_err[1]]]), np.array([evt_argmax]), line_m, line_b, evt_lside)\n",
    "loss_vec, loss_dist, dist_reco_masked = tr.loss_reg_edge(data.squeeze(1), torch.tensor(evt_item[3]).unsqueeze(0).cuda(), output_score, row_coords, col_coords, torch.tensor([evt_argmax]).cuda(), evt_line_m, evt_line_b, evt_lside)\n",
    "print(\"-- Test vector loss:\",loss_vec)\n",
    "print(\"-- Test distance loss:\",loss_dist)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "plt.imshow(dist_reco_masked.cpu().detach().squeeze(0).numpy(),extent=[0,11,11,0])\n",
    "#ax3.plot([evt_err[0]+5.5],[evt_err[1]+5.5],color='red',marker='*',markersize=10)\n",
    "ax3.plot([pred_err[0]+5.5],[pred_err[1]+5.5],color='green',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.title(\"Cluster\")\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = evt_line_m*xfit + evt_line_b\n",
    "\n",
    "print(\"COMPARE LOSS\")\n",
    "if(evt_lside == 1):\n",
    "    dist_pred *= -1\n",
    "sigma_dist = 2\n",
    "loss_vec = (pred_err[0]**2 + pred_err[1]**2)\n",
    "loss_dist = np.exp(-dist_pred/sigma_dist)\n",
    "print(\"-- Distance is:\",dist_pred)\n",
    "print(\"-- Compare vector loss:\",loss_vec)\n",
    "print(\"-- Compare distance loss:\",loss_dist)\n",
    "#print(\"True vector is ({},{})\".format(evt_vec[0],evt_vec[1]))\n",
    "#print(\"Predicted vector is ({},{})\".format(pred_vec[0],pred_vec[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = img_th_proc[480,208:308]\n",
    "xdata = np.arange(len(ydata)) + 208\n",
    "\n",
    "p0 = [max(ydata), np.median(xdata), 1, min(ydata)] # this is an mandatory initial guess\n",
    "popt, pcov = curve_fit(sigmoid, xdata, ydata,p0, method='dogbox')\n",
    "\n",
    "x = np.linspace(xdata[0], xdata[-1], 1000)\n",
    "y = sigmoid(x, *popt)\n",
    "\n",
    "plt.plot(xdata, ydata, label='data')\n",
    "plt.plot(x,y, label='fit')\n",
    "plt.legend(loc='best')\n",
    "print(\"Best fit parameters: L = {}, x0 = {}, k = {}, b = {}\".format(popt[0],popt[1],popt[2],popt[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(np.average(imgs_medsub,axis=0))\n",
    "plt.imshow(imgs_th[0][0:11,0:11])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subimg_halfsize = 5\n",
    "rmin = 0\n",
    "rmax = len(available_edges)\n",
    "cmin = 0\n",
    "cmax = imgs_th.shape[2]\n",
    "    \n",
    "# For each individual thresholded image, choose the sub-images.\n",
    "valid_subimages_th, valid_subimages = [], []\n",
    "line_m, line_b = [], []\n",
    "for iimg in range(1): #len(imgs_th.shape[0]))\n",
    "    \n",
    "    img    = imgs[iimg]\n",
    "    img_th = imgs_th[iimg]\n",
    "    \n",
    "    done = (available_edges == -1).all()\n",
    "\n",
    "    while(not done):\n",
    "        \n",
    "        # Choose a random location in the x-edges array.\n",
    "        rmid = np.random.randint(rmax)\n",
    "        \n",
    "        # Make sure the current edge is available.\n",
    "        if(available_edges[rmid] < 0):\n",
    "            continue\n",
    "        \n",
    "        # Select a window equal to the subimage size in length.\n",
    "        rstart = rmid - subimg_halfsize\n",
    "        rend   = rmid + subimg_halfsize + 1\n",
    "        if(rstart < rmin or rend >= rmax):\n",
    "            if(rstart < rmin): rstart = rmin\n",
    "            if(rend >= rmax):  rend = rmax\n",
    "            available_edges[rstart:rend] = -1\n",
    "            continue\n",
    "        \n",
    "        # Ensure all edges for this image have not yet been used.\n",
    "        aedges = available_edges[rstart:rend]\n",
    "        if((aedges > 0).all()):\n",
    "        \n",
    "            # Construct the subimage.\n",
    "            xmid = x_edges[rmid]\n",
    "            cstart = xmid - subimg_halfsize\n",
    "            cend   = xmid + subimg_halfsize + 1\n",
    "            \n",
    "            # Only continue with a subimage that fits properly in the full image.\n",
    "            if(cstart >= cmin and cend < cmax):\n",
    "                \n",
    "                # Create the subimage.\n",
    "                subimg    = img[rstart:rend,cstart:cend]\n",
    "                subimg_th = img_th[rstart:rend,cstart:cend]\n",
    "\n",
    "                # Check that the (thresholded) subimage is valid.\n",
    "                if(check_subimage_th(subimg_th)):\n",
    "                    \n",
    "                    # Compute the slopes and intercepts.\n",
    "                    x1 = (x_edges[rstart] - cstart) + 0.5\n",
    "                    x2 = (x_edges[rend] - cstart) + 0.5\n",
    "                    if(x2 == x1): x2 = x1-1\n",
    "                    y1 = 0.5\n",
    "                    y2 = (rend - rstart) - 0.5\n",
    "                    m = (y2 - y1) / (x2 - x1)\n",
    "                    b = y2 - m*x2\n",
    "                    print(\"xmid_rel = {}, cstart = {}, cend = {}\".format(xmid-cstart,cstart,cend))\n",
    "                    print(\"(x1,y1) = ({},{}); (x2,y2) = ({},{})\".format(x1,y1,x2,y2))\n",
    "                    line_m.append(m)\n",
    "                    line_b.append(b)\n",
    "\n",
    "                    # Save the subimage.\n",
    "                    valid_subimages.append(subimg)\n",
    "                    valid_subimages_th.append(subimg_th)\n",
    "\n",
    "                    # Remove all edges for this subimage from availability.\n",
    "                    available_edges[rstart:rend] = -1\n",
    "\n",
    "                # If the image is not valid, do nothing, but continue the loop.\n",
    "                \n",
    "            else:\n",
    "                available_edges[rstart:rend] = -1\n",
    "           \n",
    "        # If not all edges are available, \n",
    "        else:\n",
    "            available_edges[rstart:rend] = -1\n",
    "        \n",
    "        # Set the terminating condition.\n",
    "        done = (available_edges == -1).all()\n",
    "        print(\"Number of available edges:\",np.sum(available_edges == 1),\"/\",len(available_edges))\n",
    "\n",
    "line_m = np.array(line_m)\n",
    "line_b = np.array(line_b)\n",
    "valid_subimages = np.array(valid_subimages)\n",
    "valid_subimages_th = np.array(valid_subimages_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random multi-electron events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameset  = tr.EMFrameDataset(dset,frame_size=20,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=30.0, lside = 0, res_factor=3)\n",
    "#frameset = tr.EMFrameDataset(dset,frame_size=4855,nelec_mean=103713,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "#frameset = tr.EMFrameDataset(dset,frame_size=100,nelec_mean=88,nelec_sigma=2,noise_mean=0,noise_sigma=20)\n",
    "#frameset = tr.EMFrameDataset(dset,frame_size=100,nelec_mean=10,nelec_sigma=1,noise_mean=0,noise_sigma=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate many frames and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(fit_img, th = 0.5, pct_rng = 0.2, nbins_hdist = 80):\n",
    "    \n",
    "    A = fit_img/np.mean(fit_img)\n",
    "    nrows = A.shape[0]\n",
    "    ncols = A.shape[1]\n",
    "    ncts = np.sum(A[A >= th])\n",
    "    nzeros = np.sum(1-A[A < th])\n",
    "    wcts = 1 #nzeros/ncts\n",
    "    indices = np.indices((nrows,ncols))\n",
    "    irows = indices[0]\n",
    "    icols = indices[1]\n",
    "    print(\"nzeros = {}, ncts = {}, wcts = {}\".format(nzeros,ncts,wcts))\n",
    "\n",
    "    def count_loss(x):\n",
    "        m,b = x\n",
    "\n",
    "        # The loss L is:\n",
    "        #\n",
    "        # (number of 0s in the dark region) - wcts*(number of 1s in the dark region)\n",
    "        # + wcts*(number of 1s in the light region) - (number of 0s in the dark region)\n",
    "        # \n",
    "        # where wcts is the count weight, determined such that the number of counts multiplied by wcts is equal to\n",
    "        # the number of zeros.\n",
    "        L = 0\n",
    "        L1 = np.sum(1-A[(irows < m*icols + b) & (A < th)])\n",
    "        L2 = np.sum(A[(irows < m*icols + b) & (A >= th)])\n",
    "        L3 = np.sum(A[(irows >= m*icols + b) & (A >= th)])\n",
    "        L4 = np.sum(1-A[(irows >= m*icols + b) & (A < th)])\n",
    "\n",
    "        L = L1 - wcts*L2 + wcts*L3 - L4\n",
    "        #print(\"Loss is:\",-L,\"with L1 =\",L1,\"L2 =\",L2,\"L3 =\",L3,\"L4 =\",L4)\n",
    "        return -L\n",
    "    \n",
    "    initial_guess = [-1.5*nrows/ncols,2.0*nrows]\n",
    "    result = optimize.minimize(count_loss,initial_guess,method='Nelder-Mead',tol=1e-5)\n",
    "    m,b = result.x\n",
    "    Lmin = result.fun\n",
    "    print(\"m = \",m,\"b = \",b,\"Lmin=\",Lmin)\n",
    "    \n",
    "    # Force m and b.\n",
    "    m = -2.0\n",
    "    b = 90.0\n",
    "    print(\"NOTE: hard-coding m and b\")\n",
    "    \n",
    "    # Get the loss over a range of the parameters.\n",
    "    mrng = np.arange(m-pct_rng*m, m+pct_rng*m, 2*pct_rng*m/1000)\n",
    "    Lrng_m = np.array([count_loss([mval,b])/Lmin for mval in mrng])\n",
    "    brng = np.arange(b-pct_rng*b, b+pct_rng*b, 2*pct_rng*b/1000)\n",
    "    Lrng_b = np.array([count_loss([m,bval])/Lmin for bval in brng])\n",
    "    \n",
    "    # Get the histogram of mean value vs. distance.\n",
    "    dist = (5./3.)*(m*icols - irows + b) / (m**2 + 1)                          # compute distance to line for each point\n",
    "    hw, bb = np.histogram(dist.flatten(),weights=A.flatten(),bins=nbins_hdist)  # weighted histogram\n",
    "    hh, bb = np.histogram(dist.flatten(),bins=nbins_hdist)                      # unweighted (for normalization)\n",
    "    hh[hh == 0] = 0.1\n",
    "    hfinal = hw / hh                                                   # normalize the histogram\n",
    "    bcenters = (bb[1:] + bb[:-1]) / 2                                  # determine the bin centers\n",
    "    \n",
    "    return m,b,Lmin,mrng,Lrng_m,brng,Lrng_b,hfinal,bcenters\n",
    "\n",
    "# Fit the S-curve to a sigmoid.\n",
    "def fit_scurve():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some number of events and count them.\n",
    "th_unet = 0.2\n",
    "th_classical = 825/4.\n",
    "evts = np.arange(0,100000)\n",
    "l_frames, l_labels, l_ct_unet, l_ct_classical = [], [], [], []\n",
    "for evt in evts:\n",
    "    frame,label = frameset[evt]\n",
    "    gnd_truth = label[0]\n",
    "    \n",
    "    # Send through the model.\n",
    "    data = torch.tensor(frame).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    output_score = model(data)\n",
    "    \n",
    "    # Compute the predicted pixel values.\n",
    "    prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()\n",
    "    ct_unet = (prob > th_unet)\n",
    "    \n",
    "    # Count with a single threshold.\n",
    "    #ct_classical = (frame > th_classical)\n",
    "    \n",
    "    # Max argument classical truth.\n",
    "    #ct_classical = np.zeros(frame.shape)\n",
    "    #ct_classical[np.unravel_index(np.argmax(frame),frame.shape)] = 1\n",
    "    \n",
    "    # Threshold truth.\n",
    "    ct_classical = label[1]\n",
    "    \n",
    "    l_frames.append(frame)\n",
    "    l_labels.append(gnd_truth)\n",
    "    l_ct_unet.append(ct_unet)\n",
    "    l_ct_classical.append(ct_classical)\n",
    "    \n",
    "    if((evt-evts[0]) % (len(evts)/100) == 0):\n",
    "            print(\"{}% done\".format(int((evt-evts[0]) / (len(evts)/100))))\n",
    "            \n",
    "l_frames = np.array(l_frames)\n",
    "l_labels = np.array(l_labels)\n",
    "l_ct_unet = np.array(l_ct_unet)\n",
    "l_ct_classical = np.array(l_ct_classical)\n",
    "\n",
    "# Create a summed frame, label, and count arrays.\n",
    "frame = np.sum(l_frames,axis=0)\n",
    "label = np.sum(l_labels,axis=0)\n",
    "ct_unet = np.sum(l_ct_unet,axis=0)\n",
    "ct_classical = np.sum(l_ct_classical,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_th = 0.99\n",
    "ct_th = 0.1\n",
    "m_frame,b_frame,Lmin_frame,mrng_frame,Lrng_m_frame,brng_frame,Lrng_b_frame,hdist_frame,bcenters_frame = fit_line(frame,th=raw_th)\n",
    "m_label,b_label,Lmin_label,mrng_label,Lrng_m_label,brng_label,Lrng_b_label,hdist_label,bcenters_label = fit_line(label,th=ct_th)\n",
    "m_unet,b_unet,Lmin_unet,mrng_unet,Lrng_m_unet,brng_unet,Lrng_b_unet,hdist_unet,bcenters_unet = fit_line(ct_unet,th=ct_th)\n",
    "m_classical,b_classical,Lmin_classical,mrng_classical,Lrng_m_classical,brng_classical,Lrng_b_classical,hdist_classical,bcenters_classical = fit_line(ct_classical,th=ct_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#frame,label = frameset[0]\n",
    "logscale = False\n",
    "nrows = frame.shape[0]\n",
    "ncols = frame.shape[1]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(12.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "if(logscale):\n",
    "    plt.imshow(np.log(frame))\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"log(counts)\")\n",
    "    #plt.title(\"Raw frame (log counts, threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(raw_th,m_frame,b_frame))\n",
    "    plt.title(\"Raw frame (log counts)\\nm = {:.2f}, b = {:.2f}\".format(m_frame,b_frame))\n",
    "else:\n",
    "    plt.imshow(frame/np.max(frame))\n",
    "    cbar = plt.colorbar()\n",
    "    #plt.title(\"Raw frame (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(raw_th,m_frame,b_frame))\n",
    "    plt.title(\"Raw frame\\nm = {:.2f}, b = {:.2f}\".format(m_frame,b_frame))\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_frame*xfit + b_frame\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "plt.imshow(label/np.max(label))\n",
    "#plt.title(\"Truth (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(ct_th,m_label,b_label))\n",
    "plt.title(\"Truth\\nm = {:.2f}, b = {:.2f}\".format(m_label,b_label))\n",
    "plt.colorbar()\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_label*xfit + b_label\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "plt.imshow(ct_unet/np.max(ct_unet))\n",
    "#plt.title(\"UNet counts (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(ct_th,m_unet,b_unet))\n",
    "plt.title(\"UNet counts\\nm = {:.2f}, b = {:.2f}\".format(m_unet,b_unet))\n",
    "plt.colorbar()\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_unet*xfit + b_unet\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "plt.imshow(ct_classical/np.max(ct_classical))\n",
    "#plt.title(\"Classical counts + line info (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(ct_th,m_classical,b_classical))\n",
    "plt.title(\"Classical counts + line info\\nm = {:.2f}, b = {:.2f}\".format(m_classical,b_classical))\n",
    "plt.colorbar()\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_classical*xfit + b_classical\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "print(\"Total counts, truth:\",np.sum(label))\n",
    "print(\"Total counts, classical:\",np.sum(ct_classical))\n",
    "print(\"Total counts, unet:\",np.sum(ct_unet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "frame_mult = 1./(1.-min(hdist_frame))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(bcenters_frame,frame_mult*(hdist_frame-min(hdist_frame)),'.-',color='black',label='frame')\n",
    "plt.plot(bcenters_label,hdist_label,'.-',color='green',label='true')\n",
    "plt.plot(bcenters_unet,hdist_unet,'.-',color='blue',label='UNet')\n",
    "plt.plot(bcenters_classical,hdist_classical,'.-',color='red',label='classical+line')\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0.0,3.2])\n",
    "plt.xlabel(\"Distance from line (micrometers)\")\n",
    "plt.ylabel(\"Mean number of normalized counts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save s-curve information.\n",
    "np.savez(\"scurve_edge_th02_epoch10.npz\", bcenters_frame=bcenters_frame, hdist_frame=hdist_frame, bcenters_label=bcenters_label, hdist_label=hdist_label, \n",
    "         bcenters_unet=bcenters_unet, hdist_unet=hdist_unet, bcenters_classical=bcenters_classical, \n",
    "         hdist_classical=hdist_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(mrng_frame/m_frame,Lrng_m_frame,color='black',label='frame')\n",
    "plt.plot(mrng_label/m_label,Lrng_m_label,color='green',label='true')\n",
    "plt.plot(mrng_unet/m_unet,Lrng_m_unet,color='blue',label='UNet')\n",
    "plt.plot(mrng_classical/m_classical,Lrng_m_classical,color='red',label='classical')\n",
    "plt.xlabel(\"Parameter m/m$_0$\")\n",
    "plt.ylabel(\"Relative loss L/L(m$_0$)\")\n",
    "plt.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.plot(brng_frame/b_frame,Lrng_b_frame,color='black',label='frame')\n",
    "plt.plot(brng_label/b_label,Lrng_b_label,color='green',label='true')\n",
    "plt.plot(brng_unet/b_unet,Lrng_b_unet,color='blue',label='UNet')\n",
    "plt.plot(brng_classical/b_classical,Lrng_b_classical,color='red',label='classical')\n",
    "plt.xlabel(\"Parameter b/b$_0$\")\n",
    "plt.ylabel(\"Relative loss L/L(b$_0$)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"evt_arrays.npz\",evt_arrays=l_evt_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot s-curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scurve_noedge = np.load(\"scurve_noedge_th088.npz\")\n",
    "bc_label_noedge = scurve_noedge['bcenters_label']\n",
    "h_label_noedge = scurve_noedge['hdist_label']\n",
    "bc_unet_noedge = scurve_noedge['bcenters_unet']\n",
    "h_unet_noedge = scurve_noedge['hdist_unet']\n",
    "bc_classical_noedge = scurve_noedge['bcenters_classical']\n",
    "h_classical_noedge = scurve_noedge['hdist_classical']\n",
    "\n",
    "scurve_edge = np.load(\"scurve_edge_th08.npz\")\n",
    "bc_frame_edge = scurve_edge['bcenters_frame']\n",
    "h_frame_edge = scurve_edge['hdist_frame']\n",
    "bc_label_edge = scurve_edge['bcenters_label']\n",
    "h_label_edge = scurve_edge['hdist_label']\n",
    "bc_unet_edge = scurve_edge['bcenters_unet']\n",
    "h_unet_edge = scurve_edge['hdist_unet']\n",
    "bc_classical_edge = scurve_edge['bcenters_classical']\n",
    "h_classical_edge = scurve_edge['hdist_classical']\n",
    "\n",
    "scurve_edge10 = np.load(\"scurve_edge_th08_epoch10.npz\")\n",
    "bc_label_edge10 = scurve_edge10['bcenters_label']\n",
    "h_label_edge10 = scurve_edge10['hdist_label']\n",
    "bc_unet_edge10 = scurve_edge10['bcenters_unet']\n",
    "h_unet_edge10 = scurve_edge10['hdist_unet']\n",
    "bc_classical_edge10 = scurve_edge10['bcenters_classical']\n",
    "h_classical_edge10 = scurve_edge10['hdist_classical']\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "frame_mult = 1./(1.-min(h_frame_edge))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "plt.plot(bc_frame_edge,frame_mult*(h_frame_edge-min(h_frame_edge)),'.-',color='black',label='raw frame, scaled')\n",
    "plt.plot(bc_classical_edge,h_classical_edge,'.-',color='red',label='classical')\n",
    "plt.plot(bc_unet_noedge,h_unet_noedge,'.-',color='orange',label='UNet, no edge, epoch 500')\n",
    "plt.plot(bc_unet_edge10,h_unet_edge10,'.-',color='cyan',label='UNet + edge, epoch 11')\n",
    "plt.plot(bc_unet_edge,h_unet_edge,'.-',color='blue',label='UNet + edge, epoch 500')\n",
    "plt.plot(bc_label_edge,h_label_edge,'.-',color='green',label='true')\n",
    "plt.xlim([-5,5])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.xlabel(\"Distance from line (pixels)\")\n",
    "plt.ylabel(\"Mean number of normalized counts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine a large generated frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label[0,0:50,0:50])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sim = frame.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(img_sim[(img_sim < 400)],bins=50)\n",
    "plt.hist(img_sim,bins=50)\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "plt.yscale(\"log\")\n",
    "print(\"Total pixels:\",len(img_sim))\n",
    "#plt.xlim([0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"frame_4855x4855_11occ.npz\",frame=frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a network (multi-electron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/jrenner/local/jerenner/emsim/models'\n",
    "lrate       = 1e-3   # Learning rate to use in the training.\n",
    "load_model  = False   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 200    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/model_init_199.pt\".format(modeldir)\n",
    "\n",
    "# Create the dataset.\n",
    "# 576x576: 2927 +/- 71\n",
    "# 100x100: 88 +/- 2\n",
    "# 50x50: 22 +/- 0.5\n",
    "#dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)\n",
    "\n",
    "# \"Real-data-like\" dataset: occupancy 11, noise_mean=683, noise_sigma=11.2\n",
    "dset = tr.EMDataset(\"dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)\n",
    "#dataset_train   = tr.EMFrameDataset(dset,frame_size=50,nelec_mean=11,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "dataset_train = tr.EMFrameDataset(dset,frame_size=20,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=30.0,res_factor=3)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=50, shuffle=False, collate_fn=tr.my_collate_unet, num_workers=1)\n",
    "\n",
    "# Define the model.\n",
    "model = UNet(n_channels=1, n_classes=1)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lrate, weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    train_loss = tr.train_unet(model, epoch, train_loader, optimizer)\n",
    "    scheduler.step(train_loss)\n",
    "    #if(epoch % 50 == 0):\n",
    "    torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"{}/model_frames_20x20_noise683_2e_bcsloss_edge_unweighted_front_1em4_100kev_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy.\n",
    "tloss = np.loadtxt(\"train.txt\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.plot(tloss[:,0],tloss[:,1],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.plot(tloss[:,0],tloss[:,2],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "#dset = tr.EMDataset(\"dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)\n",
    "#dataset_train   = tr.EMFrameDataset(dset,frame_size=50,nelec_mean=11,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over many events and evaluate the true positives and false positives.\n",
    "# Store in arrays as:\n",
    "#\n",
    "#  [tp0 tp1 tp2 tp3 ... tpN], each number corresponding to a different NN threshold or classical threshold\n",
    "#\n",
    "tp_unet = []; fp_unet = []\n",
    "tp_classical = []; fp_classical = []\n",
    "#nn_thresholds = np.arange(0.05,1.0,0.1)\n",
    "nn_thresholds = np.concatenate((np.logspace(-4,-0.1,500), np.logspace(-0.1,0,500)))\n",
    "#nn_thresholds = np.logspace(-3,0,1000)\n",
    "classical_thresholds = np.arange(600/4,7000/4,10)\n",
    "evts = np.arange(100000,101000)\n",
    "for evt in evts:\n",
    "    \n",
    "    # Get the event and truth.\n",
    "    evt_item = dataset_train[evt]\n",
    "    evt_arr = evt_item[0]\n",
    "    evt_lbl = evt_item[1][0]\n",
    "    \n",
    "    # Send through the model.\n",
    "    data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    output_score = model(data)\n",
    "    \n",
    "    # Compute the predicted pixel values.\n",
    "    prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()\n",
    "    \n",
    "    # Compute the TP and FP values for unet.\n",
    "    temp_tp = []; temp_fp = []\n",
    "    for th in nn_thresholds:\n",
    "        pred = (prob > th)\n",
    "        tp = np.sum((evt_lbl == 1) & (pred == True))\n",
    "        fn = np.sum((evt_lbl == 1) & (pred == False))\n",
    "        if( (tp + fn) > 0): tp = tp / (tp + fn)\n",
    "        else: tp = 1\n",
    "        fp = np.sum((evt_lbl == 0) & (pred == True))\n",
    "        tn = np.sum((evt_lbl == 0) & (pred == False))\n",
    "        if( (fp + tn) > 0): fp = fp / (fp + tn)\n",
    "        else: fp = 0\n",
    "        temp_tp.append(tp)\n",
    "        temp_fp.append(fp)\n",
    "    tp_unet.append(temp_tp)\n",
    "    fp_unet.append(temp_fp)\n",
    "    \n",
    "    # Compute the TP and FP values for the classical threshold.\n",
    "    temp_tp = []; temp_fp = []\n",
    "    for th in classical_thresholds:\n",
    "        #pred = (evt_arr > th)\n",
    "        pred = np.zeros(evt_arr.shape)\n",
    "        pred[np.unravel_index(np.argmax(evt_arr),evt_arr.shape)] = 1\n",
    "        tp = np.sum((evt_lbl == 1) & (pred == True))\n",
    "        fn = np.sum((evt_lbl == 1) & (pred == False))\n",
    "        if( (tp + fn) > 0): tp = tp / (tp + fn)\n",
    "        else: tp = 1\n",
    "        fp = np.sum((evt_lbl == 0) & (pred == True))\n",
    "        tn = np.sum((evt_lbl == 0) & (pred == False))\n",
    "        if( (tp + tn) > 0): fp = fp / (fp + tn)\n",
    "        else: tp = 0\n",
    "        temp_tp.append(tp)\n",
    "        temp_fp.append(fp)\n",
    "    tp_classical.append(temp_tp)\n",
    "    fp_classical.append(temp_fp)\n",
    "    \n",
    "    if((evt-evts[0]) % (len(evts)/100) == 0):\n",
    "            print(\"{}% done\".format(int((evt-evts[0]) / (len(evts)/100))))\n",
    "    \n",
    "tp_unet = np.array(tp_unet)\n",
    "fp_unet = np.array(fp_unet)\n",
    "tp_classical = np.array(tp_classical)\n",
    "fp_classical = np.array(fp_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp_rate_unet = np.mean(tp_unet,axis=0)\n",
    "tp_err_unet = np.std(tp_unet,axis=0)/np.sqrt(tp_unet.shape[0])\n",
    "fp_rate_unet = np.mean(fp_unet,axis=0)\n",
    "fp_err_unet = np.std(fp_unet,axis=0)/np.sqrt(fp_unet.shape[0])\n",
    "tp_rate_classical = np.mean(tp_classical,axis=0)\n",
    "tp_err_classical = np.std(tp_classical,axis=0)/np.sqrt(tp_classical.shape[0])\n",
    "fp_rate_classical = np.mean(fp_classical,axis=0)\n",
    "fp_err_classical = np.std(fp_classical,axis=0)/np.sqrt(fp_classical.shape[0])\n",
    "\n",
    "plt.errorbar(fp_rate_unet,tp_rate_unet,xerr=fp_err_unet,yerr=tp_err_unet,label='Unet')\n",
    "plt.errorbar(fp_rate_classical,tp_rate_classical,xerr=fp_err_classical,yerr=tp_err_classical,label='Basic threshold')\n",
    "plt.xlim([0,0.02])\n",
    "plt.legend()\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NN ------------------\")\n",
    "for tpr,fpr,th in zip(tp_rate_unet,fp_rate_unet,nn_thresholds):\n",
    "    print(\"[Threshold {}] TP = {}, FP = {}\".format(th,tpr,fpr))\n",
    "print(\"Classical threshold ------------------\")\n",
    "for tpr,fpr,th in zip(tp_rate_classical,fp_rate_classical,classical_thresholds):\n",
    "    print(\"[Threshold {}] TP = {}, FP = {}\".format(th,tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 10008\n",
    "evt_item = dataset_train[evt_plt]\n",
    "evt_arr = evt_item[0]\n",
    "evt_lbl = evt_item[1][1]\n",
    "\n",
    "# Send through the model.\n",
    "data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "#target = torch.tensor(evt_lbl).float().cuda()\n",
    "output_score = model(data)\n",
    "\n",
    "# Compute the predicted pixel and (x,y) values.\n",
    "prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()\n",
    "\n",
    "# Threshold\n",
    "# prob = np.zeros(evt_arr.shape)\n",
    "# prob[evt_arr > 80] = 1\n",
    "\n",
    "# Determine number of correct pixels\n",
    "th = 0.2\n",
    "pred = (prob > th)\n",
    "nelec = int(np.sum(evt_lbl == 1))\n",
    "nelec_pred = int(np.sum(pred))\n",
    "nspace = int(np.sum(evt_lbl == 0))\n",
    "nelec_coinc = np.sum((evt_lbl == 1) & (pred == True))\n",
    "nspace_coinc = np.sum((evt_lbl == 0) & (pred == False))\n",
    "print(\"{}/{} electrons predicted\".format(nelec_pred,nelec))\n",
    "print(\"{}/{} electrons coincided exactly\".format(nelec_coinc,nelec))\n",
    "print(\"{}/{} empty spaces coincided exactly\".format(nspace_coinc,nspace))\n",
    "\n",
    "# Information for drawing the line.\n",
    "nrows = evt_arr.shape[0]\n",
    "ncols = evt_arr.shape[1]\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "m = -2*nrows/ncols\n",
    "b = 80\n",
    "print(\"Line drawn: m = {}, b = {}\".format(m,b))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_arr)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event\")\n",
    "\n",
    "# xfit = np.arange(0,ncols,0.1)\n",
    "# yfit = m*xfit + b\n",
    "# plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "plt.imshow(evt_lbl)\n",
    "plt.colorbar()\n",
    "plt.title(\"Target\")\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "#plt.imshow(np.log10(prob))\n",
    "plt.imshow(prob)\n",
    "plt.colorbar()\n",
    "plt.title(\"{}/{} electrons predicted\\n{}/{} electrons coincided exactly\\n{}/{} empty spaces coincided exactly\".format(nelec_pred,nelec,nelec_coinc,nelec,nspace_coinc,nspace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct output and label arrays for 5 events.\n",
    "frames,outputs,labels = [], [], []\n",
    "for iframe in range(50):\n",
    "    frame,label = frameset[iframe]\n",
    "    frames.append(frame)\n",
    "    outputs.append(label[1])\n",
    "    labels.append(label)\n",
    "frames = np.array(frames)\n",
    "outputs = np.array(outputs)\n",
    "labels = np.array(labels)\n",
    "print(\"Frames shape is:\",frames.shape)\n",
    "print(\"Outputs shape is:\",outputs.shape)\n",
    "print(\"Labels shape is:\",labels.shape)\n",
    "\n",
    "# Convert to tensors.\n",
    "outputs[outputs == 0] = 1e-10\n",
    "outputs[outputs == 1] = 0.99999999\n",
    "frames = torch.tensor(frames)\n",
    "output = torch.tensor(np.log(outputs/(1-outputs)))\n",
    "target = torch.tensor(labels)\n",
    "\n",
    "# Compute the loss.\n",
    "sigma_dist = 1\n",
    "real_truth = target[:,0,:,:]\n",
    "th_truth = target[:,1,:,:]\n",
    "edge_truth = target[:,2,:,:]\n",
    "dist = target[:,3,:,:]\n",
    "\n",
    "final_truth = th_truth * edge_truth\n",
    "\n",
    "wts     = torch.sum(torch.exp(-(dist)**2/(2*sigma_dist**2))*th_truth,axis=(1,2))\n",
    "wt_norm = torch.sum(th_truth,axis=(1,2))\n",
    "wt_norm[wt_norm == 0] = 1\n",
    "wts /= wt_norm\n",
    "print(\"Weights are: \",wts)\n",
    "\n",
    "w_edge = 100\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss(reduce=False)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "loss0 = bce_loss(output,final_truth)\n",
    "loss1 = w_edge*sigmoid(output)*(1-edge_truth)\n",
    "loss0W = torch.exp(-(dist)**2/(2*sigma_dist**2))*(loss0)\n",
    "loss1W = torch.exp(-(dist)**2/(2*sigma_dist**2))*(loss1)\n",
    "loss = torch.mean(torch.exp(-(dist)**2/(2*sigma_dist**2))*(loss0 + loss1))\n",
    "print(\"Mean loss is \",loss)\n",
    "print(\"Mean loss0 is \",torch.mean(loss0))\n",
    "print(\"Mean loss0W is \",torch.mean(loss0W))\n",
    "print(\"Mean loss1 is \",torch.mean(loss1))\n",
    "print(\"Mean loss1W is \",torch.mean(loss1W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mod = torch.sum(torch.abs(dist*(edge_truth-1)),axis=(1,2))\n",
    "dist_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iframe = 2\n",
    "show_sum = False\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(16.0)\n",
    "\n",
    "ax1 = fig.add_subplot(241)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(sigmoid(output)),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(sigmoid(output[iframe])),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"sigmoid(Output)\")\n",
    "\n",
    "ax2 = fig.add_subplot(242)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(th_truth),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(real_truth[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Real truth\")\n",
    "\n",
    "ax3 = fig.add_subplot(243)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(edge_truth), axis=0))\n",
    "else: plt.imshow(np.array(edge_truth[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Edge truth\")\n",
    "\n",
    "ax4 = fig.add_subplot(244)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(torch.exp(-(dist)**2/(2*sigma_dist**2))),axis=0))\n",
    "else: plt.imshow(np.array(torch.exp(-(dist)**2/(2*sigma_dist**2))[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Distance\")\n",
    "\n",
    "ax5 = fig.add_subplot(245)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(loss0), axis=0))\n",
    "else: plt.imshow(np.array(loss0[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Loss0\")\n",
    "\n",
    "ax6 = fig.add_subplot(246)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(loss1),axis=0))\n",
    "else: plt.imshow(np.array(loss1[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Loss1\")\n",
    "\n",
    "ax7 = fig.add_subplot(247)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(loss0W + loss1W),axis=0))\n",
    "else: plt.imshow(np.array(loss0W[iframe] + loss1W[iframe]))\n",
    "# if(show_sum): plt.imshow(np.sum(np.array(dist_mod),axis=0))\n",
    "# else: plt.imshow(np.array(dist_mod[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"LossW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iframe = 2\n",
    "show_sum = False\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(3.0)\n",
    "fig.set_figwidth(20.0)\n",
    "\n",
    "ax1 = fig.add_subplot(151)\n",
    "if(show_sum): plt.imshow(np.sum(frames,axis=0)/np.max(np.sum(frames,axis=0)),interpolation=None)\n",
    "else: plt.imshow(np.array(frames[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event\")\n",
    "\n",
    "ax2 = fig.add_subplot(152)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(real_truth),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(real_truth[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Real truth\")\n",
    "\n",
    "ax3 = fig.add_subplot(153)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(th_truth),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(th_truth[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Classical threshold\")\n",
    "\n",
    "ax4 = fig.add_subplot(154)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(edge_truth), axis=0))\n",
    "else: plt.imshow(np.array(edge_truth[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Edge truth\")\n",
    "\n",
    "ax5 = fig.add_subplot(155)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(final_truth), axis=0))\n",
    "else: plt.imshow(np.array(final_truth[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Combined truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate real data\n",
    "img_data_cut = img_data[-1023:,-1440:]/12\n",
    "img_data_torch = torch.tensor(img_data_cut).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "output_score = model(img_data_torch)\n",
    "prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the real data evaluation\n",
    "logscale = False\n",
    "view_row_low = -350\n",
    "view_row_high = -300\n",
    "view_col_low = -350\n",
    "view_col_high = -300\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "if(logscale):\n",
    "    plt.imshow(np.log(img_data_cut[view_row_low:view_row_high,view_col_low:view_col_high]),interpolation='none')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"log(counts)\")\n",
    "    plt.title(\"Scaled data (log counts)\")\n",
    "else:\n",
    "    plt.imshow(img_data_cut[view_row_low:view_row_high,view_col_low:view_col_high],interpolation='none') #np.log(frame))\n",
    "    cbar = plt.colorbar()\n",
    "    plt.title(\"Scaled data\")\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.imshow(prob[view_row_low:view_row_high,view_col_low:view_col_high],interpolation='none')\n",
    "plt.title(\"U-net output\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/1035340/reading-binary-file-and-looping-over-each-byte\n",
    "def bytes_from_file(filename, chunksize=4):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunksize)\n",
    "            if chunk:\n",
    "                yield struct.unpack('@I', chunk)[0]\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datfile = \"/home/jrenner/local/data/electronsim/stack_1.dat\"\n",
    "freader = iter(bytes_from_file(datfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for i in range(5760*4092):\n",
    "    img.append(next(freader))\n",
    "img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for input to a NN\n",
    "img_data = img.reshape([4092,5760])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(np.log(img.reshape([5760,4092])),vmin=9.5,vmax=10.5)\n",
    "#plt.imshow(img.reshape([5760,4092])[-100:,0:100],vmin=750,vmax=10000)\n",
    "plt.imshow(img_data,interpolation='none',vmin=750,vmax=15000)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to fit the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_th = 750*12\n",
    "max_th = 751*12\n",
    "fit_img = np.copy(img_data)\n",
    "fit_img[fit_img < noise_th] = 0\n",
    "fit_img[fit_img >= noise_th] = max_th\n",
    "fit_img = fit_img/np.max(fit_img)\n",
    "fit_img = np.array(fit_img,dtype=np.uint8)\n",
    "print(\"Min value:\",np.min(fit_img),\"; max value:\",np.max(fit_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(fit_img,interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = fit_img\n",
    "th = 0.5\n",
    "nrows = A.shape[0]\n",
    "ncols = A.shape[1]\n",
    "ncts = np.sum(A >= th)\n",
    "nzeros = np.sum(A < th)\n",
    "wcts = nzeros/ncts\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "\n",
    "def count_loss(x):\n",
    "    m,b = x\n",
    "    \n",
    "    # The loss L is:\n",
    "    #\n",
    "    # (number of 0s in the dark region) - wcts*(number of 1s in the dark region)\n",
    "    # + wcts*(number of 1s in the light region) - (number of 0s in the dark region)\n",
    "    # \n",
    "    # where wcts is the count weight, determined such that the number of counts multiplied by wcts is equal to\n",
    "    # the number of zeros.\n",
    "    L = 0\n",
    "    L1 = np.sum((irows < m*icols + b) & (A < th))\n",
    "    L2 = np.sum((irows < m*icols + b) & (A >= th))\n",
    "    L3 = np.sum((irows >= m*icols + b) & (A >= th))\n",
    "    L4 = np.sum((irows >= m*icols + b) & (A < th))\n",
    "    \n",
    "    L = L1 - wcts*L2 + wcts*L3 - L4\n",
    "    print(\"Loss is:\",-L,\"with L1 =\",L1,\"L2 =\",L2,\"L3 =\",L3,\"L4 =\",L4)\n",
    "    return -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_guess = [-nrows/ncols,nrows]\n",
    "result = optimize.minimize(count_loss,initial_guess,method='Nelder-Mead',tol=1e-6)\n",
    "m,b = result.x\n",
    "Lmin = result.fun\n",
    "print(\"m = \",m,\"b = \",b,\"Lmin=\",Lmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(fit_img,interpolation='none')\n",
    "xfit = np.arange(ncols)\n",
    "yfit = m*xfit + b\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss in a range near the parameters.\n",
    "mrng = np.arange(m-0.1*m, m+0.1*m, 0.2*m/100)\n",
    "Lrng = np.array([count_loss([mval,b])/Lmin for mval in mrng])\n",
    "print(mrng)\n",
    "plt.plot(mrng,Lrng)\n",
    "plt.xlabel(\"Parameter m\")\n",
    "plt.ylabel(\"Relative loss L/L(m$_0$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.1):\n",
    "    v = 255/2. #np.median(image)\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    print(\"Lower =\",lower,\", upper=\",upper)\n",
    "    return cv2.Canny(image, lower, upper)\n",
    "\n",
    "edges = auto_canny(image=fit_img) \n",
    "\n",
    "# Show images for testing\n",
    "#cv2.imshow('edges', edges)\n",
    "plt.imshow(edges)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the noise peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fscale = 12.\n",
    "\n",
    "def gauss(x, amp, mu, sigma):\n",
    "    if sigma <= 0.:\n",
    "        return np.inf\n",
    "    return amp/(2*np.pi)**(0.5)/sigma * np.exp(-0.5*(x-mu)**2./sigma**2)\n",
    "\n",
    "def gaussexpo(x, amp, mu, sigma, const, mean, x0):\n",
    "    if sigma <= 0.:\n",
    "        return np.inf\n",
    "    return amp/(2*np.pi)**(0.5)/sigma * np.exp(-0.5*(x-mu)**2./sigma**2) + const * np.exp(-(x-x0)/mean)\n",
    "\n",
    "\n",
    "yh, xh, _ = plt.hist(img[(img/fscale > 7000/fscale) & (img/fscale < 9300/fscale)]/fscale,bins=50)\n",
    "xh = (xh[1:] + xh[0:-1])/2\n",
    "\n",
    "#popt, pcov = curve_fit(gaussexpo, xh, yh, [3.0e6, 8200, 300, 1000, 10, -1])\n",
    "popt, pcov = curve_fit(gauss, xh, yh, [3.0e6, 8200/fscale, 135/fscale])\n",
    "xfit = np.linspace(xh[0],xh[-1],100)\n",
    "plt.plot(xfit,gauss(xfit,*popt))\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "print(\"Fit mean:\",popt[1])\n",
    "print(\"Fit sigma:\",popt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = np.load(\"frame_4855x4855_11occ.npz\")\n",
    "img_sim = fn['frame'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(img[(img > 0) & (img < 10000)],bins=50)\n",
    "plt.hist(img_sim,bins=50,range=[0,10000],label='MC')\n",
    "plt.hist(img/12,bins=50,range=[0,10000],label='data')\n",
    "\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "print(\"Total pixels:\",len(img))\n",
    "print(\"Counts near peak\",np.sum(img[(img > 29) & (img < 33)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "177383690/935130034."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(np.random.normal(loc=0,scale=50,size=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a network (single-electrons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeldir = '/home/jrenner/local/jerenner/emsim/models'\\\n",
    "modeldir = '/home/jrenner/temp/nersc'\n",
    "lrate       = 1e-3   # Learning rate to use in the training.\n",
    "load_model  = False   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 2000    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/run_11x11_chi32_60/model_init_599.pt\".format(modeldir)\n",
    "\n",
    "# Create the datasets.\n",
    "dataset_all   = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,add_shift=0)\n",
    "dataset_train = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,nstart=0,nend=-20000,add_shift=0)\n",
    "dataset_val   = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,nstart=-20000,add_shift=0)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=1000, shuffle=True, collate_fn=tr.my_collate, num_workers=8)\n",
    "val_loader = DataLoader(dataset_val, batch_size=1000, shuffle=True, collate_fn=tr.my_collate, num_workers=8)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=15, shuffle=True, collate_fn=tr.my_collate, num_workers=4)\n",
    "#test_loader = DataLoader(dataset_test, batch_size=15, shuffle=True, collate_fn=tr.my_collate, num_workers=4)\n",
    "\n",
    "# Define the model.\n",
    "#model = emnet.FCNet()\n",
    "model = emnet.basicCNN()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01, amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "#print(\"Training with weights\",sort_clsweights)\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    tr.train(model, epoch, train_loader, optimizer)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = tr.val(model, epoch, val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "#     if(epoch % 50 == 0):\n",
    "#         torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"{}/model_short_training_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses.\n",
    "tloss = np.loadtxt(\"/home/jrenner/temp/nersc/run_11x11_chi32_60_val2_trainsched/train.txt\")\n",
    "vloss = np.loadtxt(\"/home/jrenner/temp/nersc/run_11x11_chi32_60_val2_trainsched/val.txt\")\n",
    "#vloss = np.loadtxt(\"/home/jrenner/local/jerenner/emsim/val.txt\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.plot(tloss[:,0],tloss[:,1],label='training')\n",
    "plt.plot(vloss[:,0],vloss[:,1],label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.plot(tloss[:,0],tloss[:,2],label='training')\n",
    "plt.plot(vloss[:,0],vloss[:,2],label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all events from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,add_shift=0)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evts = np.arange(100000,110000)\n",
    "df, evts = emsim_utils.construct_evt_dataframe(dset,evts,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = df['xc_3x3'].values\n",
    "yvals = df['yc_3x3'].values\n",
    "rngval = 0.002\n",
    "plt.hist2d(xvals,yvals,bins=10,range=[[-rngval,rngval],[-rngval,rngval]])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = pd.plotting.scatter_matrix(df[[\"error_r_NN\",\"error_r_maxpt\",\"error_r_3x3\",\"error_r_3x3_th\",\"error_r_5x5\",\"error_r_5x5_th\"]], \n",
    "                                  figsize=[15,15], alpha=0.2, hist_kwds={'bins':100})\n",
    "for i, axs in enumerate(axes):\n",
    "    for j, ax in enumerate(axs):\n",
    "        #if i == j:  # only the histograms\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlim(0,0.01)\n",
    "        ax.set_ylim(0,0.01)\n",
    "            \n",
    "plt.savefig(\"errors_scatter_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_cut = 0.1\n",
    "rng_cut = 0.005\n",
    "sigma_cut = 1e9\n",
    "nbins = 50\n",
    "\n",
    "df_plt_NN  = df[(df.error_r_3x3 < err_cut)] # & (df.sigma_r_NN < sigma_cut)]\n",
    "df_plt_3x3 = df[(df.error_r_3x3 < err_cut)] # & (df.sigma_r_NN < sigma_cut)]\n",
    "\n",
    "plt.hist(df_plt_NN.error_r_NN,range=(0,rng_cut),alpha=0.8,bins=nbins,color='blue',label='NN error')\n",
    "plt.hist(df_plt_3x3.error_r_3x3,range=(0,rng_cut),alpha=0.8,bins=nbins,color='green',label='3x3 centroid')\n",
    "plt.xlabel(\"error $\\sqrt{\\Delta x^2 + \\Delta y^2}$ (mm)\")\n",
    "plt.ylabel(\"counts/bin\")\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "print(\"NN events:\",len(df_plt_NN[df_plt_NN.error_r_NN < rng_cut]))\n",
    "print(\"3x3 events:\",len(df_plt_3x3[df_plt_3x3.error_r_3x3 < rng_cut]))\n",
    "print(\"Mean NN error:\",np.mean(df_plt_NN[df_plt_NN.error_r_NN < rng_cut].error_r_NN))\n",
    "print(\"Mean 3x3 error:\",np.mean(df_plt_3x3[df_plt_3x3.error_r_3x3 < rng_cut].error_r_3x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean 3x3 error:\",df[(df.error_r_3x3 < err_cut) & (df.sigma_r_NN < sigma_cut)].error_r_3x3.mean())\n",
    "print(\"Mean NN error: \",df[(df.error_r_NN < err_cut) & (df.sigma_r_NN < sigma_cut)].error_r_NN.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"error_r_diff\"] = df.error_r_NN - df.error_r_3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_diff_cut = 0.005\n",
    "plt.hist(df[(df.error_r_diff < err_diff_cut) & (df.error_r_diff > -err_diff_cut) & (df.sigma_r_NN < 0.011)].error_r_diff,alpha=0.8,bins=50,color='blue',label='NN error')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"error difference (NN - 3x3-method)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.x_true > 0.02][['event','x_true']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe and event arrays.\n",
    "df.to_pickle(\"evts_80000_to_90000.pkl\")\n",
    "np.savez(\"evt_arrays.npz\",evt_arrays=l_evt_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"evts_80000_to_90000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sigma = 0.011\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(15.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.hist(df[df.sigma_r_NN < cut_sigma].error_r_NN,bins=50)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"error $\\sqrt{\\Delta x^2 + \\Delta y^2}$ (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.hist(df[df.sigma_r_NN < cut_sigma].sigma_r_NN,bins=50)\n",
    "plt.xlabel(\"$\\sqrt{\\sigma_x^2 + \\sigma_y^2}$ of probability distribution (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff, mean_err = [], []\n",
    "cut_sigmas = np.arange(0.003,0.4,0.0005)\n",
    "for cut_sigma in cut_sigmas:\n",
    "    df_cut = df[df.sigma_r_NN < cut_sigma]\n",
    "    \n",
    "    eff.append(len(df_cut)/len(df))\n",
    "    mean_err.append(df_cut.error_r_NN.mean())\n",
    "    \n",
    "    print(\"[SIGMA = {}]: EFF = {}, ERR = {}\".format(cut_sigma,len(df_cut)/len(df),df_cut.error_r_NN.mean()))\n",
    "\n",
    "eff = np.array(eff)\n",
    "mean_err = np.array(mean_err)\n",
    "plt.plot(mean_err,eff,'.-')\n",
    "plt.xlabel(\"Mean error (mm)\")\n",
    "plt.ylabel(\"Efficiency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the net for individual events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,add_shift=0)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 80388\n",
    "evt_item = dset[evt_plt]\n",
    "evt_arr = evt_item[0]\n",
    "evt_lbl = evt_item[1]\n",
    "evt_err_ind = evt_item[2]\n",
    "\n",
    "SHIFTED_ERR_RANGE_MIN = emnet.PIXEL_ERR_RANGE_MIN # - dset.add_shift*emnet.PIXEL_SIZE\n",
    "SHIFTED_ERR_RANGE_MAX = emnet.PIXEL_ERR_RANGE_MAX # + dset.add_shift*emnet.PIXEL_SIZE\n",
    "#ERR_PIXEL_SIZE = emnet.PIXEL_SIZE*(2*dset.add_shift+1)/emnet.ERR_SIZE\n",
    "ERR_PIXEL_SIZE = (emnet.PIXEL_ERR_RANGE_MAX - emnet.PIXEL_ERR_RANGE_MIN)/emnet.ERR_SIZE\n",
    "print(\"Error grid pixel size is {}\".format(ERR_PIXEL_SIZE))\n",
    "\n",
    "x_errgrid = np.arange(0,emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "y_errgrid = np.arange(0,emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "print(x_errgrid)\n",
    "\n",
    "xbin = int(emnet.ERR_SIZE*(evt_lbl[0] - SHIFTED_ERR_RANGE_MIN)/(SHIFTED_ERR_RANGE_MAX - SHIFTED_ERR_RANGE_MIN))\n",
    "xbin = max(xbin,0)\n",
    "xbin = min(xbin,emnet.ERR_SIZE-1)\n",
    "\n",
    "ybin = int(emnet.ERR_SIZE*(evt_lbl[1] - SHIFTED_ERR_RANGE_MIN)/(SHIFTED_ERR_RANGE_MAX - SHIFTED_ERR_RANGE_MIN))\n",
    "ybin = max(ybin,0)\n",
    "ybin = min(ybin,emnet.ERR_SIZE-1)\n",
    "\n",
    "print(\"Computed index:\",(ybin*emnet.ERR_SIZE) + xbin,\"for max added shift:\",dset.add_shift)\n",
    "\n",
    "# Send through the model.\n",
    "data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "target = torch.tensor(np.array(evt_err_ind)).long().cuda()\n",
    "output_score = model(data)\n",
    "\n",
    "# Compute the predicted pixel and (x,y) values.\n",
    "prob = np.array(softmax(output_score).cpu().detach().numpy()).reshape([emnet.ERR_SIZE,emnet.ERR_SIZE])\n",
    "ipred = np.argmax(prob)\n",
    "xpred = int(ipred % emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "ypred = int(ipred / emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "#print(\"[Evt\",evt,\"]: Index is\",evt_err_ind,\"with predicted\",ipred,\"; x = {} (predicted {}), y = {} (predicted {})\".format(evt_lbl[0],xpred,evt_lbl[1],ypred))\n",
    "\n",
    "# Compute the sigmas of the distribution.\n",
    "sigma_x0, sigma_y0 = emsim_utils.compute_sigmas(prob,ERR_PIXEL_SIZE,SHIFTED_ERR_RANGE_MIN)\n",
    "popt, pcov = emsim_utils.fit_sigmas(prob,x_errgrid,y_errgrid,xpred,ypred,sigma_x0,sigma_y0,ERR_PIXEL_SIZE)\n",
    "fit_data = emsim_utils.mult_gaussFun_Fit((x_errgrid,y_errgrid),*popt).reshape([emnet.ERR_SIZE,emnet.ERR_SIZE])\n",
    "print(\"Gaussian fit parameters A*exp(-0.5*((x-x0)**2/varX + (y-y0)**2/varY)) + C:\")\n",
    "print(\"A = {}\".format(popt[0]))\n",
    "print(\"(x0, y0) = ({},{})\".format(popt[1],popt[2]))\n",
    "print(\"(sigma_x, sigma_y) = ({},{})\".format(popt[3]**0.5,popt[4]**0.5))\n",
    "print(\"C = {}\".format(popt[5]))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_arr)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event {}; shift ({:.3e},{:.3e}); index {}\".format(evt_plt,evt_lbl[0],evt_lbl[1],evt_err_ind))\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot([xbin],[ybin],color='red',marker='o',markersize=10)\n",
    "plt.imshow(prob)\n",
    "plt.colorbar()\n",
    "plt.title(\"Incidence point within prediction grid\")\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "plt.imshow(fit_data)\n",
    "plt.colorbar()\n",
    "plt.title(\"2D Gaussian fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at many events:\n",
    "xpred_err, ypred_err = [], []\n",
    "for evt_plt in np.arange(8000,9999):\n",
    "    \n",
    "    evt_item = dset[evt_plt]\n",
    "    evt_arr = evt_item[0]\n",
    "    evt_lbl = evt_item[1]\n",
    "    evt_err_ind = evt_item[2]\n",
    "\n",
    "    # Send through the model.\n",
    "    data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    target = torch.tensor(np.array(evt_err_ind)).long().cuda()\n",
    "\n",
    "    output_score = model(data)\n",
    "    prob = np.argmax(np.array(softmax(output_score).cpu().detach().numpy()).reshape([10,10]))\n",
    "    xpred = (prob % tr.ERR_SIZE)*0.005/tr.ERR_SIZE + tr.ERR_RANGE_MIN + 0.005/tr.ERR_SIZE/2\n",
    "    ypred = (prob / tr.ERR_SIZE)*0.005/tr.ERR_SIZE + tr.ERR_RANGE_MIN + 0.005/tr.ERR_SIZE/2\n",
    "    print(\"[Evt\",evt_plt,\"]: Index is\",evt_err_ind,\"with predicted\",prob,\"; x = {} (predicted {}), y = {} (predicted {})\".format(evt_lbl[0],xpred,evt_lbl[1],ypred))\n",
    "    \n",
    "    xpred_err.append(xpred-evt_lbl[0])\n",
    "    ypred_err.append(ypred-evt_lbl[1])\n",
    "xpred_err = np.array(xpred_err)\n",
    "ypred_err = np.array(ypred_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(15.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.hist(xpred_err)\n",
    "plt.xlabel(\"error in x-prediction (mm)\")\n",
    "print(np.where(abs(xpred_err) > 0.001))\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.hist(ypred_err)\n",
    "plt.xlabel(\"error in y-prediction (mm)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For debugging the 3x3 sum operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.ones([6,6])\n",
    "aa[0,2] = 4\n",
    "aa[1,2] = 2\n",
    "aa[3,2] = 8\n",
    "aa[4,2] = -2\n",
    "aa[3,1] = 5\n",
    "aa[5,0] = 10\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_init   = np.unravel_index(aa.argmax(),aa.shape)\n",
    "nbsum_init = tr.sum_neighbors(aa,max_init,remove=True)\n",
    "print(\"Max at\",max_init,\"and neighbor sum\",nbsum_init)\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a dataset for noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nevts = 1000\n",
    "noise_arr = np.arange(0.,100.,50.)\n",
    "r_mean, r_sigma = [], []\n",
    "for noise in noise_arr:\n",
    "    print(\"Running for noise\",noise)\n",
    "    dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",0,noise)\n",
    "    \n",
    "    shifts_x, shifts_y, shifts_r = [], [], []\n",
    "    for evt in range(Nevts):\n",
    "        evt_arr,evt_lbl = dset[evt]\n",
    "        xs,ys = evt_lbl[0],evt_lbl[1]\n",
    "        shifts_x.append(xs)\n",
    "        shifts_y.append(ys)\n",
    "        shifts_r.append((xs**2 + ys**2)**0.5)\n",
    "    \n",
    "    shifts_r = np.array(shifts_r)\n",
    "    r_mean.append(np.mean(shifts_r))\n",
    "    r_sigma.append(np.std(shifts_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(noise_arr,r_mean,yerr=np.array(r_sigma)/Nevts**0.5)\n",
    "plt.xlabel(\"$\\sigma$ noise (electrons)\")\n",
    "plt.ylabel(\"r-error (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a dataset and examine individual events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_shift=10,add_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 8\n",
    "evt_item = dset[evt_plt]\n",
    "evt_arr = evt_item[0]\n",
    "evt_lbl = evt_item[1]\n",
    "evt_err_ind = evt_item[2]\n",
    "plt.imshow(evt_arr)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event {}; shift {}; index {}\".format(evt_plt,evt_lbl,evt_err_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_x, shifts_y, shifts_r = [], [], []\n",
    "for evt in range(1000):\n",
    "    evt_arr,evt_lbl,evt_err_ind = dset[evt]\n",
    "    xs,ys = evt_lbl[0],evt_lbl[1]\n",
    "    shifts_x.append(xs)\n",
    "    shifts_y.append(ys)\n",
    "    shifts_r.append((xs**2 + ys**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(shifts_r,bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot events directly from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"dataframes/EM_4um_back_10M_300keV.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 97\n",
    "evt_arr = np.zeros([101,101])\n",
    "df_evt = df[df.event == evt_plt]\n",
    "for row,col,counts in zip(df_evt['row'].values,df_evt['col'].values,df_evt['counts'].values):\n",
    "    evt_arr[row,col] += counts\n",
    "plt.imshow(np.log(0.1 + evt_arr))\n",
    "plt.colorbar()\n",
    "plt.title(\"Event {}; max at {}\".format(evt_plt,np.unravel_index(evt_arr.argmax(),evt_arr.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
