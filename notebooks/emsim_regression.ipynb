{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import training as tr\n",
    "import emnet\n",
    "import emsim_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import cv2\n",
    "\n",
    "from unet import UNet\n",
    "import scipy.optimize as optimize\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression net with line events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset for events from 20000 to 178917\n",
      "Created dataset for events from 0 to 20000\n"
     ]
    }
   ],
   "source": [
    "modeldir = '/home/jrenner/local/jerenner/emsim/models/regression_line'\n",
    "lrate       = 1e-3   # Learning rate to use in the training.\n",
    "load_model  = True   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 300    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/save/rl_L_exp0pt5_299.pt\".format(modeldir)\n",
    "\n",
    "ltest = True\n",
    "line_m = -2\n",
    "line_b = 15\n",
    "\n",
    "# \"Real-data-like\" dataset: occupancy 11, noise_mean=683, noise_sigma=11.2\n",
    "dset_train = tr.EMDataset(\"../dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=-1,nstart=20000, Ltest=ltest)\n",
    "dset_val = tr.EMDataset(\"../dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=-1,nstart=0,nend=20000, Ltest=ltest)\n",
    "\n",
    "dataset_train = tr.EMFrameDataset(dset_train,frame_size=11,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=15.0,res_factor=1,lside=-1)\n",
    "dataset_val = tr.EMFrameDataset(dset_val,frame_size=11,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=15.0,res_factor=1,lside=-1)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=50, shuffle=False, collate_fn=tr.my_collate_reg_line, num_workers=1)\n",
    "\n",
    "# Define the model.\n",
    "model = emnet.basicCNN_reg()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=lrate, weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20)\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    for m in model.modules():\n",
    "        for child in m.children():\n",
    "            if type(child) == nn.BatchNorm2d:\n",
    "                child.track_running_stats = False\n",
    "                child.running_mean = None\n",
    "                child.running_var = None\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network (regression with line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    train_loss = tr.train_regression_line(model, epoch, train_loader, optimizer, line_m, line_b, 50)\n",
    "    scheduler.step(train_loss)\n",
    "    #if(epoch % 50 == 0):\n",
    "    torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy.\n",
    "tloss = np.loadtxt(\"train.txt\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.plot(tloss[:,0],tloss[:,1],label='total loss')\n",
    "plt.plot(tloss[:,0],tloss[:,2],label='vector loss')\n",
    "plt.plot(tloss[:,0],tloss[:,3],label='distance loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.plot(tloss[:,0],tloss[:,4],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Light side is 1\n",
      "True point ( 3.5 , 5.5 )\n",
      "Line drawn: m = -2, b = 15\n",
      "True distance to the line: 1.118033988749895\n",
      "Pred point ( 3.20965975522995 , 4.785636126995087 )\n",
      "Pred vector ( -0.29034024 , -0.7143639 )\n",
      "Predicted distance to the line: 1.6971954344556013\n",
      "TEST LOSS\n",
      "-- Test vector loss: tensor(0., device='cuda:0', dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "-- Test distance loss: tensor(0.4280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "COMPARE LOSS\n",
      "-- Compare vector loss: 0.5946132007877125\n",
      "-- Compare distance loss: 0.1831965910986039\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+cAAAEXCAYAAADP8zWMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAA+6UlEQVR4nO3deZhlVX3v//e3B+ZBoFHmIREH4gBJCypGjcikBryJA6JeNN4Q8xOHaxIvmqtGEnONufFqrtzEVhGjDCGoScegDApkUJBGCQqItozN3IBAAzbdVd/fH3tX9emihlNVu87a55z363nOU2fYZ+91CvpTa53vXmtHZiJJkiRJkspZVLoBkiRJkiQNOwfnkiRJkiQV5uBckiRJkqTCHJxLkiRJklSYg3NJkiRJkgpzcC5JkiRJUmEOzjVnEfHSiFizgPu/OSJevlD7lyRJkqS2cHA+iXpQ+FhErOu4fXqBj7mgA922i4gzIuLPFnD/GRE/jIhFHc/9WUScUd/fr97m/Anv+3JE/MlCtUuSJEmSwMH5dH4zM7fruJ1cukGatz2A42fY5tCIeGEvGiNJkiRJYxycz0JEbBkRP4+IZ3U8t2tdZX9y/fhVEXF1vd13IuI5HdveHBF/GBHXRMSDEfH3EbFVRGwLfAPYo6NSv0cX7fkfEXF7RDwcETdExOH184si4pSI+FlE3BcR50bEzh3ve37dtp9HxH9GxEs7Xrs0Iv40Iv6j3u+FEbFshnZ8ICLW1p/vjfVzz4uIuyNiccd2vxUR/znJ+08C3gi8r/7s/9zx8kETf18d75vydz2FjwMfiYglM2zz0Rn2I0mSJEmNcnA+C5m5Hvgq8IaOp18HXJaZ90TEwcDpwO8BuwCfAVZGxJYTtj8a2B94DvCWzHwEOAa4o6NSf8d0bYmIpwMnA8/LzO2Bo4Cb65ffCbwaeAlVtfgB4LT6fXsC/wL8GbAz8IfAVyJi147dnwC8FXgysEW9zVR2A5YBewInAisi4umZeSVwH3Bkx7ZvBv5u4g4ycwVwJvDx+rP/ZsfLT/h91Z+jm9/1RF8FHhrbxxT+H/A057pLkiRJ6iUH51P7x7oiO3b73fr5s9j81OgT6ucATgI+k5lXZOZIZn4RWA88v2P7v87MOzLzfuCfgYPm2L4RYEvgwIhYmpk3Z+bP6tfeDvxxZq6pv1D4E+A1dcX4TcD5mXl+Zo5m5kXAKuAVHfv+Qmb+JDMfA87too0fzMz1mXkZ1cD/dfXzX6yPR125P4pNv6tuTfX76uZ3PVECHwQ+GBFbTLHNY1SV8wWb/y5JkiRJEzk4n9qrM/NJHbfP1s9fAmwTEYdGxH5Ug8Wv1a/tC/xB56Ae2Juqej3mro77jwLbzaVxmbkaeA/VwPueiDin41T4fYGvdbTheqrB/FPq1147oY0vAnafYxsfqCv/Y25h0+f9MvCb9Wn7rwP+LTPvnOVHnaot3fyunyAzzwfWUFXcp/I54CkR8ZvTbCNJkiRJjXFwPkuZOUJVTX5Dfft6Zj5cv3wb8NEJg/ptMvPsbnY9h7aclZkvohqoJvAXHe04ZkI7tsrM2+vXvjThtW0z82OzPX5tp3rwPWYf4I66fbcD3wV+i+qU9i9N93Fmedz5/K7/GPgAsM2kDcl8HPgI8KdAzLJdkiRJkjRrDs7n5izg9VSLmHWepv1Z4O11VT0iYtuIeGVEbN/FPu8GdomIHceeiOryapMOWiPi6RHxsnqO9S+oTscerV/+W+CjEbFvve2uEXFc/dpYNfuoiFhcL0j30ojYq/uP/wQfiYgtIuLXgVcB/9Dx2t8B7wOeTTXneyp3A780i2PO+XedmZcCP6KaIz+VLwFbUc13lyRJkqQF5eB8av8cm1/nfOzUdTLzCuARqlOov9Hx/Crgd4FPUy3CtprpFx8bl5k/Bs4GbqxP096D6jTt70zxli2BjwFrqU79fjLw/vq1TwErgQsj4mHgcuDQ+ji3AcdRVY7vpapA/xFz/3/hLqrPegfVom5vrz/LmK9Rn2afmY9Os5/PU82f/3lE/ONMB53P77r2P6kWxJtq/yPAh6bbRpIkSZKaEpmzPptaPRIRnwP+ITMvKN2W+YiInwG/l5kXl26LJEmSJLWRg3MtqIj4baq58E/LzNGZtpckSZKkYbSkdAM0uCLiUuBA4M0OzCVJkiRpalbOJUmSJEkqzAXhJEmSJEkqzMF5LSL+V0S8p3Q7mlRfIm1N6XaUEhFvjIgLG9jPzRHx8vr+OyPiL2Z6jyQtlIj4RkRMeSnIiDgjIv6sy33tFxEZEUu62bckDYKI+JOI+HLpdkgTOTinug448F+BzyzwcS6NiP+2kMfQJpl5ZmYe2fBuPwu8MSKe3PB+JfVI/YXbYxMul/npBT5mY1+WZuYxmfnFer9viYh/b2K/E/ctSf0uIk6IiFV1zt9ZfwH5oob2vdmXm1ITHJxX3gKcn5mPlW7IdCJicek2DLvM/AXVte3/a+m2SJqX38zM7TpuJ5dukCSpORHxXuCTwJ8DTwH2Af4fcFzBZo1zUK/JODivHANc1vlERBwXEVdHxEMR8bOIOLp+fseI+Hz97dvtEfFnY4PmsQpGRPzviHggIm6KiGPq1z4K/Drw6c4qTUQ8IyIuioj7I+KGiHhdRxvOiIi/iYjzI+IR4DciYo+I+EpE3Fvv/10d229dv+eBiLgOeN50HzoiXhgRV0bEg/XPF3a8dmlE/GlE/EdEPBwRF0bEsin289KIWBMR74uIe+rfzasj4hUR8ZP6s32gY/tDIuK7EfHzettPR8QWHW1aGxF714+fW3+eZ0xx7IyId0XEjfX7/jIiFnX+9+hmvxHxqvq/988j4jsR8ZxpfnWXAq+c7ncrqf9ExJZ1Bjyr47ld6yr7k+vHU2ZFXZH/w4i4ps7Vv4+IrSJiW6ov9faITZX6PSYce/96n2P59dmIuKfj9S9FPfWqzuf/FhHPBP4WeEG9z5937HKniPiXOr+viIhf7vJ3MH6G13R/0+rXp/x7KEklRcSOwKnAOzLzq5n5SGZuyMx/zsw/mrDtE85sis2nNB4SVfX9oYi4OyI+UW/2r/XPn9cZ/IJ6+9+JiOvr3LwgIvbt2G9GxDsi4qfATxfo46uPOTivPBu4YexBRBwC/B3wR8CTgBcDN9cvnwFsBJ4KHAwcCXSeqn5ova9lwMeBz0dEZOYfA/8GnDxWpak7bBcBZwFPBo4H/l9EHNixvxOAjwLbA98B/hn4T2BP4HDgPRFxVL3th4Ffrm9HAdPNSdwZ+Bfgr4FdgE8A/xIRu0w49lvrtm0B/OFU+wN2A7aq2/UhqtO/3wT8GtWXEh+MiP3rbUeA/17/jl5Qf47/DyAzv0M1veCLEbE18GXgg5n542mO/V+A5cCvUn0b+jsTN5huvxFxMHA68Hv17+IzwMqI2HKK410PPHea9kjqQ5m5Hvgq8IaOp18HXJaZ93SZFa8Djgb2B54DvCUzH6H6EviOjkr9HROOfRPwENXfFaj+7qyrB+AAL2HCl8iZeT3wduC79T6f1PHy8cBHgJ2A1VR/R+Zi0r9p9WtnMP3fQ0kq5QVU/dKvNbCvTwGfyswdqPrY59bPv7j++aQ6g78bEccBHwB+C9iVqu9/9oT9vZoqWw9EmsDBeeVJwMMdj98GnJ6ZF2XmaGbeXg/ingK8AnhP/Q3cPcD/oeoEjbklMz+bmSPAF4HdqU6lmcyrgJsz8wuZuTEzfwB8BXhtxzb/lJn/UV8n/NnArpl5amY+npk3Ug2Cx47/OuCjmXl/Zt5GNfCeyiuBn2bml+pjnw38GPjNjm2+kJk/qU/3Pxc4aJr9baiPvQE4h6oj96nMfDgzrwWuox7QZuZVmXl5fdybqTq4L+nY158AOwLfA24HTpvmuAB/UX/mW6lOX3rDFNtNtd+TgM9k5hWZOVLPt1wPPH+K/Txc70dS//rHulI9dvvd+vmz2DzTT6ifg+6y4q8z847MvJ/qy9SDZtGmy4CXRMRu9ePz6sf7AztQfTHbra9l5vcycyNw5izb0WnSv2ld/j2UpFJ2AdbWGThfG4CnRsSyzFyXmZdPs+3bgf+VmdfXx/5z4KDO6nn9+v1tn06rMpzrUHmAqjI9Zm/g/Em22xdYCty5qXDAIuC2jm3uGruTmY/W2203xXH3BQ6dcCriEuBLHY9vm7D9HhO2X0z1rRzAHhO2v2WK445tO/H1W6gq32Pu6rj/KFN/DoD76s4bwFjY3N3x+mNj74+Ip1FV6pcD21B95qvGNszMDRFxBtWXC+/NzJzmuPDEz7zHZBtNs999gRMj4p0dm28x1X6o/l95cIY2SWq3V2fmxZM8fwmwTUQcSpVhB7Gp8tJNVkzMzalyZDKXAccCa6hOl7wUeDPwC+Df6i9puzWb/O5qPxP+pu3MzH8PJamU+4BlEbGkgQH626hOkf9xRNwEfCQzvz7FtvsCn4qIv+p4Lqj612P9bnNSU3JwXrkGeBpwZf34NqrTVia6japKsmyO/9AnDjJvozpd8ogu33MbcFNmHjDFtndSfbFwbf14n2n2ewdVgHTaB/jmNO9pyt8APwDekJkP1/MoXzP2YkTsSXWK/heAv4qI59Wnm05l4me+Y7KNptnvbVRV/25P+3wms6tgSeoTmTkSEedSnYFzN/D1zBw7s2q2WbHZrrvY5jLgL6kG55cB/041p/wXTDilfZb7XQjz/XsoSQvpu1QZ9Wqqs5Cm8whVsQgYX4B517HHmflT4A31miC/BZxXTwOdLH/H/k6cOc3xSuW2+oCntVfOZ/PTqj8PvDUiDo+IRRGxZ0Q8IzPvBC6kGtjtUL/2yxHxkkn3+kR3A7/U8fjrwNMi4s0RsbS+Pa9jjuFE3wMejoj/EdXib4sj4lkRMbbw27nA+yNip4jYC3jnFPsZ+8xPi+oSE0si4vVUc1+m+iawSdtTza1cF9WCbL8/9kI9l/EMqv8Gb6P6wuFPZ9jfH9WfeW/g3cDfT9xghv1+Fnh7RBwalW0j4pURsf3E/dReQrW4k6TBdBbweuCNbDqlHWafFZ3uBnapFymaVN0BfIxqvY7LMvOh+n2/zdSD87uBvaJeVLNXGvh7KEkLJjMfpFoD6bSoFinepu5nHxMRH5+w+U+Areo8Xwr8T2B8LZGIeFNE7FqfvfTz+ulR4N76Z2ff/m+p+uK/Ur93x4jonK4qTcvBeeXvgFfUC4WRmd+jWgjt/1CdvnwZm6rM/5XqNMbrqE6HP49qDl43PgW8JqrVG/+6rsYcSTVH7w6q0wf/go5A6FSfNv4qqtMsbwLWAp9j0/znj1CdMnMTVafpS0/cy/i+7qv39QdUp/68D3hVZq7t8rPMxx9SzeN8mKqz2zmYfhfVAnQfrE87fyvVFyW/Ps3+/onqtPirqRa5+/wk20y538xcBfwu8Gmq/6arqS6v9wQRsRXVPEuvAyz1t3+Oza9zPr5oUGZeQVVJ2YOOL+JmkxUT1Ytang3cWM9xn+p098uopgnd1vE4gO9Psf23qc4cuisiepHfnebz91CSFlRm/hXwXqrB9r1UVe2TgX+csN2DVAsTf45qTaJHqM5gGnM0cG1ErKPqyx+fmY9l5qNUi23+R53rz8/Mr1H15c+JiIeAH1EtCCp1JWaezjscIuLPgXsy85Ol26LuRUQCB2Tm6h4d753A3pn5vl4cT5IkSdJwcHCuvtbrwbkkSZIkLYR5ndYeEUdHxA0RsToiTmmqUZKk7pnFklSWOSypCXOunNcrGf4EOIJqXsaVVKtvX9dc8yRJ0zGLJaksc1hSU+ZTOT8EWJ2ZN2bm48A5wHHNNEuS1CWzWJLKMoclNWI+1znfk2rVwzFrgEMnbhQRJwEnASxm8a9tww7zOKSk6fyCR3g818dc3nvUb2yb990/0tW2V12z/oLMPHoux1HjZsxic1jqrYd5YG1m7jrzlk/UbRabw60y6z7xttvErz3jqT29AqI0VG6+bQNr7x/puz7xfAbnXcnMFcAKgB1i5zw0Dl/oQ0pD64r81pzfu/b+Ea64YK+utl26+8+WzbRNRBxNdcmRxcDnMvNjE15/O/AOYARYB5w0dgpgRLyf6nr0I8C7MvOCWXwUTWAOS711cZ53y1zf220Wm8P9pzOLlz93q/zeBXsXbpE0uA456raZN5pC033i2ZjP4Px2oDNV9qqfk9SXkpEcbWRP9fy70+iYfxcRKyfMvzsrM/+23v5Y4BPA0RFxIHA88CtU15m+OCKelpndfYU5fMxi9V4mxJwKEppRM1lsDveUOSwNlOb6xLM1nznnVwIHRMT+EbEFVYivbKZZknotgVGyq1sXZpx/l5kPdTzctm4C9XbnZOb6zLwJWF3vT5Mzi9VTu+c6VnARL8m5VyU0tW6zuAvmcO+Yw9IAabhPPCtzrpxn5saIOBm4gOp0qdMz89rGWiap50bp+lvCZRGxquPxivp0vTHdzr97B/BeYAvgZR3vvXzCe/fstmHDxixWr53Aj9mfh3ged3EZnpa7ELrMYnO4JcxhafDMok/cqHnNOc/M84HzG2qLpIKSZEP3p/Cszczl8z5m5mnAaRFxAvA/gRPnu89hZBarV3bPdRzBLYwQnMUzSzdnIM0ii83hFjGHpcExyz5xoxZ8QThJ/SGBkeZOz5nt/LtzgL+Z43sl9cgJ/JjFJBewL3fEdqWbM5AazGJzWJLmoOE+8azMZ865pAHT4PyaGeffRcQBHQ9fCfy0vr8SOD4itoyI/YEDgO/N+8NJmher5r1jDktSWX0351zSYElgJJsJmanm30XEqcCqzFwJnBwRLwc2AA9Qn0pZb3cucB2wEXiHKwRL5Y1VzS+0ar6gmspic1iS5qbJPvFsOTiXNK7J2TWTzb/LzA913H/3NO/9KPDRBpsjaR46q+ZnWjVfcE1lsTksSXNTZsa5g3NJtSSLza+R1G7ONe8ds1iSyiqZww7OJQGQCRvsD0qawLnmvWUWS1JZJXPYwbmkWjBClG6EpJZxrnmvmcWSVFa5HHZwLgmoFr8YtVojqYNzzXvPLJakskrmsINzSeOs1kjq5FzzMsxiSSrLyrmkohI7hJI2ca55GWaxJJVVMocdnEsCqiDakItKN0NSS1g1L8MslqSySuawg3NJACTBCHYIJVk1L8kslqSySuawg3NJ40bTUyklWTUvzSyWpLJK5bCDc0mA8xwlVayal2UWS1JZzjmX1ALBiPMcpaHndc1LM4slqazmcjging78fcdTvwR8KDM/Odn2Ds4lAfU1HZ3nKA01r2tenlksSWU1mcOZeQNwEEBELAZuB7421fYOziUBkBk8notLN0NSQc41L88slqSyFjCHDwd+lpm3TLWBg3NJ40ad5ygNLeeat4dZLEllLVAOHw+cPd0GDs4lAWOLX3gqpTSsrJq3g1ksSWXNMoeXRcSqjscrMnPFxI0iYgvgWOD90+3MwbmkmosQScPKqnmbmMWSVNascnhtZi7vYrtjgO9n5t3TbeTgXBLgIkTSMLNq3h5msSSVtUA5/AZmOKUdHJxL6jCSznOUhs1uVs1bxyyWpLKazOGI2BY4Avi9mbZ1cC4JgCTYkEaCNGysmreLWSxJZTWdw5n5CLBLN9ua/pIAFyGShtFuuY4jva55q5jFklRWyRx2cC4JqL4l9FRKabiMVc2/yX7cadW8FcxiSSqrZA47OJc0zkWIpOHRWTU/i2eUbo46mMWSVFapHHZwLgmATLx8jzRErJq3k1ksSWWVzGEH55KAscUvFpduhqQesGreXmaxJJVVMocdnEsa5yJE0nCwat5uZrEkleWCcJKKSoJRFyGSBt7uVs1bzSyWpLJK5rCDc0njrNZIg6/zuuZWzdvJLJakskrl8JyPGhF7R8QlEXFdRFwbEe9usmGSeiuB0VzU1a0bEXF0RNwQEasj4pRJXn9vnR/XRMS3ImLfjtdGIuLq+rayuU85eMxizcbuuY4jvK55q3Wbxd0wh3vDHJYGS9N94tmYT+V8I/AHmfn9iNgeuCoiLsrM6xpqm6SeCkZo5hSeiFgMnAYcAawBroyIlRPy4QfA8sx8NCJ+H/g48Pr6tccy86BGGjP4zGJ1zbnm/aCZLDaHe8oclgZKc33i2Zrz4Dwz7wTurO8/HBHXA3sCBpHUhxKaXJnyEGB1Zt4IEBHnAMfRkQ+ZeUnH9pcDb2rq4MPELFa3OqvmzjVvrwaz2BzuEXNYGiwN94lnpZFafETsBxwMXDHJaydFxKqIWLWB9U0cTtICyIzZnMKzbOzfdX07acLu9gRu63i8pn5uKm8DvtHxeKt6v5dHxKub+HzDYKosNocFm6rmF7OPVfMW6zaLMYdbqds+8b33jfS8bZK6M8s+caPmvSBcRGwHfAV4T2Y+NPH1zFwBrADYIXbO+R5P0sIZ6T5k1mbm8iaOGRFvApYDL+l4et/MvD0ifgn4dkT8MDN/1sTxBtV0WWwOy7nm/aXLLDaHW2Y2feLlz93KLJZabBZ94kbNa3AeEUupQujMzPxqM02SVEICo83Nr7kd2Lvj8V71c5uJiJcDfwy8JDPHS7qZeXv988aIuJSqCmGncApmsWbiXPP+0WAWm8M9ZA5Lg6PhPvGszGe19gA+D1yfmZ9orkmSyghGclFXty5cCRwQEftHxBbA8cBmq/1GxMHAZ4BjM/Oejud3iogt6/vLgMNw3t6UzGLNxLnm/aa7LO6COdwj5rA0aBrtE8/KfCrnhwFvBn4YEVfXz30gM8+fd6sk9Vx12YhmviXMzI0RcTJwAbAYOD0zr42IU4FVmbkS+EtgO+Afqn4Nt2bmscAzgc9ExCjVF4gfc8XbaZnFmpbXNe8vTWWxOdxT5rA0QJrsE8/WfFZr/3coVO+X1LgkGl2Zsu6UnD/huQ913H/5FO/7DvDsxhoy4MxiTWfzqrlzzftBk1lsDveGOSwNlqb7xLMx7wXhJA2O0WYu4CCpJTqr5ndYNe8bZrEklVUqhx2cSwIgE0YKncIjqXm7WTXvS2axJJVVMocdnEsaV2p+jaTmWTXvX2axJJXVd3POJQ2WJBgtdE1HSc3aLddxpFXzvmQWS1JZJXPYwbkkoFqZcoMdQmkgvNGqed8yiyWprKZzOCKeBHwOeFa9+9/JzO9Otq2Dc0k1qzXSIHCF9n5nFktSWY3n8KeAb2bmayJiC2CbqTZ0cC5p3KhXgpH6nnPN+59ZLEllNZXDEbEj8GLgLQCZ+Tjw+FTbOziXBLhCsDQIrJr3P7NYksqaZQ4vi4hVHY9XZOaKjsf7A/cCX4iI5wJXAe/OzEcm25mDc0njPJVS6m9WzQeDWSxJZc0ih9dm5vJpXl8C/Crwzsy8IiI+BZwCfHCqjSWpXpnSao3Ur6yaDwazWJLKajiH1wBrMvOK+vF5VIPzSTk4lwRUS0dutFoj9S2r5oPBLJaksprM4cy8KyJui4inZ+YNwOHAdVNt7+Bc0jhPpZT6k1XzwWIWS1JZDefwO4Ez65XabwTeOtWGDs4lVdJTKaV+ZdV8gJjFklRWwzmcmVcD081LH+fgvE/8l+vubXR/f3X1EY3u75dOuLrR/an3Ei/fo/KazLphyTmr5oPFLJamd8wrTmh0f/H4xkb3d/7F5za6P/VeyRx2cC5pnNUaqf9YNR88ZrEklVUqhx2cSwLqbwntEEp9xar54DGLJamskjns4FwSUF02YuOoixBJ/cSq+eAxiyWprJI57OBc0jjnOUr9w6r54DKLJaks55xLKis9lVLqJ1bNB5RZLEllFcxhB+eSAOc5Sv3EqvngMoslqSznnEtqBTuEUn+waj7YzGJJKsvBuaSikmDERYik1rNqPtjMYkkqq2QOOziXNM5FiKT2s2o++MxiSSrLBeEkFZUuQiS1nlXzwWcWS1JZJXPYwbmkcWmHUGo1q+bDwSyWpLJK5bCDc0m1sFojtdhuVs2HhFksSWWVy2FXHJE0LjO6unUjIo6OiBsiYnVEnDLJ6++NiOsi4pqI+FZE7Nvx2okR8dP6dmKDH1HqW2NV82+xj1XzAWcOS1JZTfaJZ8PKuSSgml8zMtpMyETEYuA04AhgDXBlRKzMzOs6NvsBsDwzH42I3wc+Drw+InYGPgwsp7rU5FX1ex9opHFSH9ot13FkXTU/06r5QGsqi81hSZqbJvvEs2XlXNK4UaKrWxcOAVZn5o2Z+ThwDnBc5waZeUlmPlo/vBzYq75/FHBRZt5fdwQvAo5u5ANKfeqNddX8YqvmQ8EclqSyGuwTz4qVc0lAVRqZxek5yyJiVcfjFZm5ouPxnsBtHY/XAIdOs7+3Ad+Y5r17dtswadB0rtBu1XzwzSKLzWFJWgCz7BM3at6D8/q0qVXA7Zn5qvk3SVIZs1r8Ym1mLm/kqBFvojp18iVN7G9YmcWDq3OF9jutmg+BrrPYHG4Zc1gaFP29INy7gesb2I+kwjK7u3XhdmDvjsd71c9tJiJeDvwxcGxmrp/Ne/UEZvEAsmo+nMzhvmUOSwOiwT7xrMxrcB4RewGvBD7XTHMkldTgypRXAgdExP4RsQVwPLCyc4OIOBj4DFWH8J6Oly4AjoyInSJiJ+DI+jlNwSweXGNV84usmg8Vc7j/mMPSYOnX1do/CbwP2H6qDSLiJOAkgK3YZp6HG15fO3DXRvf3rmsvaXR/X2enRven3qtWpmxmjcjM3BgRJ1N15hYDp2fmtRFxKrAqM1cCfwlsB/xDRADcmpnHZub9EfGnVB1LgFMz8/5GGja4Psk0WdxPOdxk1vV7zu2+2XXNn9HTY6ucprLYHO65TzKLPvE+e7rs01x94/yzGt3fUXsc1Oj+1P+a7BMDRMTNwMPACLBxuilJc06GiHgVcE9mXhURL51qu3pxkhUAO8TOC1D8l9SUJk/PyczzgfMnPPehjvsvn+a9pwOnN9eawdVNFpvD/cm55sOrqSw2h3tjLn3i5c/dyiyWWmwBTln/jcxcO9NG8/na7jDg2Ih4BbAVsENEfDkz3zSPfUoqqNTKlJoXs3gAbV41d675sDGL+445LA2YUjk853p9Zr4/M/fKzP2o5jF92xCS+lfS3dwaO43tYhYPphO8rvnQ6jaL1R7msDRYZtknXhYRqzpuJ026S7gwIq6a4vVxTniRNM5z7KTyrJrLLJaksmaRw91c1vJFmXl7RDwZuCgifpyZ/zrZho0MzjPzUuDSJvYlqZCEHLUa08/M4sHQOdfcqvkQMov7mjksDYCGczgzb69/3hMRXwMOASYdnDe3DJ2kvudp7VJZVs0FjV1KTZI0R031iSNi24jYfuw+1aUpfzTV9p7WLmncAqxMKWkWrJoLzGJJKq3BHH4K8LX6cpVLgLMy85tTbezgXBJQza2xGiOVs5tVc2EWS1JpTeZwZt4IPLfb7R2cS6okYIdQKsaquQCzWJJKK5jDDs4ljfNUSqmM3XIdR1o1V80slqSySuWwg/Mhtnbdo1xyw62suuUu1m8cYcsli1m+7278xtP3Ydl225RunnouXCFYA6kfss6quTYxi6Vee3Snddz6gp9w53NuZWSLjSx+fAm7X7MP+3z3aWzzgJk8fMrlsIPzIXXdnWs54zs/ZONoMlp/NbR+4wjfvfEOrrz5Tt7ywmdz4O7LCrdSPWe1RgOmH7LOqrmewCyWembtU+/kmtdeTi4eJRdX//hGttzI7b96E3c89xae8w/PZ9nq3Qu3Uj1XKIe9lNoQenSndZzxnR/y+MjoeGd1zGgmj4+McsZ3fsjadY8WaqGKSC+lpsGydt2jfZF1Y1Xzi9nHqrm6zmJJ8/ezBzdwzWsvZ3SLkfGB+ZhcnIxuMcI1r72cR3daV6iFKqJgn9jB+RC69QU/YePo9F8HbRxNLr3h1h61SK2RXd6kPnDJDbe2PuusmmtS5rDUE5+45gFy8ei02+TiUW59/k971CK1RqE+sYPzIXTnc259QhVpotFMVt1yV49apPaILm9S+6265a7WZ51Vc03OHJZ64cyfPvyEivlEuTi58zm39KhFao8yfWLnnA+hkS02drXd+o0jC9wStc70Xx5LfaXbDCuVdVbNNSWzWOqJdRu6K31223fWACmUww7Oh9Dix5cwsuXMIbPlksU9aI1aw2vrasBsuWRxVwPvUlnnCu2alFks9cx2S4OHuxigL37cIdNQKZjDntY+hHa/Zh8WxfT/wy2KYPm+u/WoRWqLzO5uUj9Yvu9urc06q+aajjks9cYbD9ieGJn+70SMBLtfs2+PWqS2KNUndnA+hPb57tNYsmj6IFqyKHjp0/fpUYvUGi4IpwHyG0/fp7VZ51xzTcsclnrivc/ZiRiZfjgUI4vY5/IDetQitUahPrHnaAyhbR7Yjq8e+RRed9FdbBhJOs/mWRqwdHFw7hFP4Zi97+16n19npwVoqXrOUyk1QD6y9708v8GsayrnrJprRmax1BO/vONSDvrGEVz9iovJRZtfTi1GghhdzEHfeDnbszd2dYeMp7Wrl47ZZ1uufs0+/O6BO7LD0kUsAnZYuojfPXBHrn7NPhyzz7alm6gCIru7Sf2ijVln1VwzMYel3tn1lr057KzfZq8fPZMl65dCwpL1S9nrR8/ksLN+m11v2bt0E1VAqT6xlfMh9ss7LuX/vmhX/u+Ldi3dFLVBBoxardHgaVPWWTXXjMxiqee2eXAHDrzsMA687LDSTVEbFMxhB+eSNrEaIy0oV2hXV8xiSSqrUA47OJe0iR1CacFYNVfXzGJJKqtQDjvnXFN7aIR4653w0MzXCdaAcLV2DaMeZZ1zzdU1c1hqlW1GH+dDD1/INqOPl26KeqVQn9jBuaZ2wSPENx+BCx8t3RL1QlLNsenmJg2SHmSdVXN1rdssltQzL9hwCy/ccAvP33BL6aaoFwr2iR2ca0pxzkOb/dTgc7V2DaNeZJ1Vc82GOSy1y5Hrb9jspwafq7WruHjd7cS/PTb+OJfWd773GIt2X73p+V/fmjx3zx63Tj1hh09DoNdZZ9Vcs2YWS0X9r4f+hYM33jH+eENdz/yVjXfzzfs/O/78D5bswft3eGXP26cecM65Sst370Ruven0jNiw+U+A3DrI9+zU45apV5r8ljAijo6IGyJidUScMsnrL46I70fExoh4zYTXRiLi6vq2splPJ1V6nXVWzTVb5rBU1tlbH8wvOmqYSxnd7CfAL1jC2Vsf3PO2qTdKVc4dnGuTw7Yhv7T7Zp3WTrl1kF/eHV64TY8bpp5paH5NRCwGTgOOAQ4E3hARB07Y7FbgLcBZk+ziscw8qL4dO78PJU3Qw6yzaq45MYeloq5Zugcf2v6ozQbonX7BEj60/VFcs3SPHrdMPdPgnPOIWBwRP4iIr8+0rYNzbe6wbcjP7EZuufn/bLllkJ/ZzYH5IOt2VcruviU8BFidmTdm5uPAOcBxmx0u8+bMvAY6voaWeqVHWWfVXLNmDkutcM3SPfjz7V7GehZv9vx6FvPn273Mgfkga7ZPDPBu4PpuNnRwrid6cBSWQC6C3CrIRVSrEzzo3+6B130QLYuIVR23kybsaU/gto7Ha+rnurVVvd/LI+LVc/ko0owWOOusmmvOzGGpFbbLxxkhGCFYz+Lx+9ull1QbeA0NziNiL+CVwOe6OayDcz1BnP0gPJpw4BbkGbvDgVvAo+mq7UMgRru7AWszc3nHbUXDTdk3M5cDJwCfjIhfbnj/0oJnnVVzzZU5LLXDUetvYCs2ctPinfmT7Y/kpsU7sxUbXbV9CMyiTzzTF6WfBN5Hl2couVq7nmj7ReSHdoGTngSLgnzR3rDi58QVj834VvW55ha2uB3Yu+PxXvVz3TUj8/b6540RcSlwMPCzxlonwYJmnVVzzUszWWwOS/P0SGzBZ7c+lH/c6tlkBO/aYQ9e/Ysf8ayNd5VumhZa9zm8tv4i8wki4lXAPZl5VUS8tJudOTjXE+QZE+bQLA74/Z3I33eV9kHW8KqTVwIHRMT+VJ3B46mqLzO3I2In4NHMXB8Ry4DDgI831jKptpBZN1Y1v4B9rZprVhrMYnNYmqdTtz9ys8ejsYivbv0cvspzCrVIvdBgDh8GHBsRrwC2AnaIiC9n5pumeoOntUvapKGVKTNzI3AycAHVAhjnZua1EXFqRBwLEBHPi4g1wGuBz0TEtfXbnwmsioj/BC4BPpaZ1y3Ap5UWhFVzzZs5LEllNdAnzsz3Z+Zembkf1Rek355uYA7zrJxHxJOoJrc/i6r4/zuZ+d357FNSQQ1erzEzzwfOn/DchzruX0l1muXE930HeHZzLRl8ZnG7WDXXvDWUxeZw75jD0oBZgGuYd2O+p7V/CvhmZr4mIrYAvM6W1McaPK1dvWUWt4RVczXBLO5L5rA0QJrO4cy8FLh0pu3mPDiPiB2BFwNvqQ/4OOB1BaR+leOrTqqPmMXtYtVc82YW9x1zWBowBXN4PpXz/YF7gS9ExHOBq4B3Z+YjnRvVy8mfBLCVXyK2xlF7HFS6CWojqzX9aMYsHtYc7nXOWTVXY8zifjPrPvE+e7omc1ucf+0lpZugNiqUw/NZEG4J8KvA32TmwcAjwCkTN8rMFWPX4FzKlvM4nKQFl13e1CYzZrE53Bte11yNMYf7zaz7xLvusrjXbZQ0G4X6xPMZnK8B1mTmFfXj86iCSVKfGrt0xEw3tYpZ3AJWzdUkc7jvmMPSgCnVJ57z4Dwz7wJui4in108dDniZDUnqIbO4HayaS8PLHJbUlPlOeHkncGa9KuWNwFvn3yRJxViN6VdmcUFWzdU4s7gfmcPSIOnHS6ll5tXA8maaIqkoVwjuW2ZxWa7QrkaZxX3JHJYGSJ+u1i5p0FitkWals2p+plVzNcUslqSy+rFyLmlwBC4yJM3WG+uq+TfZjzutmqsBZrEklVUyhx2cS9rEDqHUtd1zHUeMzzV/RunmaJCYxZJUloNzSUV5eR5pVk6waq6FYBZLUlkFc9jBuaRNXIRI6opVcy0os1iSynJBOEmlWa2RumPVXAvJLJaksqycSyrPDqE0I6vmWnBmsSSV5eBcUlGJHUKpC1bNtaDMYkkqq2AOOziXNM5TKaXpWTVXL5jFklSWp7VLKs8OoTQtq+bqCbNYkspycC6ptHCFYGlKnVXzs62aawGZxZJUVqkcdnAuqeI8R2laY1XzC9iXO6yaa6GYxZJUlnPOJZUW9U3SE20+1/yZpZujAWYWS1JZJXPYwbmkTazWSJOyaq6eMoslqayGcjgitgL+FdiSaux9XmZ+eKrtHZxLGucKwdITWTVXr5nFklRWgzm8HnhZZq6LiKXAv0fENzLz8sk2dnAuaRM7hNITWDVXz5nFklRWQzmcmQmsqx8urW9T7t3BuaRKukKwNJFVc/WcWSxJZc0uh5dFxKqOxysyc0XnBhGxGLgKeCpwWmZeMdXOHJxL2sRqjbQZq+YqwiyWpLK6z+G1mbl82l1ljgAHRcSTgK9FxLMy80eTbbtoNm2UNNgiu7t1ta+IoyPihohYHRGnTPL6iyPi+xGxMSJeM+G1EyPip/XtxGY+nTQ7Vs1VijksSWU12Scek5k/By4Bjp5qGwfnkjbJLm8zqE/fOQ04BjgQeENEHDhhs1uBtwBnTXjvzsCHgUOBQ4APR8ROc/xE0pyNVc0vZh+r5uotc1iSymquT7xrXTEnIrYGjgB+PNX2Ds4ljWvwW8JDgNWZeWNmPg6cAxzXuUFm3pyZ1wATZ/UcBVyUmfdn5gPARUzzDaO0EKyaqyRzWJLKarBPvDtwSURcA1xJla1fn2pj55xLqiRP7J7N3Z7AbR2P11BVYOb63j0bapfUFeeaq5jmstgclqS5aLBPXH8BenC32zs4lwRAMKu5MzOuTCn1K6vmKmkWWWwOS9ICmGWfuFEOziVt0tzKlLcDe3c83qt+rhu3Ay+d8N5Lu26ZNE9WzVVcd1lsDkvSQik0OHfOuaRxkdnVrQtXAgdExP4RsQVwPLCyy2ZcABwZETvVCxAdWT8nLTir5moDc1iSymqwTzwrDs4lVbpdlbKLHMrMjcDJVJ2564FzM/PaiDg1Io4FiIjnRcQa4LXAZyLi2vq99wN/StWxvBI4tX5OWnCu0K7izGFJKqvBPvFseVq7pHFNzq/JzPOB8yc896GO+1dSnSo52XtPB05vrjXSzKyaqy2aymJzWJLmxjnnkoqL5lZrl/qOc83VFmaxJJVVKocdnEvapNC3hFJpVs3VKmaxJJVl5VxSUVnuFB6pNKvmag2zWJLKKpjD81oQLiL+e0RcGxE/ioizI2KrphomqYBCi19ofszi+bFqrtYxh/uOOSwNmEJ94jkPziNiT+BdwPLMfBawmOoyHZL6UFB9S9jNTe1hFs+fK7SrTbrNYrWHOSwNlpJ94vme1r4E2DoiNgDbAHfMv0mSSolRe3x9yiyeI6vmaiOzuC+Zw9IAKZXDc66cZ+btwP8GbgXuBB7MzAsnbhcRJ0XEqohYtYH1c2+ppIVV8JqOmrtustgcnppVc7WOOdx35tInvve+kV43U1K3CvaJ53Na+07AccD+wB7AthHxponbZeaKzFyemcuXsuXcWyppwcVodze1RzdZbA5Pzqq52soc7i9z6RPvusviXjdT0iyU6hPPZ0G4lwM3Zea9mbkB+CrwwmaaJakIKzb9yCyeI6vmai1zuN+Yw9KgKdQnns+c81uB50fENsBjwOHAqkZaJakIFxnqS2bxHFg1V5uZxX3HHJYGTKkcnvPgPDOviIjzgO8DG4EfACuaapikHksg7RH2G7N4bryuuVrLLO475rA0YArm8LxWa8/MDwMfbqgtkgpzHmN/Motnx6q52s4s7j/msDRYSuXwfC+lJmlAjF3TURp0Vs3VZmaxJJVVMocdnEuqZHoqpQaeVXO1nlksSWUVzGEH55LGWa3RoLNqrn5gFktSWVbOJZVnh1ADzKq5+oZZLEllFcrh+VznXNKAiezuJvUjr2uufmEOS1JZTfWJI2LviLgkIq6LiGsj4t3TbW/lXFIlgRF7fBpMVs3VN8xiSSqr2RzeCPxBZn4/IrYHroqIizLzusk2dnAuaZzVGA0q55qrn5jFklRWUzmcmXcCd9b3H46I64E9AQfnkmbgCsEaQFbN1XfMYkkqq/scXhYRqzoer8jMFZNtGBH7AQcDV0y1MwfnksZZrdEgsmqufmMWS1JZs8jhtZm5fMb9RWwHfAV4T2Y+NNV2Ds4lVRJXCNbAsWquvmMWS1JZDedwRCylGpifmZlfnW5bB+eSAAggXIRIA8aqufqNWSxJZTWZwxERwOeB6zPzEzNt7+Bc0rhwnqMGiFVz9SuzWJLKajCHDwPeDPwwIq6un/tAZp4/2cYOziVVPJVSA8aqufqSWSxJZTWYw5n571TF+K44OJdUS1cI1sCwaq7+ZRZLUlnlctjBuaRxrhCsQWHVXP3MLJakskrl8KIyh5XUSpnd3boQEUdHxA0RsToiTpnk9S0j4u/r16+or/1IROwXEY9FxNX17W+b/ZAadFbN1ffMYUkqq8E+8WxYOZdUyUZXplwMnAYcAawBroyIlZl5XcdmbwMeyMynRsTxwF8Ar69f+1lmHtRIYzR0rJqrrzWUxeawJM1Rg33i2bJyLmmT7PI2s0OA1Zl5Y2Y+DpwDHDdhm+OAL9b3zwMOry83Ic2ZVXMNBHNYkspqrk88Kw7OJY2LzK5uwLKIWNVxO2nCrvYEbut4vKZ+btJtMnMj8CCwS/3a/hHxg4i4LCJ+vfEPqoH1hrpq/i32sWquvmUOS1JZs+gTN8rT2iVt0n3IrM3M5QvUijuBfTLzvoj4NeAfI+JXMvOhBTqeBsRuuY4j66r5mVbN1c+6y2JzWJIWSqHV2q2cS6okMNrlbWa3A3t3PN6rfm7SbSJiCbAjcF9mrs/M+wAy8yrgZ8DT5vKRNFzG5ppfbNVc/azbLJ6ZOSxJc9Fsn3hWHJxLAiDo7vSdLk/huRI4ICL2j4gtgOOBlRO2WQmcWN9/DfDtzMyI2LVeyIiI+CXgAODGRj6kBlZn1dy55upn3WZxF8xhSZqDhvvEs+Jp7ZI2GW3mK8DM3BgRJwMXAIuB0zPz2og4FViVmSuBzwNfiojVwP1UHUeAFwOnRsQGqu8k356Z9zfSMA2ssar5ha7QrkHQQBabw5I0Dw31iWfLwbmkytgpPE3tLvN84PwJz32o4/4vgNdO8r6vAF9priUadM4110BpMIvNYUmag4b7xLPh4FzSuIU4PUdaaF7XXIPGLJakskrlsINzSZvYIVSfca65BpJZLEllOTiXVFbaIVTfca65Bo9ZLElllcthB+eSKgmM2CFU/3CuuQaSWSxJZRXMYQfnksY5z1H9xLnmGlRmsSSV5ZxzSeXZIVSfcK65BppZLEllOTiXVFQCo3YI1R+ca66BZRZLUlkFc3jRTBtExOkRcU9E/KjjuZ0j4qKI+Gn9c6eFbaakhVcvftHNTT1nFm/iXHMNNnO4rcxhaViU6xPPODgHzgCOnvDcKcC3MvMA4Fv1Y0n9zsF5m52BWQxsqppfzD5WzTWYzOG2OgNzWBoObR2cZ+a/AvdPePo44Iv1/S8Cr262WZJ6LoGR0e5u6jmzuOJccw28brNYPWcOS0OiwT7xZGfcTKebyvlknpKZd9b37wKeMk2DToqIVRGxagPr53g4SQsvIUe7u6ktusriQcrhsar5t6yaa2B1mcVqizn1ie+9b6Q3rZM0B432ic/giWfcTGmug/NxmZlU3y9M9fqKzFyemcuXsuV8DydpIXlae9+aLosHJYeda66hYQ73pdn0iXfdZXEPWyZp1hrqE09xxs2U5jo4vzsidgeof94zx/1IaouxlSm7uakthiqLnWuuodBtFqsthiqHpaEwuz7xsrEzYurbSfM59FwH5yuBE+v7JwL/NJ9GSGoJK+f9Zmiy2LnmGirmcD8ZmhyWhkr3feK1Y2fE1LcV8zlsN5dSOxv4LvD0iFgTEW8DPgYcERE/BV5eP5bU7xyct9awZ7FzzTVUzOFWGvYcloZKoT7xkpnblW+Y4qXDG26LpJIyYcQFatpqmLPYueYaKmZxaw1zDktDpWAOz3tBOEkDxMq5Wsi55ho65rAkldVQn3iKM26mNGPlXNIQscOnlnGuuYaSWSxJZTWUw9OccTMpB+eSaq4ArPYZq5pfyL5WzTUkzGJJKqtcDjs4l1RJyBwt3QppnHPNNZTMYkkqq2AOOziXtMmIHUK1x1jV/AKr5ho2ZrEklVUohx2cS6pkwqgdQrXD7s4117AyiyWprII57OBc0iYuQqSWcK65hppZLEllFcphB+eSxqXVGrXA7rmOI5xrriFmFktSWaVy2MG5pJrXzlU7ONdcw80slqSyyuWwg3NJlcTL96i4zqq5c801lMxiSSqrYA47OJcEVDmUIyOlm6Ehty0b+BlP4iZ2sGquoWQWS1JZJXN4UZGjSmqfTMjR7m5diIijI+KGiFgdEadM8vqWEfH39etXRMR+Ha+9v37+hog4qrkPqbZbHTvxDl7Gpzm4dFOkMrrN4i6Yw5I0Bw33iWfDyrmkcdnQKTwRsRg4DTgCWANcGRErM/O6js3eBjyQmU+NiOOBvwBeHxEHAscDvwLsAVwcEU/LTEtJwyKCX/jnSUOsiSw2hyVp7prqE8+WlXNJmzT3LeEhwOrMvDEzHwfOAY6bsM1xwBfr++cBh0dE1M+fk5nrM/MmYHW9P0kaDuawJJU1DJXzh3lg3cV53g29POYsLAPWlm7ENNrcvja3Ddrdvqbbtu9c3/gwD1xwcZ63rMvNt4qIVR2PV2Tmio7HewK3dTxeAxw6YR/j22Tmxoh4ENilfv7yCe/ds8t2aQYtz2EYrn+vTWtz+9rcNujPLDaH+9hV16xft3j31W3N4mH799qkNrcN2t2+fsxhaPj32evzBm/IzOU9PmZXImJVW9sG7W5fm9sG7W5fm9qWmUeXboN6orU5DO36NzFRm9sG7W5fm9sG7WqfWTw0WpvFbfr3MJk2t6/NbYN2t69NbSuZw57WLmkh3A7s3fF4r/q5SbeJiCXAjsB9Xb5XkjQ9c1iS+oyDc0kL4UrggIjYPyK2oFpYaOWEbVYCJ9b3XwN8OzOzfv74ehXh/YEDgO/1qN2SNCjMYUnqM70+rX3FzJsU0+a2Qbvb1+a2Qbvb1+a2zVk9d/Fk4AJgMXB6Zl4bEacCqzJzJfB54EsRsRq4n6rjSL3ducB1wEbgHa4Q3Ki2/z/X5va1uW3Q7va1uW3Q/vbNmjncem3+f67NbYN2t6/NbYN2t6/NbeuZqL4glSRJkiRJpXhauyRJkiRJhTk4lyRJkiSpsJ4MziPi6Ii4ISJWR8QpvThmtyJi74i4JCKui4hrI+Ldpds0UUQsjogfRMTXS7dlooh4UkScFxE/jojrI+IFpds0JiL+e/3f9EcRcXZEbFW4PadHxD0R8aOO53aOiIsi4qf1z51KtlGDra1Z3A85DO3N4jbnMJjFUqe25jD0Rxa3NYeh3VlsDvePBR+cR8Ri4DTgGOBA4A0RceBCH3cWNgJ/kJkHAs8H3tGy9gG8G7i+dCOm8Cngm5n5DOC5tKSdEbEn8C5geWY+i2oxnOPLtoozgInXTTwF+FZmHgB8q34sNa7lWdwPOQztzeJW5jCYxVKnlucw9EcWtzWHoaVZbA73l15Uzg8BVmfmjZn5OHAOcFwPjtuVzLwzM79f33+Y6h/SnmVbtUlE7AW8Evhc6bZMFBE7Ai+mWu2VzHw8M39etFGbWwJsXV+7dRvgjpKNycx/pVoNt9NxwBfr+18EXt3LNmmotDaL257D0N4s7oMcBrNYGtPaHIb2Z3Fbcxj6IovN4T7Ri8H5nsBtHY/X0KJ/6J0iYj/gYOCKwk3p9EngfcBo4XZMZn/gXuAL9SlGn4uIbUs3CiAzbwf+N3ArcCfwYGZeWLZVk3pKZt5Z378LeErJxmig9UUWtzSHob1Z3NocBrNYmqAvchham8WfpJ05DC3OYnO4v7ggXC0itgO+ArwnMx8q3R6AiHgVcE9mXlW6LVNYAvwq8DeZeTDwCC05BaWep3IcVVjuAWwbEW8q26rpZXVdQ69tqKHVxhyG1mdxa3MYzGKpH7Uxi1uew9DiLDaH+0svBue3A3t3PN6rfq41ImIpVQidmZlfLd2eDocBx0bEzVSnPr0sIr5ctkmbWQOsycyxb1XPowqmNng5cFNm3puZG4CvAi8s3KbJ3B0RuwPUP+8p3B4NrlZncYtzGNqdxW3OYTCLpU6tzmFodRa3OYeh3VlsDveRXgzOrwQOiIj9I2ILqgUIVvbguF2JiKCaH3J9Zn6idHs6Zeb7M3OvzNyP6vf27cxszTddmXkXcFtEPL1+6nDguoJN6nQr8PyI2Kb+b3w4LVmYY4KVwIn1/ROBfyrYFg221mZxm3MY2p3FLc9hMIulTq3NYWh3Frc5h6H1WWwO95ElC32AzNwYEScDF1CtDnh6Zl670MedhcOANwM/jIir6+c+kJnnl2tSX3kncGb9R+ZG4K2F2wNAZl4REecB36daffQHwIqSbYqIs4GXAssiYg3wYeBjwLkR8TbgFuB15VqoQdbyLDaH56eVOQxmsdSp5TkMZvF8tTKLzeH+EtUp/ZIkSZIkqRQXhJMkSZIkqTAH55IkSZIkFebgXJIkSZKkwhycS5IkSZJUmINzSZIkSZIKc3AuSZIkSVJhDs4lSZIkSSrs/wcBj5Wt+6OncQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at individual events\n",
    "evt_plt = 14273\n",
    "evt_item = dataset_val[evt_plt]\n",
    "evt_frame_cmax = np.array(evt_item[0])\n",
    "evt_frame      = np.array(evt_item[1])\n",
    "evt_argmax     = np.array(evt_item[2])\n",
    "evt_err        = np.array(evt_item[3])/emnet.PIXEL_SIZE\n",
    "evt_lside      = np.array(evt_item[4])\n",
    "print(\"Light side is\",evt_lside)\n",
    "\n",
    "# Compute the true row and col.\n",
    "row_true = evt_err[1] + evt_argmax[0] + 0.5\n",
    "col_true = evt_err[0] + evt_argmax[1] + 0.5\n",
    "print(\"True point (\",col_true,\",\",row_true,\")\")\n",
    "\n",
    "# Information for drawing the line.\n",
    "nrows = evt_frame.shape[0]\n",
    "ncols = evt_frame.shape[1]\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "print(\"Line drawn: m = {}, b = {}\".format(line_m,line_b))\n",
    "\n",
    "# Compute the distance from the true point to the line.\n",
    "dist_true = (line_m*col_true - row_true + line_b) / (line_m**2 + 1)**0.5\n",
    "print(\"True distance to the line:\",dist_true)\n",
    "\n",
    "# Run the model and compute the reconstructed point.\n",
    "data = torch.tensor(evt_frame_cmax).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "output_score = model(data)\n",
    "pred_err = np.array(output_score.cpu().detach().numpy()).squeeze()\n",
    "row_pred = pred_err[1] + evt_argmax[0] + 0.5\n",
    "col_pred = pred_err[0] + evt_argmax[1] + 0.5\n",
    "print(\"Pred point (\",col_pred,\",\",row_pred,\")\")\n",
    "print(\"Pred vector (\",pred_err[0],\",\",pred_err[1],\")\")\n",
    "\n",
    "# Compute the distance from the reconstructed point to the line.\n",
    "dist_pred = (line_m*col_pred - row_pred + line_b) / (line_m**2 + 1)**0.5\n",
    "print(\"Predicted distance to the line:\",dist_pred)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_frame_cmax,extent=[0,11,11,0])\n",
    "ax1.plot([evt_err[0]+5.5],[evt_err[1]+5.5],color='red',marker='*',markersize=10)\n",
    "ax1.plot([pred_err[0]+5.5],[pred_err[1]+5.5],color='green',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event, seen by the NN\\n(centered on max pixel)\")\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "plt.imshow(evt_frame,extent=[0,11,11,0])\n",
    "ax2.plot([col_true],[row_true],color='red',marker='*',markersize=10)\n",
    "ax2.plot([col_pred],[row_pred],color='green',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event with line\")\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = line_m*xfit + line_b\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)],linewidth=2,color='red')\n",
    "\n",
    "# Compute a test loss\n",
    "batch_size = 1\n",
    "indices = np.indices((emnet.EVT_SIZE,emnet.EVT_SIZE))\n",
    "row_coords = torch.tensor(indices[0] + 0.5 - ((emnet.EVT_SIZE-1)/2 + 0.5)).repeat([batch_size,1,1]).cuda()\n",
    "col_coords = torch.tensor(indices[1] + 0.5 - ((emnet.EVT_SIZE-1)/2 + 0.5)).repeat([batch_size,1,1]).cuda()\n",
    "print(\"TEST LOSS\")\n",
    "#loss_vec, loss_dist = tr.loss_reg_edge(torch.tensor([[evt_err[0],evt_err[1]]]), np.array([evt_argmax]), line_m, line_b, evt_lside)\n",
    "loss_vec, loss_dist, dist_reco_masked = tr.loss_reg_edge(data.squeeze(1), torch.tensor(evt_item[3]).unsqueeze(0).cuda(), output_score, row_coords, col_coords, torch.tensor([evt_argmax]).cuda(), line_m, line_b, evt_lside)\n",
    "print(\"-- Test vector loss:\",loss_vec)\n",
    "print(\"-- Test distance loss:\",loss_dist)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "plt.imshow(dist_reco_masked.cpu().detach().squeeze(0).numpy(),extent=[0,11,11,0])\n",
    "ax3.plot([evt_err[0]+5.5],[evt_err[1]+5.5],color='red',marker='*',markersize=10)\n",
    "ax3.plot([pred_err[0]+5.5],[pred_err[1]+5.5],color='green',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.title(\"Cluster\")\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = line_m*xfit + line_b\n",
    "\n",
    "print(\"COMPARE LOSS\")\n",
    "if(evt_lside == 0):\n",
    "    dist_true *= -1\n",
    "    dist_pred *= -1\n",
    "sigma_dist = 1\n",
    "loss_vec = (pred_err[0]**2 + pred_err[1]**2)\n",
    "loss_dist = np.exp(-dist_pred/sigma_dist)\n",
    "print(\"-- Compare vector loss:\",loss_vec)\n",
    "print(\"-- Compare distance loss:\",loss_dist)\n",
    "#print(\"True vector is ({},{})\".format(evt_vec[0],evt_vec[1]))\n",
    "#print(\"Predicted vector is ({},{})\".format(pred_vec[0],pred_vec[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate several events\n",
    "err_list = []\n",
    "dist_list = []\n",
    "maxval_list = []\n",
    "err3_x = []\n",
    "err3_y = []\n",
    "err_x = []\n",
    "err_y = []\n",
    "err3_list = []\n",
    "\n",
    "for evt in range(0,20000):\n",
    "\n",
    "    evt_item = dataset_val[evt]\n",
    "    evt_frame_cmax = np.array(evt_item[0])\n",
    "    evt_err        = np.array(evt_item[3])/emnet.PIXEL_SIZE\n",
    "\n",
    "    # Record the maximum event value.\n",
    "    maxval_list.append(np.max(evt_frame_cmax))\n",
    "    \n",
    "    sum3 = np.sum(evt_frame_cmax[4:7,4:7])\n",
    "    x3 = (- evt_frame_cmax[4,4] - evt_frame_cmax[5,4] - evt_frame_cmax[6,4] + evt_frame_cmax[4,6] + evt_frame_cmax[5,6] + evt_frame_cmax[6,6])/sum3\n",
    "    y3 = (- evt_frame_cmax[4,4] - evt_frame_cmax[4,5] - evt_frame_cmax[4,6] + evt_frame_cmax[6,4] + evt_frame_cmax[6,5] + evt_frame_cmax[6,6])/sum3\n",
    "    err3 = ((y3 - evt_err[0])**2 + (x3 - evt_err[1])**2)**0.5*emnet.PIXEL_SIZE\n",
    "    \n",
    "    # Send through the model.\n",
    "    data = torch.tensor(evt_frame_cmax).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    output_score = model(data)\n",
    "    pred_err = np.array(output_score.cpu().detach().numpy()).squeeze()\n",
    "    \n",
    "    # Compute the error.\n",
    "    err = ((evt_err[0] - pred_err[0])**2 + (evt_err[1] - pred_err[1])**2)**0.5*emnet.PIXEL_SIZE\n",
    "    dist = (evt_err[0]**2 + evt_err[1]**2)**0.5*emnet.PIXEL_SIZE\n",
    "    ex = (evt_err[1] - pred_err[1])*emnet.PIXEL_SIZE\n",
    "    ey = (evt_err[0] - pred_err[0])*emnet.PIXEL_SIZE\n",
    "    \n",
    "    err_list.append(err)\n",
    "    err3_list.append(err3)\n",
    "    err_x.append(ex)\n",
    "    err_y.append(ey)\n",
    "    err3_x.append(x3)\n",
    "    err3_y.append(y3)\n",
    "    dist_list.append(dist)\n",
    "    \n",
    "    if(evt % 100 == 0):\n",
    "        print(\"Event\",evt)\n",
    "        \n",
    "err_list = np.array(err_list)\n",
    "err3_list = np.array(err3_list)\n",
    "err_x = np.array(err_x)\n",
    "err_y = np.array(err_y)\n",
    "err3_x = np.array(err3_x)\n",
    "err3_y = np.array(err3_y)\n",
    "dist_list = np.array(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(6.0)\n",
    "\n",
    "rng_max = 0.01\n",
    "plt.hist(err_list,bins=50,color='blue',alpha=0.9,label=\"Prediction error\",range=[0,rng_max])\n",
    "plt.hist(dist_list,bins=50,color='green',alpha=0.9,label=\"Error from max pixel\",range=[0,rng_max])\n",
    "plt.hist(err3_list,bins=50,color='red',alpha=0.9,label=\"3x3 error\",range=[0,rng_max])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "#plt.yscale(\"log\")\n",
    "np.savez(\"err_rl_line.npz\", err_list=err_list, dist_list=dist_list, err3_list=err3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(6.0)\n",
    "\n",
    "rng = 0.01\n",
    "plt.hist(err3_x*emnet.PIXEL_SIZE,bins=100,color='blue',alpha=0.9,label=\"Prediction error\",range=[-rng,rng])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the truth and line\n",
    "f_true = np.load(\"err_rl_true.npz\")\n",
    "f_line = np.load(\"err_rl_line.npz\")\n",
    "\n",
    "err_true = f_true['err_list']\n",
    "err_line = f_line['err_list']\n",
    "errm_true = f_true['dist_list']\n",
    "errm_line = f_line['dist_list']\n",
    "err3_true = f_true['err3_list']\n",
    "err3_line = f_line['err3_list']\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(6.0)\n",
    "\n",
    "rng_max = 0.008\n",
    "plt.hist(err_true,bins=50,color='blue',alpha=0.8,label=\"NN (true points)\",range=[0,rng_max])\n",
    "plt.hist(err_line,bins=50,color='green',alpha=0.8,label=\"NN (line method)\",range=[0,rng_max])\n",
    "plt.hist(errm_true,bins=50,color='orange',alpha=0.8,label=\"Max pixel error\",range=[0,rng_max])\n",
    "plt.hist(err3_true,bins=50,color='red',alpha=0.8,label=\"3x3 error\",range=[0,rng_max])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "#plt.yscale(\"log\")\n",
    "\n",
    "# Mean and median computations\n",
    "print(\"NN (true points):   mean = {}, median = {}\".format(np.mean(err_true[err_true < rng_max]),np.median(err_true[err_true < rng_max])))\n",
    "print(\"NN (line method):   mean = {}, median = {}\".format(np.mean(err_line[err_line < rng_max]),np.median(err_line[err_line < rng_max])))\n",
    "print(\"Max pixel error:    mean = {}, median = {}\".format(np.mean(errm_true[errm_true < rng_max]),np.median(errm_true[errm_true < rng_max])))\n",
    "print(\"3x3 centroid error: mean = {}, median = {}\".format(np.mean(err3_true[err3_true < rng_max]),np.median(err3_true[err3_true < rng_max])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/jrenner/local/jerenner/emsim/models'\n",
    "lrate       = 1e-5   # Learning rate to use in the training.\n",
    "load_model  = False   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 200    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/regression/model_basicCNN_regression_vector_11x11.pt\".format(modeldir)\n",
    "\n",
    "# \"Real-data-like\" dataset: occupancy 11, noise_mean=683, noise_sigma=11.2\n",
    "dset_train = tr.EMDataset(\"../dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=-1,nstart=20000)\n",
    "dset_val = tr.EMDataset(\"../dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=-1,nstart=0,nend=20000)\n",
    "#dataset_train   = tr.EMFrameDataset(dset,frame_size=50,nelec_mean=11,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "dataset_train = tr.EMFrameDataset(dset_train,frame_size=11,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=30.0,res_factor=1)\n",
    "dataset_val = tr.EMFrameDataset(dset_val,frame_size=11,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=30.0,res_factor=1)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=50, shuffle=False, collate_fn=tr.my_collate_unet, num_workers=1)\n",
    "\n",
    "# Define the model.\n",
    "model = emnet.basicCNN_reg()\n",
    "#model = emnet.FCNet()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lrate, weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    train_loss = tr.train_regression(model, epoch, train_loader, optimizer)\n",
    "    scheduler.step(train_loss)\n",
    "    #if(epoch % 50 == 0):\n",
    "    torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy.\n",
    "tloss = np.loadtxt(\"train.txt\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.plot(tloss[:,0],tloss[:,1],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.plot(tloss[:,0],tloss[:,2],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a test event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evt_plt = 14273\n",
    "evt_item = dataset_val[evt_plt]\n",
    "evt_arr = np.array(evt_item[0])\n",
    "evt_vec = np.array(evt_item[1])\n",
    "\n",
    "# Send through the model.\n",
    "data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "#target = torch.tensor(evt_lbl).float().cuda()\n",
    "output_score = model(data)\n",
    "\n",
    "# Compute the predicted pixel and (x,y) values.\n",
    "pred_vec = np.array(output_score.cpu().detach().numpy()).squeeze()\n",
    "\n",
    "# Scale the errors.\n",
    "# err_max = int(evt_arr.shape[0]-1)/2*emnet.PIXEL_SIZE\n",
    "# err_rng = 2*err_max\n",
    "# print(\"Max err is\",err_max)\n",
    "# evt_vec = (evt_vec*err_rng - err_max)/emnet.PIXEL_SIZE\n",
    "# pred_vec = (pred_vec*err_rng - err_max)/emnet.PIXEL_SIZE\n",
    "\n",
    "# Information for drawing the line.\n",
    "nrows = evt_arr.shape[0]\n",
    "ncols = evt_arr.shape[1]\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "m = -2*nrows/ncols\n",
    "b = 80\n",
    "print(\"Line drawn: m = {}, b = {}\".format(m,b))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_arr)\n",
    "#ax1.plot([evt_vec[0]],[evt_vec[1]],color='red',marker='o',markersize=10)\n",
    "plt.colorbar()\n",
    "plt.plot([5+evt_vec[1]],[5+evt_vec[0]],color='red',marker='o',markersize=10)\n",
    "plt.plot([5+pred_vec[1]],[5+pred_vec[0]],color='green',marker='o',markersize=10)\n",
    "plt.title(\"Event\")\n",
    "print(\"True vector is ({},{})\".format(evt_vec[0],evt_vec[1]))\n",
    "print(\"Predicted vector is ({},{})\".format(pred_vec[0],pred_vec[1]))\n",
    "\n",
    "# xfit = np.arange(0,ncols,0.1)\n",
    "# yfit = m*xfit + b\n",
    "# plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate several events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_list = []\n",
    "dist_list = []\n",
    "maxval_list = []\n",
    "err_x = []\n",
    "err_y = []\n",
    "\n",
    "for evt in range(0,20000):\n",
    "\n",
    "    evt_item = dataset_val[evt]\n",
    "    evt_arr = np.array(evt_item[0])\n",
    "    evt_vec = np.array(evt_item[1])\n",
    "\n",
    "    # Record the maximum event value.\n",
    "    maxval_list.append(np.max(evt_arr))\n",
    "    \n",
    "    # Send through the model.\n",
    "    data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    output_score = model(data)\n",
    "\n",
    "    # Compute the predicted pixel and (x,y) values.\n",
    "    pred_vec = np.array(output_score.cpu().detach().numpy()).squeeze()\n",
    "    \n",
    "    # Scale the errors.\n",
    "#     err_max = int(evt_arr.shape[0]-1)/2*emnet.PIXEL_SIZE\n",
    "#     err_rng = 2*err_max\n",
    "#     #print(\"Max err is\",err_max)\n",
    "#     evt_vec = (evt_vec*err_rng - err_max)/emnet.PIXEL_SIZE\n",
    "#     pred_vec = (pred_vec*err_rng - err_max)/emnet.PIXEL_SIZE\n",
    "    \n",
    "    # Compute the error.\n",
    "    err = ((evt_vec[0] - pred_vec[0])**2 + (evt_vec[1] - pred_vec[1])**2)**0.5\n",
    "    dist = (evt_vec[0]**2 + evt_vec[1]**2)**0.5\n",
    "    ex = evt_vec[1] - pred_vec[1]\n",
    "    ey = evt_vec[0] - pred_vec[0]\n",
    "    \n",
    "    err_list.append(err)\n",
    "    err_x.append(ex)\n",
    "    err_y.append(ey)\n",
    "    dist_list.append(dist)\n",
    "    \n",
    "    if(evt % 100 == 0):\n",
    "        print(\"Event\",evt)\n",
    "        \n",
    "err_list = np.array(err_list)\n",
    "err_x = np.array(err_x)\n",
    "err_y = np.array(err_y)\n",
    "dist_list = np.array(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(maxval_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(6.0)\n",
    "\n",
    "plt.hist(err_list*0.005,bins=50,color='blue',alpha=0.9,label=\"Prediction error\",range=[0,0.005])\n",
    "plt.hist(dist_list*0.005,bins=50,color='green',alpha=0.9,label=\"Error from max pixel\",range=[0,0.005])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "#plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(6.0)\n",
    "\n",
    "rng = 0.05\n",
    "plt.hist(err_x*0.005,bins=100,color='blue',alpha=0.9,label=\"Prediction error\",range=[-rng,rng])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(dist_list > 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random multi-electron events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameset  = tr.EMFrameDataset(dset,frame_size=20,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=30.0, lside = 0, res_factor=3)\n",
    "#frameset = tr.EMFrameDataset(dset,frame_size=4855,nelec_mean=103713,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "#frameset = tr.EMFrameDataset(dset,frame_size=100,nelec_mean=88,nelec_sigma=2,noise_mean=0,noise_sigma=20)\n",
    "#frameset = tr.EMFrameDataset(dset,frame_size=100,nelec_mean=10,nelec_sigma=1,noise_mean=0,noise_sigma=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate many frames and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_line(fit_img, th = 0.5, pct_rng = 0.2, nbins_hdist = 80):\n",
    "    \n",
    "    A = fit_img/np.mean(fit_img)\n",
    "    nrows = A.shape[0]\n",
    "    ncols = A.shape[1]\n",
    "    ncts = np.sum(A[A >= th])\n",
    "    nzeros = np.sum(1-A[A < th])\n",
    "    wcts = 1 #nzeros/ncts\n",
    "    indices = np.indices((nrows,ncols))\n",
    "    irows = indices[0]\n",
    "    icols = indices[1]\n",
    "    print(\"nzeros = {}, ncts = {}, wcts = {}\".format(nzeros,ncts,wcts))\n",
    "\n",
    "    def count_loss(x):\n",
    "        m,b = x\n",
    "\n",
    "        # The loss L is:\n",
    "        #\n",
    "        # (number of 0s in the dark region) - wcts*(number of 1s in the dark region)\n",
    "        # + wcts*(number of 1s in the light region) - (number of 0s in the dark region)\n",
    "        # \n",
    "        # where wcts is the count weight, determined such that the number of counts multiplied by wcts is equal to\n",
    "        # the number of zeros.\n",
    "        L = 0\n",
    "        L1 = np.sum(1-A[(irows < m*icols + b) & (A < th)])\n",
    "        L2 = np.sum(A[(irows < m*icols + b) & (A >= th)])\n",
    "        L3 = np.sum(A[(irows >= m*icols + b) & (A >= th)])\n",
    "        L4 = np.sum(1-A[(irows >= m*icols + b) & (A < th)])\n",
    "\n",
    "        L = L1 - wcts*L2 + wcts*L3 - L4\n",
    "        #print(\"Loss is:\",-L,\"with L1 =\",L1,\"L2 =\",L2,\"L3 =\",L3,\"L4 =\",L4)\n",
    "        return -L\n",
    "    \n",
    "    initial_guess = [-1.5*nrows/ncols,2.0*nrows]\n",
    "    result = optimize.minimize(count_loss,initial_guess,method='Nelder-Mead',tol=1e-5)\n",
    "    m,b = result.x\n",
    "    Lmin = result.fun\n",
    "    print(\"m = \",m,\"b = \",b,\"Lmin=\",Lmin)\n",
    "    \n",
    "    # Force m and b.\n",
    "    m = -2.0\n",
    "    b = 90.0\n",
    "    print(\"NOTE: hard-coding m and b\")\n",
    "    \n",
    "    # Get the loss over a range of the parameters.\n",
    "    mrng = np.arange(m-pct_rng*m, m+pct_rng*m, 2*pct_rng*m/1000)\n",
    "    Lrng_m = np.array([count_loss([mval,b])/Lmin for mval in mrng])\n",
    "    brng = np.arange(b-pct_rng*b, b+pct_rng*b, 2*pct_rng*b/1000)\n",
    "    Lrng_b = np.array([count_loss([m,bval])/Lmin for bval in brng])\n",
    "    \n",
    "    # Get the histogram of mean value vs. distance.\n",
    "    dist = (5./3.)*(m*icols - irows + b) / (m**2 + 1)                          # compute distance to line for each point\n",
    "    hw, bb = np.histogram(dist.flatten(),weights=A.flatten(),bins=nbins_hdist)  # weighted histogram\n",
    "    hh, bb = np.histogram(dist.flatten(),bins=nbins_hdist)                      # unweighted (for normalization)\n",
    "    hh[hh == 0] = 0.1\n",
    "    hfinal = hw / hh                                                   # normalize the histogram\n",
    "    bcenters = (bb[1:] + bb[:-1]) / 2                                  # determine the bin centers\n",
    "    \n",
    "    return m,b,Lmin,mrng,Lrng_m,brng,Lrng_b,hfinal,bcenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some number of events and count them.\n",
    "th_unet = 0.2\n",
    "th_classical = 825/4.\n",
    "evts = np.arange(0,100000)\n",
    "l_frames, l_labels, l_ct_unet, l_ct_classical = [], [], [], []\n",
    "for evt in evts:\n",
    "    frame,label = frameset[evt]\n",
    "    gnd_truth = label[0]\n",
    "    \n",
    "    # Send through the model.\n",
    "    data = torch.tensor(frame).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    output_score = model(data)\n",
    "    \n",
    "    # Compute the predicted pixel values.\n",
    "    prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()\n",
    "    ct_unet = (prob > th_unet)\n",
    "    \n",
    "    # Count with a single threshold.\n",
    "    #ct_classical = (frame > th_classical)\n",
    "    \n",
    "    # Max argument classical truth.\n",
    "    #ct_classical = np.zeros(frame.shape)\n",
    "    #ct_classical[np.unravel_index(np.argmax(frame),frame.shape)] = 1\n",
    "    \n",
    "    # Threshold truth.\n",
    "    ct_classical = label[1]\n",
    "    \n",
    "    l_frames.append(frame)\n",
    "    l_labels.append(gnd_truth)\n",
    "    l_ct_unet.append(ct_unet)\n",
    "    l_ct_classical.append(ct_classical)\n",
    "    \n",
    "    if((evt-evts[0]) % (len(evts)/100) == 0):\n",
    "            print(\"{}% done\".format(int((evt-evts[0]) / (len(evts)/100))))\n",
    "            \n",
    "l_frames = np.array(l_frames)\n",
    "l_labels = np.array(l_labels)\n",
    "l_ct_unet = np.array(l_ct_unet)\n",
    "l_ct_classical = np.array(l_ct_classical)\n",
    "\n",
    "# Create a summed frame, label, and count arrays.\n",
    "frame = np.sum(l_frames,axis=0)\n",
    "label = np.sum(l_labels,axis=0)\n",
    "ct_unet = np.sum(l_ct_unet,axis=0)\n",
    "ct_classical = np.sum(l_ct_classical,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_th = 0.99\n",
    "ct_th = 0.1\n",
    "m_frame,b_frame,Lmin_frame,mrng_frame,Lrng_m_frame,brng_frame,Lrng_b_frame,hdist_frame,bcenters_frame = fit_line(frame,th=raw_th)\n",
    "m_label,b_label,Lmin_label,mrng_label,Lrng_m_label,brng_label,Lrng_b_label,hdist_label,bcenters_label = fit_line(label,th=ct_th)\n",
    "m_unet,b_unet,Lmin_unet,mrng_unet,Lrng_m_unet,brng_unet,Lrng_b_unet,hdist_unet,bcenters_unet = fit_line(ct_unet,th=ct_th)\n",
    "m_classical,b_classical,Lmin_classical,mrng_classical,Lrng_m_classical,brng_classical,Lrng_b_classical,hdist_classical,bcenters_classical = fit_line(ct_classical,th=ct_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#frame,label = frameset[0]\n",
    "logscale = False\n",
    "nrows = frame.shape[0]\n",
    "ncols = frame.shape[1]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(12.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "if(logscale):\n",
    "    plt.imshow(np.log(frame))\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"log(counts)\")\n",
    "    #plt.title(\"Raw frame (log counts, threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(raw_th,m_frame,b_frame))\n",
    "    plt.title(\"Raw frame (log counts)\\nm = {:.2f}, b = {:.2f}\".format(m_frame,b_frame))\n",
    "else:\n",
    "    plt.imshow(frame/np.max(frame))\n",
    "    cbar = plt.colorbar()\n",
    "    #plt.title(\"Raw frame (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(raw_th,m_frame,b_frame))\n",
    "    plt.title(\"Raw frame\\nm = {:.2f}, b = {:.2f}\".format(m_frame,b_frame))\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_frame*xfit + b_frame\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "plt.imshow(label/np.max(label))\n",
    "#plt.title(\"Truth (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(ct_th,m_label,b_label))\n",
    "plt.title(\"Truth\\nm = {:.2f}, b = {:.2f}\".format(m_label,b_label))\n",
    "plt.colorbar()\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_label*xfit + b_label\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "plt.imshow(ct_unet/np.max(ct_unet))\n",
    "#plt.title(\"UNet counts (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(ct_th,m_unet,b_unet))\n",
    "plt.title(\"UNet counts\\nm = {:.2f}, b = {:.2f}\".format(m_unet,b_unet))\n",
    "plt.colorbar()\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_unet*xfit + b_unet\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "plt.imshow(ct_classical/np.max(ct_classical))\n",
    "#plt.title(\"Classical counts + line info (threshold = {})\\nm = {:.2f}, b = {:.2f}\".format(ct_th,m_classical,b_classical))\n",
    "plt.title(\"Classical counts + line info\\nm = {:.2f}, b = {:.2f}\".format(m_classical,b_classical))\n",
    "plt.colorbar()\n",
    "xfit = np.arange(0,ncols-1,0.1)\n",
    "yfit = m_classical*xfit + b_classical\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows-1)],yfit[(yfit > 0) & (yfit < nrows-1)],color='red',linewidth=2)\n",
    "\n",
    "print(\"Total counts, truth:\",np.sum(label))\n",
    "print(\"Total counts, classical:\",np.sum(ct_classical))\n",
    "print(\"Total counts, unet:\",np.sum(ct_unet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "frame_mult = 1./(1.-min(hdist_frame))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(bcenters_frame,frame_mult*(hdist_frame-min(hdist_frame)),'.-',color='black',label='frame')\n",
    "plt.plot(bcenters_label,hdist_label,'.-',color='green',label='true')\n",
    "plt.plot(bcenters_unet,hdist_unet,'.-',color='blue',label='UNet')\n",
    "plt.plot(bcenters_classical,hdist_classical,'.-',color='red',label='classical+line')\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([0.0,3.2])\n",
    "plt.xlabel(\"Distance from line (micrometers)\")\n",
    "plt.ylabel(\"Mean number of normalized counts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save s-curve information.\n",
    "np.savez(\"scurve_edge_th08_epoch10.npz\", bcenters_frame=bcenters_frame, hdist_frame=hdist_frame, bcenters_label=bcenters_label, hdist_label=hdist_label, \n",
    "         bcenters_unet=bcenters_unet, hdist_unet=hdist_unet, bcenters_classical=bcenters_classical, \n",
    "         hdist_classical=hdist_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.plot(mrng_frame/m_frame,Lrng_m_frame,color='black',label='frame')\n",
    "plt.plot(mrng_label/m_label,Lrng_m_label,color='green',label='true')\n",
    "plt.plot(mrng_unet/m_unet,Lrng_m_unet,color='blue',label='UNet')\n",
    "plt.plot(mrng_classical/m_classical,Lrng_m_classical,color='red',label='classical')\n",
    "plt.xlabel(\"Parameter m/m$_0$\")\n",
    "plt.ylabel(\"Relative loss L/L(m$_0$)\")\n",
    "plt.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.plot(brng_frame/b_frame,Lrng_b_frame,color='black',label='frame')\n",
    "plt.plot(brng_label/b_label,Lrng_b_label,color='green',label='true')\n",
    "plt.plot(brng_unet/b_unet,Lrng_b_unet,color='blue',label='UNet')\n",
    "plt.plot(brng_classical/b_classical,Lrng_b_classical,color='red',label='classical')\n",
    "plt.xlabel(\"Parameter b/b$_0$\")\n",
    "plt.ylabel(\"Relative loss L/L(b$_0$)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"evt_arrays.npz\",evt_arrays=l_evt_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot s-curve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scurve_noedge = np.load(\"scurve_noedge_th088.npz\")\n",
    "bc_label_noedge = scurve_noedge['bcenters_label']\n",
    "h_label_noedge = scurve_noedge['hdist_label']\n",
    "bc_unet_noedge = scurve_noedge['bcenters_unet']\n",
    "h_unet_noedge = scurve_noedge['hdist_unet']\n",
    "bc_classical_noedge = scurve_noedge['bcenters_classical']\n",
    "h_classical_noedge = scurve_noedge['hdist_classical']\n",
    "\n",
    "scurve_edge = np.load(\"scurve_edge_th08.npz\")\n",
    "bc_frame_edge = scurve_edge['bcenters_frame']\n",
    "h_frame_edge = scurve_edge['hdist_frame']\n",
    "bc_label_edge = scurve_edge['bcenters_label']\n",
    "h_label_edge = scurve_edge['hdist_label']\n",
    "bc_unet_edge = scurve_edge['bcenters_unet']\n",
    "h_unet_edge = scurve_edge['hdist_unet']\n",
    "bc_classical_edge = scurve_edge['bcenters_classical']\n",
    "h_classical_edge = scurve_edge['hdist_classical']\n",
    "\n",
    "scurve_edge10 = np.load(\"scurve_edge_th08_epoch10.npz\")\n",
    "bc_label_edge10 = scurve_edge10['bcenters_label']\n",
    "h_label_edge10 = scurve_edge10['hdist_label']\n",
    "bc_unet_edge10 = scurve_edge10['bcenters_unet']\n",
    "h_unet_edge10 = scurve_edge10['hdist_unet']\n",
    "bc_classical_edge10 = scurve_edge10['bcenters_classical']\n",
    "h_classical_edge10 = scurve_edge10['hdist_classical']\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "frame_mult = 1./(1.-min(h_frame_edge))\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "plt.plot(bc_frame_edge,frame_mult*(h_frame_edge-min(h_frame_edge)),'.-',color='black',label='raw frame, scaled')\n",
    "plt.plot(bc_classical_edge,h_classical_edge,'.-',color='red',label='classical')\n",
    "plt.plot(bc_unet_noedge,h_unet_noedge,'.-',color='orange',label='UNet, no edge, epoch 500')\n",
    "plt.plot(bc_unet_edge10,h_unet_edge10,'.-',color='cyan',label='UNet + edge, epoch 11')\n",
    "plt.plot(bc_unet_edge,h_unet_edge,'.-',color='blue',label='UNet + edge, epoch 500')\n",
    "plt.plot(bc_label_edge,h_label_edge,'.-',color='green',label='true')\n",
    "plt.xlim([-5,5])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.xlabel(\"Distance from line (pixels)\")\n",
    "plt.ylabel(\"Mean number of normalized counts\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examine a large generated frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label[0,0:50,0:50])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sim = frame.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.hist(img_sim[(img_sim < 400)],bins=50)\n",
    "plt.hist(img_sim,bins=50)\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "plt.yscale(\"log\")\n",
    "print(\"Total pixels:\",len(img_sim))\n",
    "#plt.xlim([0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"frame_4855x4855_11occ.npz\",frame=frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a network (multi-electron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/jrenner/local/jerenner/emsim/models'\n",
    "lrate       = 1e-3   # Learning rate to use in the training.\n",
    "load_model  = True   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 200    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/model_init_199.pt\".format(modeldir)\n",
    "\n",
    "# Create the dataset.\n",
    "# 576x576: 2927 +/- 71\n",
    "# 100x100: 88 +/- 2\n",
    "# 50x50: 22 +/- 0.5\n",
    "#dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)\n",
    "\n",
    "# \"Real-data-like\" dataset: occupancy 11, noise_mean=683, noise_sigma=11.2\n",
    "dset = tr.EMDataset(\"dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)\n",
    "#dataset_train   = tr.EMFrameDataset(dset,frame_size=50,nelec_mean=11,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "dataset_train = tr.EMFrameDataset(dset,frame_size=20,nelec_mean=2,nelec_sigma=0.1,noise_mean=683,noise_sigma=11.2,m_line=-2.0,b_line=30.0,res_factor=3)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=50, shuffle=False, collate_fn=tr.my_collate_unet, num_workers=1)\n",
    "\n",
    "# Define the model.\n",
    "model = UNet(n_channels=1, n_classes=1)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lrate, weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    train_loss = tr.train_unet(model, epoch, train_loader, optimizer)\n",
    "    scheduler.step(train_loss)\n",
    "    #if(epoch % 50 == 0):\n",
    "    torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"{}/model_frames_20x20_noise683_2e_bcsloss_noedge_unweighted_front_1em4_100kev_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy.\n",
    "tloss = np.loadtxt(\"train.txt\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.plot(tloss[:,0],tloss[:,1],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.plot(tloss[:,0],tloss[:,2],label='training')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "#dset = tr.EMDataset(\"dataframes/EM_5um_front_3M_100keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=False,add_shift=0)\n",
    "#dataset_train   = tr.EMFrameDataset(dset,frame_size=50,nelec_mean=11,nelec_sigma=0.5,noise_mean=683,noise_sigma=11.2)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over many events and evaluate the true positives and false positives.\n",
    "# Store in arrays as:\n",
    "#\n",
    "#  [tp0 tp1 tp2 tp3 ... tpN], each number corresponding to a different NN threshold or classical threshold\n",
    "#\n",
    "tp_unet = []; fp_unet = []\n",
    "tp_classical = []; fp_classical = []\n",
    "#nn_thresholds = np.arange(0.05,1.0,0.1)\n",
    "nn_thresholds = np.concatenate((np.logspace(-4,-0.1,500), np.logspace(-0.1,0,500)))\n",
    "#nn_thresholds = np.logspace(-3,0,1000)\n",
    "classical_thresholds = np.arange(600/4,7000/4,10)\n",
    "evts = np.arange(100000,101000)\n",
    "for evt in evts:\n",
    "    \n",
    "    # Get the event and truth.\n",
    "    evt_item = dataset_train[evt]\n",
    "    evt_arr = evt_item[0]\n",
    "    evt_lbl = evt_item[1][0]\n",
    "    \n",
    "    # Send through the model.\n",
    "    data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    output_score = model(data)\n",
    "    \n",
    "    # Compute the predicted pixel values.\n",
    "    prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()\n",
    "    \n",
    "    # Compute the TP and FP values for unet.\n",
    "    temp_tp = []; temp_fp = []\n",
    "    for th in nn_thresholds:\n",
    "        pred = (prob > th)\n",
    "        tp = np.sum((evt_lbl == 1) & (pred == True))\n",
    "        fn = np.sum((evt_lbl == 1) & (pred == False))\n",
    "        if( (tp + fn) > 0): tp = tp / (tp + fn)\n",
    "        else: tp = 1\n",
    "        fp = np.sum((evt_lbl == 0) & (pred == True))\n",
    "        tn = np.sum((evt_lbl == 0) & (pred == False))\n",
    "        if( (fp + tn) > 0): fp = fp / (fp + tn)\n",
    "        else: fp = 0\n",
    "        temp_tp.append(tp)\n",
    "        temp_fp.append(fp)\n",
    "    tp_unet.append(temp_tp)\n",
    "    fp_unet.append(temp_fp)\n",
    "    \n",
    "    # Compute the TP and FP values for the classical threshold.\n",
    "    temp_tp = []; temp_fp = []\n",
    "    for th in classical_thresholds:\n",
    "        #pred = (evt_arr > th)\n",
    "        pred = np.zeros(evt_arr.shape)\n",
    "        pred[np.unravel_index(np.argmax(evt_arr),evt_arr.shape)] = 1\n",
    "        tp = np.sum((evt_lbl == 1) & (pred == True))\n",
    "        fn = np.sum((evt_lbl == 1) & (pred == False))\n",
    "        if( (tp + fn) > 0): tp = tp / (tp + fn)\n",
    "        else: tp = 1\n",
    "        fp = np.sum((evt_lbl == 0) & (pred == True))\n",
    "        tn = np.sum((evt_lbl == 0) & (pred == False))\n",
    "        if( (tp + tn) > 0): fp = fp / (fp + tn)\n",
    "        else: tp = 0\n",
    "        temp_tp.append(tp)\n",
    "        temp_fp.append(fp)\n",
    "    tp_classical.append(temp_tp)\n",
    "    fp_classical.append(temp_fp)\n",
    "    \n",
    "    if((evt-evts[0]) % (len(evts)/100) == 0):\n",
    "            print(\"{}% done\".format(int((evt-evts[0]) / (len(evts)/100))))\n",
    "    \n",
    "tp_unet = np.array(tp_unet)\n",
    "fp_unet = np.array(fp_unet)\n",
    "tp_classical = np.array(tp_classical)\n",
    "fp_classical = np.array(fp_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp_rate_unet = np.mean(tp_unet,axis=0)\n",
    "tp_err_unet = np.std(tp_unet,axis=0)/np.sqrt(tp_unet.shape[0])\n",
    "fp_rate_unet = np.mean(fp_unet,axis=0)\n",
    "fp_err_unet = np.std(fp_unet,axis=0)/np.sqrt(fp_unet.shape[0])\n",
    "tp_rate_classical = np.mean(tp_classical,axis=0)\n",
    "tp_err_classical = np.std(tp_classical,axis=0)/np.sqrt(tp_classical.shape[0])\n",
    "fp_rate_classical = np.mean(fp_classical,axis=0)\n",
    "fp_err_classical = np.std(fp_classical,axis=0)/np.sqrt(fp_classical.shape[0])\n",
    "\n",
    "plt.errorbar(fp_rate_unet,tp_rate_unet,xerr=fp_err_unet,yerr=tp_err_unet,label='Unet')\n",
    "plt.errorbar(fp_rate_classical,tp_rate_classical,xerr=fp_err_classical,yerr=tp_err_classical,label='Basic threshold')\n",
    "plt.xlim([0,0.02])\n",
    "plt.legend()\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NN ------------------\")\n",
    "for tpr,fpr,th in zip(tp_rate_unet,fp_rate_unet,nn_thresholds):\n",
    "    print(\"[Threshold {}] TP = {}, FP = {}\".format(th,tpr,fpr))\n",
    "print(\"Classical threshold ------------------\")\n",
    "for tpr,fpr,th in zip(tp_rate_classical,fp_rate_classical,classical_thresholds):\n",
    "    print(\"[Threshold {}] TP = {}, FP = {}\".format(th,tpr,fpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 10008\n",
    "evt_item = dataset_train[evt_plt]\n",
    "evt_arr = evt_item[0]\n",
    "evt_lbl = evt_item[1][1]\n",
    "\n",
    "# Send through the model.\n",
    "data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "#target = torch.tensor(evt_lbl).float().cuda()\n",
    "output_score = model(data)\n",
    "\n",
    "# Compute the predicted pixel and (x,y) values.\n",
    "prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()\n",
    "\n",
    "# Threshold\n",
    "# prob = np.zeros(evt_arr.shape)\n",
    "# prob[evt_arr > 80] = 1\n",
    "\n",
    "# Determine number of correct pixels\n",
    "th = 0.2\n",
    "pred = (prob > th)\n",
    "nelec = int(np.sum(evt_lbl == 1))\n",
    "nelec_pred = int(np.sum(pred))\n",
    "nspace = int(np.sum(evt_lbl == 0))\n",
    "nelec_coinc = np.sum((evt_lbl == 1) & (pred == True))\n",
    "nspace_coinc = np.sum((evt_lbl == 0) & (pred == False))\n",
    "print(\"{}/{} electrons predicted\".format(nelec_pred,nelec))\n",
    "print(\"{}/{} electrons coincided exactly\".format(nelec_coinc,nelec))\n",
    "print(\"{}/{} empty spaces coincided exactly\".format(nspace_coinc,nspace))\n",
    "\n",
    "# Information for drawing the line.\n",
    "nrows = evt_arr.shape[0]\n",
    "ncols = evt_arr.shape[1]\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "m = -2*nrows/ncols\n",
    "b = 80\n",
    "print(\"Line drawn: m = {}, b = {}\".format(m,b))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_arr)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event\")\n",
    "\n",
    "# xfit = np.arange(0,ncols,0.1)\n",
    "# yfit = m*xfit + b\n",
    "# plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)])\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "plt.imshow(evt_lbl)\n",
    "plt.colorbar()\n",
    "plt.title(\"Target\")\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "#plt.imshow(np.log10(prob))\n",
    "plt.imshow(prob)\n",
    "plt.colorbar()\n",
    "plt.title(\"{}/{} electrons predicted\\n{}/{} electrons coincided exactly\\n{}/{} empty spaces coincided exactly\".format(nelec_pred,nelec,nelec_coinc,nelec,nspace_coinc,nspace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct output and label arrays for 5 events.\n",
    "frames,outputs,labels = [], [], []\n",
    "for iframe in range(50):\n",
    "    frame,label = frameset[iframe]\n",
    "    frames.append(frame)\n",
    "    outputs.append(label[1])\n",
    "    labels.append(label)\n",
    "frames = np.array(frames)\n",
    "outputs = np.array(outputs)\n",
    "labels = np.array(labels)\n",
    "print(\"Frames shape is:\",frames.shape)\n",
    "print(\"Outputs shape is:\",outputs.shape)\n",
    "print(\"Labels shape is:\",labels.shape)\n",
    "\n",
    "# Convert to tensors.\n",
    "outputs[outputs == 0] = 1e-10\n",
    "outputs[outputs == 1] = 0.99999999\n",
    "frames = torch.tensor(frames)\n",
    "output = torch.tensor(np.log(outputs/(1-outputs)))\n",
    "target = torch.tensor(labels)\n",
    "\n",
    "# Compute the loss.\n",
    "sigma_dist = 1\n",
    "real_truth = target[:,0,:,:]\n",
    "th_truth = target[:,1,:,:]\n",
    "edge_truth = target[:,2,:,:]\n",
    "dist = target[:,3,:,:]\n",
    "\n",
    "final_truth = th_truth * edge_truth\n",
    "\n",
    "wts     = torch.sum(torch.exp(-(dist)**2/(2*sigma_dist**2))*th_truth,axis=(1,2))\n",
    "wt_norm = torch.sum(th_truth,axis=(1,2))\n",
    "wt_norm[wt_norm == 0] = 1\n",
    "wts /= wt_norm\n",
    "print(\"Weights are: \",wts)\n",
    "\n",
    "w_edge = 100\n",
    "bce_loss = torch.nn.BCEWithLogitsLoss(reduce=False)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "loss0 = bce_loss(output,final_truth)\n",
    "loss1 = w_edge*sigmoid(output)*(1-edge_truth)\n",
    "loss0W = torch.exp(-(dist)**2/(2*sigma_dist**2))*(loss0)\n",
    "loss1W = torch.exp(-(dist)**2/(2*sigma_dist**2))*(loss1)\n",
    "loss = torch.mean(torch.exp(-(dist)**2/(2*sigma_dist**2))*(loss0 + loss1))\n",
    "print(\"Mean loss is \",loss)\n",
    "print(\"Mean loss0 is \",torch.mean(loss0))\n",
    "print(\"Mean loss0W is \",torch.mean(loss0W))\n",
    "print(\"Mean loss1 is \",torch.mean(loss1))\n",
    "print(\"Mean loss1W is \",torch.mean(loss1W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mod = torch.sum(torch.abs(dist*(edge_truth-1)),axis=(1,2))\n",
    "dist_mod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iframe = 2\n",
    "show_sum = False\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(16.0)\n",
    "\n",
    "ax1 = fig.add_subplot(241)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(sigmoid(output)),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(sigmoid(output[iframe])),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"sigmoid(Output)\")\n",
    "\n",
    "ax2 = fig.add_subplot(242)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(th_truth),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(real_truth[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Real truth\")\n",
    "\n",
    "ax3 = fig.add_subplot(243)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(edge_truth), axis=0))\n",
    "else: plt.imshow(np.array(edge_truth[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Edge truth\")\n",
    "\n",
    "ax4 = fig.add_subplot(244)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(torch.exp(-(dist)**2/(2*sigma_dist**2))),axis=0))\n",
    "else: plt.imshow(np.array(torch.exp(-(dist)**2/(2*sigma_dist**2))[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Distance\")\n",
    "\n",
    "ax5 = fig.add_subplot(245)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(loss0), axis=0))\n",
    "else: plt.imshow(np.array(loss0[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Loss0\")\n",
    "\n",
    "ax6 = fig.add_subplot(246)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(loss1),axis=0))\n",
    "else: plt.imshow(np.array(loss1[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Loss1\")\n",
    "\n",
    "ax7 = fig.add_subplot(247)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(loss0W + loss1W),axis=0))\n",
    "else: plt.imshow(np.array(loss0W[iframe] + loss1W[iframe]))\n",
    "# if(show_sum): plt.imshow(np.sum(np.array(dist_mod),axis=0))\n",
    "# else: plt.imshow(np.array(dist_mod[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"LossW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iframe = 2\n",
    "show_sum = False\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(3.0)\n",
    "fig.set_figwidth(20.0)\n",
    "\n",
    "ax1 = fig.add_subplot(151)\n",
    "if(show_sum): plt.imshow(np.sum(frames,axis=0)/np.max(np.sum(frames,axis=0)),interpolation=None)\n",
    "else: plt.imshow(np.array(frames[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event\")\n",
    "\n",
    "ax2 = fig.add_subplot(152)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(real_truth),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(real_truth[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Real truth\")\n",
    "\n",
    "ax3 = fig.add_subplot(153)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(th_truth),axis=0),interpolation=None)\n",
    "else: plt.imshow(np.array(th_truth[iframe]),interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.title(\"Classical threshold\")\n",
    "\n",
    "ax4 = fig.add_subplot(154)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(edge_truth), axis=0))\n",
    "else: plt.imshow(np.array(edge_truth[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Edge truth\")\n",
    "\n",
    "ax5 = fig.add_subplot(155)\n",
    "if(show_sum): plt.imshow(np.sum(np.array(final_truth), axis=0))\n",
    "else: plt.imshow(np.array(final_truth[iframe]))\n",
    "plt.colorbar()\n",
    "plt.title(\"Combined truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate real data\n",
    "img_data_cut = img_data[-1023:,-1440:]/12\n",
    "img_data_torch = torch.tensor(img_data_cut).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "output_score = model(img_data_torch)\n",
    "prob = np.array(sigmoid(output_score).cpu().detach().numpy()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of the real data evaluation\n",
    "logscale = False\n",
    "view_row_low = -350\n",
    "view_row_high = -300\n",
    "view_col_low = -350\n",
    "view_col_high = -300\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "if(logscale):\n",
    "    plt.imshow(np.log(img_data_cut[view_row_low:view_row_high,view_col_low:view_col_high]),interpolation='none')\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"log(counts)\")\n",
    "    plt.title(\"Scaled data (log counts)\")\n",
    "else:\n",
    "    plt.imshow(img_data_cut[view_row_low:view_row_high,view_col_low:view_col_high],interpolation='none') #np.log(frame))\n",
    "    cbar = plt.colorbar()\n",
    "    plt.title(\"Scaled data\")\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.imshow(prob[view_row_low:view_row_high,view_col_low:view_col_high],interpolation='none')\n",
    "plt.title(\"U-net output\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stackoverflow.com/questions/1035340/reading-binary-file-and-looping-over-each-byte\n",
    "def bytes_from_file(filename, chunksize=4):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunksize)\n",
    "            if chunk:\n",
    "                yield struct.unpack('@I', chunk)[0]\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datfile = \"/home/jrenner/local/data/electronsim/stack_1.dat\"\n",
    "freader = iter(bytes_from_file(datfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for i in range(5760*4092):\n",
    "    img.append(next(freader))\n",
    "img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for input to a NN\n",
    "img_data = img.reshape([4092,5760])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.imshow(np.log(img.reshape([5760,4092])),vmin=9.5,vmax=10.5)\n",
    "#plt.imshow(img.reshape([5760,4092])[-100:,0:100],vmin=750,vmax=10000)\n",
    "plt.imshow(img_data,interpolation='none',vmin=750,vmax=15000)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to fit the edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_th = 750*12\n",
    "max_th = 751*12\n",
    "fit_img = np.copy(img_data)\n",
    "fit_img[fit_img < noise_th] = 0\n",
    "fit_img[fit_img >= noise_th] = max_th\n",
    "fit_img = fit_img/np.max(fit_img)\n",
    "fit_img = np.array(fit_img,dtype=np.uint8)\n",
    "print(\"Min value:\",np.min(fit_img),\"; max value:\",np.max(fit_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(fit_img,interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = fit_img\n",
    "th = 0.5\n",
    "nrows = A.shape[0]\n",
    "ncols = A.shape[1]\n",
    "ncts = np.sum(A >= th)\n",
    "nzeros = np.sum(A < th)\n",
    "wcts = nzeros/ncts\n",
    "indices = np.indices((nrows,ncols))\n",
    "irows = indices[0]\n",
    "icols = indices[1]\n",
    "\n",
    "def count_loss(x):\n",
    "    m,b = x\n",
    "    \n",
    "    # The loss L is:\n",
    "    #\n",
    "    # (number of 0s in the dark region) - wcts*(number of 1s in the dark region)\n",
    "    # + wcts*(number of 1s in the light region) - (number of 0s in the dark region)\n",
    "    # \n",
    "    # where wcts is the count weight, determined such that the number of counts multiplied by wcts is equal to\n",
    "    # the number of zeros.\n",
    "    L = 0\n",
    "    L1 = np.sum((irows < m*icols + b) & (A < th))\n",
    "    L2 = np.sum((irows < m*icols + b) & (A >= th))\n",
    "    L3 = np.sum((irows >= m*icols + b) & (A >= th))\n",
    "    L4 = np.sum((irows >= m*icols + b) & (A < th))\n",
    "    \n",
    "    L = L1 - wcts*L2 + wcts*L3 - L4\n",
    "    print(\"Loss is:\",-L,\"with L1 =\",L1,\"L2 =\",L2,\"L3 =\",L3,\"L4 =\",L4)\n",
    "    return -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initial_guess = [-nrows/ncols,nrows]\n",
    "result = optimize.minimize(count_loss,initial_guess,method='Nelder-Mead',tol=1e-6)\n",
    "m,b = result.x\n",
    "Lmin = result.fun\n",
    "print(\"m = \",m,\"b = \",b,\"Lmin=\",Lmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(fit_img,interpolation='none')\n",
    "xfit = np.arange(ncols)\n",
    "yfit = m*xfit + b\n",
    "plt.plot(xfit[(yfit > 0) & (yfit < nrows)],yfit[(yfit > 0) & (yfit < nrows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss in a range near the parameters.\n",
    "mrng = np.arange(m-0.1*m, m+0.1*m, 0.2*m/100)\n",
    "Lrng = np.array([count_loss([mval,b])/Lmin for mval in mrng])\n",
    "print(mrng)\n",
    "plt.plot(mrng,Lrng)\n",
    "plt.xlabel(\"Parameter m\")\n",
    "plt.ylabel(\"Relative loss L/L(m$_0$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_canny(image, sigma=0.1):\n",
    "    v = 255/2. #np.median(image)\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "    print(\"Lower =\",lower,\", upper=\",upper)\n",
    "    return cv2.Canny(image, lower, upper)\n",
    "\n",
    "edges = auto_canny(image=fit_img) \n",
    "\n",
    "# Show images for testing\n",
    "#cv2.imshow('edges', edges)\n",
    "plt.imshow(edges)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the noise peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fscale = 12.\n",
    "\n",
    "def gauss(x, amp, mu, sigma):\n",
    "    if sigma <= 0.:\n",
    "        return np.inf\n",
    "    return amp/(2*np.pi)**(0.5)/sigma * np.exp(-0.5*(x-mu)**2./sigma**2)\n",
    "\n",
    "def gaussexpo(x, amp, mu, sigma, const, mean, x0):\n",
    "    if sigma <= 0.:\n",
    "        return np.inf\n",
    "    return amp/(2*np.pi)**(0.5)/sigma * np.exp(-0.5*(x-mu)**2./sigma**2) + const * np.exp(-(x-x0)/mean)\n",
    "\n",
    "\n",
    "yh, xh, _ = plt.hist(img[(img/fscale > 7000/fscale) & (img/fscale < 9300/fscale)]/fscale,bins=50)\n",
    "xh = (xh[1:] + xh[0:-1])/2\n",
    "\n",
    "#popt, pcov = curve_fit(gaussexpo, xh, yh, [3.0e6, 8200, 300, 1000, 10, -1])\n",
    "popt, pcov = curve_fit(gauss, xh, yh, [3.0e6, 8200/fscale, 135/fscale])\n",
    "xfit = np.linspace(xh[0],xh[-1],100)\n",
    "plt.plot(xfit,gauss(xfit,*popt))\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "print(\"Fit mean:\",popt[1])\n",
    "print(\"Fit sigma:\",popt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fn = np.load(\"frame_4855x4855_11occ.npz\")\n",
    "img_sim = fn['frame'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(img[(img > 0) & (img < 10000)],bins=50)\n",
    "plt.hist(img_sim,bins=50,range=[0,10000],label='MC')\n",
    "plt.hist(img/12,bins=50,range=[0,10000],label='data')\n",
    "\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "print(\"Total pixels:\",len(img))\n",
    "print(\"Counts near peak\",np.sum(img[(img > 29) & (img < 33)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "177383690/935130034."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.abs(np.random.normal(loc=0,scale=50,size=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a network (single-electrons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeldir = '/home/jrenner/local/jerenner/emsim/models'\\\n",
    "modeldir = '/home/jrenner/temp/nersc'\n",
    "lrate       = 1e-3   # Learning rate to use in the training.\n",
    "load_model  = True   # Load an existing model\n",
    "tr.augment  = False  # Enable/disable data augmentation\n",
    "epoch_start = 0      # Number of initial epoch\n",
    "epoch_end   = 2000    # Number of final epoch\n",
    "model_load_checkpoint = \"{}/run_11x11_chi32_60/model_init_599.pt\".format(modeldir)\n",
    "\n",
    "# Create the datasets.\n",
    "dataset_all   = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,add_shift=0)\n",
    "dataset_train = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,nstart=0,nend=-20000,add_shift=0)\n",
    "dataset_val   = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,nstart=-20000,add_shift=0)\n",
    "\n",
    "# Create the loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=1000, shuffle=True, collate_fn=tr.my_collate, num_workers=8)\n",
    "val_loader = DataLoader(dataset_val, batch_size=1000, shuffle=True, collate_fn=tr.my_collate, num_workers=8)\n",
    "#val_loader = DataLoader(dataset_val, batch_size=15, shuffle=True, collate_fn=tr.my_collate, num_workers=4)\n",
    "#test_loader = DataLoader(dataset_test, batch_size=15, shuffle=True, collate_fn=tr.my_collate, num_workers=4)\n",
    "\n",
    "# Define the model.\n",
    "#model = emnet.FCNet()\n",
    "model = emnet.basicCNN()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lrate, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01, amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Load the model from file.\n",
    "if(load_model):\n",
    "    model.load_state_dict(torch.load(model_load_checkpoint))\n",
    "    #model.load_state_dict(torch.load(model_load_checkpoint,map_location=torch.device('cpu')))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training.\n",
    "#print(\"Training with weights\",sort_clsweights)\n",
    "for epoch in range(epoch_start,epoch_end):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    model.train()\n",
    "    tr.train(model, epoch, train_loader, optimizer)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = tr.val(model, epoch, val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "#     if(epoch % 50 == 0):\n",
    "#         torch.save(model.state_dict(), \"{}/model_init_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"{}/model_short_training_{}.pt\".format(modeldir,epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses.\n",
    "tloss = np.loadtxt(\"/home/jrenner/temp/nersc/run_11x11_chi32_60_val2_trainsched/train.txt\")\n",
    "vloss = np.loadtxt(\"/home/jrenner/temp/nersc/run_11x11_chi32_60_val2_trainsched/val.txt\")\n",
    "#vloss = np.loadtxt(\"/home/jrenner/local/jerenner/emsim/val.txt\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(12.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.plot(tloss[:,0],tloss[:,1],label='training')\n",
    "plt.plot(vloss[:,0],vloss[:,1],label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.yscale('log')\n",
    "\n",
    "ax2 = fig.add_subplot(122);\n",
    "plt.plot(tloss[:,0],tloss[:,2],label='training')\n",
    "plt.plot(vloss[:,0],vloss[:,2],label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all events from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,add_shift=0)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evts = np.arange(100000,110000)\n",
    "df, evts = emsim_utils.construct_evt_dataframe(dset,evts,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = df['xc_3x3'].values\n",
    "yvals = df['yc_3x3'].values\n",
    "rngval = 0.002\n",
    "plt.hist2d(xvals,yvals,bins=10,range=[[-rngval,rngval],[-rngval,rngval]])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = pd.plotting.scatter_matrix(df[[\"error_r_NN\",\"error_r_maxpt\",\"error_r_3x3\",\"error_r_3x3_th\",\"error_r_5x5\",\"error_r_5x5_th\"]], \n",
    "                                  figsize=[15,15], alpha=0.2, hist_kwds={'bins':100})\n",
    "for i, axs in enumerate(axes):\n",
    "    for j, ax in enumerate(axs):\n",
    "        #if i == j:  # only the histograms\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlim(0,0.01)\n",
    "        ax.set_ylim(0,0.01)\n",
    "            \n",
    "plt.savefig(\"errors_scatter_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_cut = 0.1\n",
    "rng_cut = 0.005\n",
    "sigma_cut = 1e9\n",
    "nbins = 50\n",
    "\n",
    "df_plt_NN  = df[(df.error_r_3x3 < err_cut)] # & (df.sigma_r_NN < sigma_cut)]\n",
    "df_plt_3x3 = df[(df.error_r_3x3 < err_cut)] # & (df.sigma_r_NN < sigma_cut)]\n",
    "\n",
    "plt.hist(df_plt_NN.error_r_NN,range=(0,rng_cut),alpha=0.8,bins=nbins,color='blue',label='NN error')\n",
    "plt.hist(df_plt_3x3.error_r_3x3,range=(0,rng_cut),alpha=0.8,bins=nbins,color='green',label='3x3 centroid')\n",
    "plt.xlabel(\"error $\\sqrt{\\Delta x^2 + \\Delta y^2}$ (mm)\")\n",
    "plt.ylabel(\"counts/bin\")\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "print(\"NN events:\",len(df_plt_NN[df_plt_NN.error_r_NN < rng_cut]))\n",
    "print(\"3x3 events:\",len(df_plt_3x3[df_plt_3x3.error_r_3x3 < rng_cut]))\n",
    "print(\"Mean NN error:\",np.mean(df_plt_NN[df_plt_NN.error_r_NN < rng_cut].error_r_NN))\n",
    "print(\"Mean 3x3 error:\",np.mean(df_plt_3x3[df_plt_3x3.error_r_3x3 < rng_cut].error_r_3x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean 3x3 error:\",df[(df.error_r_3x3 < err_cut) & (df.sigma_r_NN < sigma_cut)].error_r_3x3.mean())\n",
    "print(\"Mean NN error: \",df[(df.error_r_NN < err_cut) & (df.sigma_r_NN < sigma_cut)].error_r_NN.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"error_r_diff\"] = df.error_r_NN - df.error_r_3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "err_diff_cut = 0.005\n",
    "plt.hist(df[(df.error_r_diff < err_diff_cut) & (df.error_r_diff > -err_diff_cut) & (df.sigma_r_NN < 0.011)].error_r_diff,alpha=0.8,bins=50,color='blue',label='NN error')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"error difference (NN - 3x3-method)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.x_true > 0.02][['event','x_true']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe and event arrays.\n",
    "df.to_pickle(\"evts_80000_to_90000.pkl\")\n",
    "np.savez(\"evt_arrays.npz\",evt_arrays=l_evt_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"evts_80000_to_90000.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sigma = 0.011\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(15.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.hist(df[df.sigma_r_NN < cut_sigma].error_r_NN,bins=50)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"error $\\sqrt{\\Delta x^2 + \\Delta y^2}$ (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.hist(df[df.sigma_r_NN < cut_sigma].sigma_r_NN,bins=50)\n",
    "plt.xlabel(\"$\\sqrt{\\sigma_x^2 + \\sigma_y^2}$ of probability distribution (mm)\")\n",
    "plt.ylabel(\"Counts/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff, mean_err = [], []\n",
    "cut_sigmas = np.arange(0.003,0.4,0.0005)\n",
    "for cut_sigma in cut_sigmas:\n",
    "    df_cut = df[df.sigma_r_NN < cut_sigma]\n",
    "    \n",
    "    eff.append(len(df_cut)/len(df))\n",
    "    mean_err.append(df_cut.error_r_NN.mean())\n",
    "    \n",
    "    print(\"[SIGMA = {}]: EFF = {}, ERR = {}\".format(cut_sigma,len(df_cut)/len(df),df_cut.error_r_NN.mean()))\n",
    "\n",
    "eff = np.array(eff)\n",
    "mean_err = np.array(mean_err)\n",
    "plt.plot(mean_err,eff,'.-')\n",
    "plt.xlabel(\"Mean error (mm)\")\n",
    "plt.ylabel(\"Efficiency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the net for individual events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_noise=True,add_shift=0)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 80388\n",
    "evt_item = dset[evt_plt]\n",
    "evt_arr = evt_item[0]\n",
    "evt_lbl = evt_item[1]\n",
    "evt_err_ind = evt_item[2]\n",
    "\n",
    "SHIFTED_ERR_RANGE_MIN = emnet.PIXEL_ERR_RANGE_MIN # - dset.add_shift*emnet.PIXEL_SIZE\n",
    "SHIFTED_ERR_RANGE_MAX = emnet.PIXEL_ERR_RANGE_MAX # + dset.add_shift*emnet.PIXEL_SIZE\n",
    "#ERR_PIXEL_SIZE = emnet.PIXEL_SIZE*(2*dset.add_shift+1)/emnet.ERR_SIZE\n",
    "ERR_PIXEL_SIZE = (emnet.PIXEL_ERR_RANGE_MAX - emnet.PIXEL_ERR_RANGE_MIN)/emnet.ERR_SIZE\n",
    "print(\"Error grid pixel size is {}\".format(ERR_PIXEL_SIZE))\n",
    "\n",
    "x_errgrid = np.arange(0,emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "y_errgrid = np.arange(0,emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "print(x_errgrid)\n",
    "\n",
    "xbin = int(emnet.ERR_SIZE*(evt_lbl[0] - SHIFTED_ERR_RANGE_MIN)/(SHIFTED_ERR_RANGE_MAX - SHIFTED_ERR_RANGE_MIN))\n",
    "xbin = max(xbin,0)\n",
    "xbin = min(xbin,emnet.ERR_SIZE-1)\n",
    "\n",
    "ybin = int(emnet.ERR_SIZE*(evt_lbl[1] - SHIFTED_ERR_RANGE_MIN)/(SHIFTED_ERR_RANGE_MAX - SHIFTED_ERR_RANGE_MIN))\n",
    "ybin = max(ybin,0)\n",
    "ybin = min(ybin,emnet.ERR_SIZE-1)\n",
    "\n",
    "print(\"Computed index:\",(ybin*emnet.ERR_SIZE) + xbin,\"for max added shift:\",dset.add_shift)\n",
    "\n",
    "# Send through the model.\n",
    "data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "target = torch.tensor(np.array(evt_err_ind)).long().cuda()\n",
    "output_score = model(data)\n",
    "\n",
    "# Compute the predicted pixel and (x,y) values.\n",
    "prob = np.array(softmax(output_score).cpu().detach().numpy()).reshape([emnet.ERR_SIZE,emnet.ERR_SIZE])\n",
    "ipred = np.argmax(prob)\n",
    "xpred = int(ipred % emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "ypred = int(ipred / emnet.ERR_SIZE)*ERR_PIXEL_SIZE + SHIFTED_ERR_RANGE_MIN + ERR_PIXEL_SIZE/2\n",
    "#print(\"[Evt\",evt,\"]: Index is\",evt_err_ind,\"with predicted\",ipred,\"; x = {} (predicted {}), y = {} (predicted {})\".format(evt_lbl[0],xpred,evt_lbl[1],ypred))\n",
    "\n",
    "# Compute the sigmas of the distribution.\n",
    "sigma_x0, sigma_y0 = emsim_utils.compute_sigmas(prob,ERR_PIXEL_SIZE,SHIFTED_ERR_RANGE_MIN)\n",
    "popt, pcov = emsim_utils.fit_sigmas(prob,x_errgrid,y_errgrid,xpred,ypred,sigma_x0,sigma_y0,ERR_PIXEL_SIZE)\n",
    "fit_data = emsim_utils.mult_gaussFun_Fit((x_errgrid,y_errgrid),*popt).reshape([emnet.ERR_SIZE,emnet.ERR_SIZE])\n",
    "print(\"Gaussian fit parameters A*exp(-0.5*((x-x0)**2/varX + (y-y0)**2/varY)) + C:\")\n",
    "print(\"A = {}\".format(popt[0]))\n",
    "print(\"(x0, y0) = ({},{})\".format(popt[1],popt[2]))\n",
    "print(\"(sigma_x, sigma_y) = ({},{})\".format(popt[3]**0.5,popt[4]**0.5))\n",
    "print(\"C = {}\".format(popt[5]))\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4.0)\n",
    "fig.set_figwidth(18.0)\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "plt.imshow(evt_arr)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event {}; shift ({:.3e},{:.3e}); index {}\".format(evt_plt,evt_lbl[0],evt_lbl[1],evt_err_ind))\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot([xbin],[ybin],color='red',marker='o',markersize=10)\n",
    "plt.imshow(prob)\n",
    "plt.colorbar()\n",
    "plt.title(\"Incidence point within prediction grid\")\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "plt.imshow(fit_data)\n",
    "plt.colorbar()\n",
    "plt.title(\"2D Gaussian fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc. code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at many events:\n",
    "xpred_err, ypred_err = [], []\n",
    "for evt_plt in np.arange(8000,9999):\n",
    "    \n",
    "    evt_item = dset[evt_plt]\n",
    "    evt_arr = evt_item[0]\n",
    "    evt_lbl = evt_item[1]\n",
    "    evt_err_ind = evt_item[2]\n",
    "\n",
    "    # Send through the model.\n",
    "    data = torch.tensor(evt_arr).float().unsqueeze(0).unsqueeze(1).cuda()\n",
    "    target = torch.tensor(np.array(evt_err_ind)).long().cuda()\n",
    "\n",
    "    output_score = model(data)\n",
    "    prob = np.argmax(np.array(softmax(output_score).cpu().detach().numpy()).reshape([10,10]))\n",
    "    xpred = (prob % tr.ERR_SIZE)*0.005/tr.ERR_SIZE + tr.ERR_RANGE_MIN + 0.005/tr.ERR_SIZE/2\n",
    "    ypred = (prob / tr.ERR_SIZE)*0.005/tr.ERR_SIZE + tr.ERR_RANGE_MIN + 0.005/tr.ERR_SIZE/2\n",
    "    print(\"[Evt\",evt_plt,\"]: Index is\",evt_err_ind,\"with predicted\",prob,\"; x = {} (predicted {}), y = {} (predicted {})\".format(evt_lbl[0],xpred,evt_lbl[1],ypred))\n",
    "    \n",
    "    xpred_err.append(xpred-evt_lbl[0])\n",
    "    ypred_err.append(ypred-evt_lbl[1])\n",
    "xpred_err = np.array(xpred_err)\n",
    "ypred_err = np.array(ypred_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figheight(6.0)\n",
    "fig.set_figwidth(15.0)\n",
    "\n",
    "ax1 = fig.add_subplot(121);\n",
    "plt.hist(xpred_err)\n",
    "plt.xlabel(\"error in x-prediction (mm)\")\n",
    "print(np.where(abs(xpred_err) > 0.001))\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.hist(ypred_err)\n",
    "plt.xlabel(\"error in y-prediction (mm)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For debugging the 3x3 sum operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.ones([6,6])\n",
    "aa[0,2] = 4\n",
    "aa[1,2] = 2\n",
    "aa[3,2] = 8\n",
    "aa[4,2] = -2\n",
    "aa[3,1] = 5\n",
    "aa[5,0] = 10\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_init   = np.unravel_index(aa.argmax(),aa.shape)\n",
    "nbsum_init = tr.sum_neighbors(aa,max_init,remove=True)\n",
    "print(\"Max at\",max_init,\"and neighbor sum\",nbsum_init)\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate a dataset for noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nevts = 1000\n",
    "noise_arr = np.arange(0.,100.,50.)\n",
    "r_mean, r_sigma = [], []\n",
    "for noise in noise_arr:\n",
    "    print(\"Running for noise\",noise)\n",
    "    dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",0,noise)\n",
    "    \n",
    "    shifts_x, shifts_y, shifts_r = [], [], []\n",
    "    for evt in range(Nevts):\n",
    "        evt_arr,evt_lbl = dset[evt]\n",
    "        xs,ys = evt_lbl[0],evt_lbl[1]\n",
    "        shifts_x.append(xs)\n",
    "        shifts_y.append(ys)\n",
    "        shifts_r.append((xs**2 + ys**2)**0.5)\n",
    "    \n",
    "    shifts_r = np.array(shifts_r)\n",
    "    r_mean.append(np.mean(shifts_r))\n",
    "    r_sigma.append(np.std(shifts_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(noise_arr,r_mean,yerr=np.array(r_sigma)/Nevts**0.5)\n",
    "plt.xlabel(\"$\\sigma$ noise (electrons)\")\n",
    "plt.ylabel(\"r-error (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a dataset and examine individual events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tr.EMDataset(\"dataframes/EM_4um_back_10M_300keV.pkl\",noise_mean=0,noise_sigma=20,add_shift=10,add_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 8\n",
    "evt_item = dset[evt_plt]\n",
    "evt_arr = evt_item[0]\n",
    "evt_lbl = evt_item[1]\n",
    "evt_err_ind = evt_item[2]\n",
    "plt.imshow(evt_arr)\n",
    "plt.colorbar()\n",
    "plt.title(\"Event {}; shift {}; index {}\".format(evt_plt,evt_lbl,evt_err_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_x, shifts_y, shifts_r = [], [], []\n",
    "for evt in range(1000):\n",
    "    evt_arr,evt_lbl,evt_err_ind = dset[evt]\n",
    "    xs,ys = evt_lbl[0],evt_lbl[1]\n",
    "    shifts_x.append(xs)\n",
    "    shifts_y.append(ys)\n",
    "    shifts_r.append((xs**2 + ys**2)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(shifts_r,bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot events directly from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"dataframes/EM_4um_back_10M_300keV.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt_plt = 97\n",
    "evt_arr = np.zeros([101,101])\n",
    "df_evt = df[df.event == evt_plt]\n",
    "for row,col,counts in zip(df_evt['row'].values,df_evt['col'].values,df_evt['counts'].values):\n",
    "    evt_arr[row,col] += counts\n",
    "plt.imshow(np.log(0.1 + evt_arr))\n",
    "plt.colorbar()\n",
    "plt.title(\"Event {}; max at {}\".format(evt_plt,np.unravel_index(evt_arr.argmax(),evt_arr.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
