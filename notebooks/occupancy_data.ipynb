{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6d1e61-7bb4-48da-8e6c-e730b678799a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import pylandau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9da8f7-1ca1-4368-82d1-fea1f2e88dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 percius ncemhub 10957890448 Oct  2  2023 /global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan0000001544.h5\n"
     ]
    }
   ],
   "source": [
    "!ls -l /global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan0000001544.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc787e0-cbeb-4cba-80b0-aafbf5f67d59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened \"counted_halo_scan0000000159.h5\" with sec2 driver.\n",
      "/                        Group\n",
      "    Location:  1:96\n",
      "    Links:     1\n",
      "/electron_events         Group\n",
      "    Location:  1:800\n",
      "    Links:     1\n",
      "/electron_events/scan_positions Dataset {1049600/1049600}\n",
      "    Attribute: Nx scalar\n",
      "        Type:      native long\n",
      "        Data:  1025\n",
      "    Attribute: Ny scalar\n",
      "        Type:      native long\n",
      "        Data:  1024\n",
      "    Location:  1:3403823760\n",
      "    Links:     1\n",
      "    Storage:   8396800 logical bytes, 8396800 allocated bytes, 100.00% utilization\n",
      "    Type:      native long\n",
      "/electron_events/weights Dataset {1049600/1049600}\n",
      "    Location:  1:2432\n",
      "    Links:     1\n",
      "    Storage:   information not available\n",
      "    Type:      variable length of\n",
      "                   native int\n",
      "/metadata                Group\n",
      "    Location:  1:3403824032\n",
      "    Links:     1\n",
      "/metadata/electron_counting Group\n",
      "    Attribute: background_threshold scalar\n",
      "        Type:      native double\n",
      "        Data:  28.5471\n",
      "    Attribute: background_threshold_n_sigma scalar\n",
      "        Type:      native double\n",
      "        Data:  4.5\n",
      "    Attribute: max_sample scalar\n",
      "        Type:      native double\n",
      "        Data:  785\n",
      "    Attribute: mean scalar\n",
      "        Type:      native double\n",
      "        Data:  20.5534\n",
      "    Attribute: min_sample scalar\n",
      "        Type:      native double\n",
      "        Data:  10\n",
      "    Attribute: number_of_bins scalar\n",
      "        Type:      native long\n",
      "        Data:  775\n",
      "    Attribute: number_of_samples scalar\n",
      "        Type:      native long\n",
      "        Data:  1200\n",
      "    Attribute: optimized_mean scalar\n",
      "        Type:      native double\n",
      "        Data:  20.4929\n",
      "    Attribute: optimized_std_dev scalar\n",
      "        Type:      native double\n",
      "        Data:  1.7898\n",
      "    Attribute: std_dev scalar\n",
      "        Type:      native double\n",
      "        Data:  3.74098\n",
      "    Attribute: threshold_calculated scalar\n",
      "        Type:      enum native signed char {\n",
      "                   FALSE            = 0\n",
      "                   TRUE             = 1\n",
      "               }\n",
      "        Data:  TRUE\n",
      "    Attribute: variance scalar\n",
      "        Type:      native double\n",
      "        Data:  13.9949\n",
      "    Attribute: x_ray_threshold scalar\n",
      "        Type:      native double\n",
      "        Data:  675.225\n",
      "    Attribute: x_ray_threshold_n_sigma scalar\n",
      "        Type:      native double\n",
      "        Data:  175\n",
      "    Location:  1:3403824736\n",
      "    Links:     1\n"
     ]
    }
   ],
   "source": [
    "!h5ls -lvr counted_halo_scan0000000159.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d392852-6115-47b4-b693-0452b1ef744d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba9db2-4827-440a-aad7-c911d4d6bbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pylandau\n",
    "#!pip install DuSC_explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1efab2-7d96-4dbc-a928-10b2971cb5cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Does not work in Jupyter (must run from terminal)\n",
    "#import DuSC_explorer\n",
    "#DuSC_explorer.open_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431cb4c",
   "metadata": {},
   "source": [
    "## Fit-based counting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097d7adf-0360-4186-9ee1-b3d4a0459147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nframes = 1049600 # 2064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006a2750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gaussian_splash_pytorch(A, sigma, size=3, device='cpu'):\n",
    "    \"\"\"Prepare the Gaussian splash as a PyTorch convolution kernel.\"\"\"\n",
    "    x = y = torch.arange(0, size, device=device) - (size // 2)\n",
    "    X, Y = torch.meshgrid(x, y)\n",
    "    gaussian = A * torch.exp(- (X**2 + Y**2) / (2 * sigma**2))\n",
    "    # Reshape to 4D tensor: (out_channels, in_channels, height, width)\n",
    "    gaussian_kernel = gaussian.unsqueeze(0).unsqueeze(0)\n",
    "    return gaussian_kernel\n",
    "\n",
    "def construct_modeled_frame_pytorch(frame_ct, splash_kernel, noise_sigma, device='cpu'):\n",
    "    \"\"\"Construct the modeled frame using convolution to apply the Gaussian splash.\"\"\"\n",
    "    # Ensure frame_ct is a float tensor and add batch and channel dimensions\n",
    "    frame_ct_tensor = frame_ct.float().unsqueeze(1)\n",
    "    \n",
    "    # Apply the Gaussian splash across the entire frame using convolution\n",
    "    modeled_frame = F.conv2d(frame_ct_tensor, splash_kernel, padding=splash_kernel.shape[-1]//2)\n",
    "    \n",
    "    # Add noise\n",
    "    #modeled_frame = modeled_frame + torch.normal(0, noise_sigma, size=frame.shape, device=device)\n",
    "    \n",
    "    # Remove batch and channel dimensions from the output\n",
    "    modeled_frame = modeled_frame.squeeze(1)\n",
    "    \n",
    "    return modeled_frame\n",
    "\n",
    "def count_frame_pytorch(frame_bls, frame_ct, n_steps_max=500, loss_lim = 1):\n",
    "    \"\"\"Counts a frame given initial guess frame_ct\"\"\"\n",
    "    \n",
    "    # Convert frame and frame_ct to a PyTorch tensor\n",
    "    frame_tensor = torch.from_numpy(frame_bls).to(device)\n",
    "    frame_ct_tensor = torch.tensor(frame_ct, dtype=torch.float32, device=device, requires_grad=True)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam([frame_ct_tensor], lr=0.01)\n",
    "\n",
    "    # Define the single-electron Gaussian splash\n",
    "    splash = gaussian_splash_pytorch(A,sigma,device=device)\n",
    "\n",
    "    # Set up the parameters for the iteration\n",
    "    n_steps = 0\n",
    "    loss = loss_lim + 1\n",
    "    \n",
    "    while(loss > loss_lim and n_steps < n_steps_max):\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        # Construct the modeled frame\n",
    "        modeled_frame = construct_modeled_frame_pytorch(frame_ct_tensor, splash, noise_sigma, device=device)\n",
    "\n",
    "        # Compute the loss (negative likelihood)\n",
    "        loss = torch.sum((frame_tensor - modeled_frame) ** 2)\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update frame_ct_tensor based on gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        n_steps += 1\n",
    "        if(n_steps >= n_steps_max):\n",
    "            print(f\"Warning, stopping at max steps {n_steps} with loss {loss}\")\n",
    "        if n_steps % 100 == 0:\n",
    "            print(f\"Step {n_steps}, Loss: {loss.item()}\")\n",
    "    print(f\"Counted in n_steps = {n_steps} with loss = {loss}\")\n",
    "    \n",
    "    return frame_ct_tensor.cpu().detach().numpy(), modeled_frame.cpu().detach().numpy()\n",
    "\n",
    "def frame_to_indices_weights(counted_frames):\n",
    "    \"\"\"Convert a batch of 2D counted frames into lists of linear indices and weights.\"\"\"\n",
    "    batch_size = counted_frames.shape[0]\n",
    "    frame_shape = counted_frames.shape[1:]\n",
    "    all_linear_indices = []\n",
    "    all_weights = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        frame = counted_frames[i]\n",
    "        nonzero_indices = np.nonzero(frame)\n",
    "        weights = frame[nonzero_indices]\n",
    "        linear_indices = np.ravel_multi_index(nonzero_indices, frame_shape)\n",
    "        \n",
    "        all_linear_indices.append(linear_indices)\n",
    "        all_weights.append(weights)\n",
    "\n",
    "    return all_linear_indices, all_weights\n",
    "\n",
    "def update_counted_data_hdf5(file_path, batch_start_idx, frames_indices, frames_weights, group_name='electron_events'):\n",
    "    # Ensure frames_indices and frames_weights are lists of arrays, \n",
    "    # with each array corresponding to one frame's data.\n",
    "    \n",
    "    with h5py.File(file_path, 'a') as f:  # Open file in append mode\n",
    "        # Create or access the group\n",
    "        if group_name not in f:\n",
    "            grp = f.create_group(group_name)\n",
    "        else:\n",
    "            grp = f[group_name]\n",
    "\n",
    "        # Check if the VL datasets exist, create them if not\n",
    "        if 'frames' not in grp:\n",
    "            vl_dtype_indices = h5py.special_dtype(vlen=np.dtype('uint32'))\n",
    "            vl_dataset_indices = grp.create_dataset(\"frames\", (nframes,), dtype=vl_dtype_indices)\n",
    "        else:\n",
    "            vl_dataset_indices = grp['frames']\n",
    "\n",
    "        if 'weights' not in grp:\n",
    "            vl_dtype_weights = h5py.special_dtype(vlen=np.dtype('uint32'))\n",
    "            vl_dataset_weights = grp.create_dataset(\"weights\", (nframes,), dtype=vl_dtype_weights)\n",
    "        else:\n",
    "            vl_dataset_weights = grp['weights']\n",
    "\n",
    "        # Assuming the length of frames_indices matches the expected number of frames,\n",
    "        # iterate through each and update the datasets\n",
    "        for i, (indices, weights) in enumerate(zip(frames_indices, frames_weights)):\n",
    "            vl_dataset_indices[batch_start_idx+i] = indices\n",
    "            vl_dataset_weights[batch_start_idx+i] = weights\n",
    "            \n",
    "def compute_conditional_probabilities(lam_grid):\n",
    "    \"\"\"\n",
    "    Compute the array of conditional probabilities P(n >= 2) / P(n >= 1) for a grid of lambda values.\n",
    "    \n",
    "    :param lam_grid: A 576x576 numpy array of lambda values.\n",
    "    :return: A 576x576 numpy array of conditional probabilities.\n",
    "    \"\"\"\n",
    "    # Compute P(n >= 1) and P(n >= 2) for each lambda in the grid\n",
    "    p_at_least_1 = 1 - poisson.cdf(0, lam_grid)  # P(n >= 1) = 1 - P(n < 1)\n",
    "    p_at_least_2 = 1 - poisson.cdf(1, lam_grid)  # P(n >= 2) = 1 - P(n <= 1)\n",
    "    \n",
    "    # Conditional probability P(n >= 2) / P(n >= 1)\n",
    "    # Safeguard division by zero by using np.where to only compute valid divisions\n",
    "    conditional_prob = np.where(p_at_least_1 > 0, p_at_least_2 / p_at_least_1, 0)\n",
    "    \n",
    "    return conditional_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2d4bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial variables\n",
    "dataset_path = \"/pscratch/sd/j/jrenner1/data_scan0000000159.h5\" # \"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan0000001544.h5\"\n",
    "counted_data_path = \"counted_halo_scan0000000159.h5\" #\"counted_phase2_scan0000001544.h5\"\n",
    "A = 19.032279882831432\n",
    "sigma = 0.5065158792172835\n",
    "noise_sigma = 1.0\n",
    "device='cuda'\n",
    "frame_width = 576\n",
    "nbatch = 2064\n",
    "baseline = 20.0\n",
    "th_single_elec = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b309a9-3723-424c-a62d-d4ce21dbfe8e",
   "metadata": {},
   "source": [
    "##### Prior determination (skip this to operate without prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a205c-fe7a-4ddd-b62d-0f6ef7027d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the \"prior\"\n",
    "with h5py.File(dataset_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    \n",
    "    # Get all frames and subtract the baseline.\n",
    "    prior_bls = np.array(data[0:nframes,:,:],dtype=np.float32) - baseline\n",
    "    print(f\"Prior values, {nframes} frames, shape: {prior_bls.shape}\")\n",
    "    \n",
    "# Compute the summed frame.\n",
    "prior_frame = np.sum(prior_bls,axis=0)\n",
    "print(\"Summed frame dimensions:\",prior_frame.shape)\n",
    "\n",
    "# Divide by the average electron amplitude.\n",
    "prior_frame /= A\n",
    "\n",
    "# Normalize by the number of frames to get the final \"prior\".\n",
    "prior_frame /= nframes\n",
    "\n",
    "# Eliminate negative values.\n",
    "prior_frame[prior_frame < 0] = 0.\n",
    "\n",
    "# Compute the conditional probabilities of having >= 2 counts, given >= 1 count.\n",
    "conditional_prob = compute_conditional_probabilities(prior_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5615ff1-a1e4-449d-9ec0-4ff0222d092a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the prior\n",
    "plt_frame = prior_frame\n",
    "\n",
    "iprior = plt.imshow(plt_frame, vmin=plt_frame.min(), vmax=plt_frame.max(), cmap='gray')\n",
    "plt.colorbar()\n",
    "#axs[i].set_title(f'Scan No: {scan_no}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde9215-7ede-43d1-89fc-c165a60b91e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Repeat the conditional probability matrix nbatch times.\n",
    "conditional_prob_batch = np.repeat(conditional_prob[np.newaxis,:,:], nbatch, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9baf13b-05f1-4a94-ba9a-1332f5fda704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_matrix = np.random.rand(nbatch,frame_width,frame_width)\n",
    "single_electron_pixels = random_matrix > conditional_prob_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d65f1-9514-40b5-b1f5-6cd35eca792d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_single_elec = np.sum(single_electron_pixels)\n",
    "n_total = nbatch*frame_width*frame_width\n",
    "print(f\"TEST: {n_single_elec} of {n_total} ({n_single_elec/n_total*100:.5f}%) were single-electrons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1e03f-4678-4a4b-9068-1b13c29d55dc",
   "metadata": {},
   "source": [
    "##### Perform the counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5375ea01-a735-4e24-9b49-85836942debc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing in 509 batches...\n",
      "\n",
      "\n",
      " ** BATCH 494 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/nersc/pm-2022q4/sw/pytorch/2.0.1/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pscratch/sd/s/swowner/pytorch-build/pytorch/2.0.1/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 1387016960.0\n",
      "Step 200, Loss: 1034559872.0\n",
      "Warning, stopping at max steps 300 with loss 787358400.0\n",
      "Step 300, Loss: 787358400.0\n",
      "Counted in n_steps = 300 with loss = 787358400.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 495 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1379265792.0\n",
      "Step 200, Loss: 1028332800.0\n",
      "Warning, stopping at max steps 300 with loss 782325376.0\n",
      "Step 300, Loss: 782325376.0\n",
      "Counted in n_steps = 300 with loss = 782325376.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 496 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1382154496.0\n",
      "Step 200, Loss: 1030253056.0\n",
      "Warning, stopping at max steps 300 with loss 783710208.0\n",
      "Step 300, Loss: 783710208.0\n",
      "Counted in n_steps = 300 with loss = 783710208.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 497 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1380526848.0\n",
      "Step 200, Loss: 1028940544.0\n",
      "Warning, stopping at max steps 300 with loss 782513920.0\n",
      "Step 300, Loss: 782513920.0\n",
      "Counted in n_steps = 300 with loss = 782513920.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 498 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1378980864.0\n",
      "Step 200, Loss: 1028065408.0\n",
      "Warning, stopping at max steps 300 with loss 782164608.0\n",
      "Step 300, Loss: 782164608.0\n",
      "Counted in n_steps = 300 with loss = 782164608.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 499 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1379271808.0\n",
      "Step 200, Loss: 1028540608.0\n",
      "Warning, stopping at max steps 300 with loss 782803840.0\n",
      "Step 300, Loss: 782803840.0\n",
      "Counted in n_steps = 300 with loss = 782803840.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 500 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1384891648.0\n",
      "Step 200, Loss: 1033171712.0\n",
      "Warning, stopping at max steps 300 with loss 786776320.0\n",
      "Step 300, Loss: 786776320.0\n",
      "Counted in n_steps = 300 with loss = 786776320.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 501 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1389653504.0\n",
      "Step 200, Loss: 1036331712.0\n",
      "Warning, stopping at max steps 300 with loss 789204928.0\n",
      "Step 300, Loss: 789204928.0\n",
      "Counted in n_steps = 300 with loss = 789204928.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 502 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1387490304.0\n",
      "Step 200, Loss: 1034302336.0\n",
      "Warning, stopping at max steps 300 with loss 787446784.0\n",
      "Step 300, Loss: 787446784.0\n",
      "Counted in n_steps = 300 with loss = 787446784.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 503 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1391160320.0\n",
      "Step 200, Loss: 1030623744.0\n",
      "Warning, stopping at max steps 300 with loss 784613440.0\n",
      "Step 300, Loss: 784613440.0\n",
      "Counted in n_steps = 300 with loss = 784613440.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 504 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1400514432.0\n",
      "Step 200, Loss: 1044027840.0\n",
      "Warning, stopping at max steps 300 with loss 794934656.0\n",
      "Step 300, Loss: 794934656.0\n",
      "Counted in n_steps = 300 with loss = 794934656.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 505 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1393080576.0\n",
      "Step 200, Loss: 1036924096.0\n",
      "Warning, stopping at max steps 300 with loss 788504704.0\n",
      "Step 300, Loss: 788504704.0\n",
      "Counted in n_steps = 300 with loss = 788504704.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 506 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1379668224.0\n",
      "Step 200, Loss: 1026032896.0\n",
      "Warning, stopping at max steps 300 with loss 779483712.0\n",
      "Step 300, Loss: 779483712.0\n",
      "Counted in n_steps = 300 with loss = 779483712.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 507 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 1371577600.0\n",
      "Step 200, Loss: 1017840256.0\n",
      "Warning, stopping at max steps 300 with loss 772032960.0\n",
      "Step 300, Loss: 772032960.0\n",
      "Counted in n_steps = 300 with loss = 772032960.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 2064 and weights = 2064\n",
      "\n",
      "\n",
      " ** BATCH 508 **\n",
      "-- Processing frames...\n",
      "-- Counting frames...\n",
      "Step 100, Loss: 722610944.0\n",
      "Step 200, Loss: 536295520.0\n",
      "Warning, stopping at max steps 300 with loss 406854720.0\n",
      "Step 300, Loss: 406854720.0\n",
      "Counted in n_steps = 300 with loss = 406854720.0\n",
      "-- Saving frames...\n",
      "Frame indices len = 1088 and weights = 1088\n"
     ]
    }
   ],
   "source": [
    "batches = round(nframes / nbatch)\n",
    "print(f\"Analyzing in {batches} batches...\")\n",
    "for batch in range(batches)[494:]:\n",
    "    print(f\"\\n\\n ** BATCH {batch} **\")\n",
    "    \n",
    "    # Get the frames for this batch\n",
    "    print(\"-- Processing frames...\")\n",
    "    with h5py.File(dataset_path, 'r') as f0:\n",
    "        data = f0['frames']\n",
    "\n",
    "        # Get the frames\n",
    "        frame_bls = np.array(data[batch*nbatch:batch*nbatch+nbatch,:,:],dtype=np.float32) - baseline\n",
    "\n",
    "        # Compute an initial counted frame\n",
    "        frame_ct = np.rint(frame_bls / A, out=np.zeros(frame_bls.shape,dtype=np.int16), casting='unsafe')\n",
    "        frame_ct[frame_ct < th_single_elec] = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Count the frames.\n",
    "    print(\"-- Counting frames...\")\n",
    "    frame_ct_reco, modeled_frame_reco = count_frame_pytorch(frame_bls, frame_ct, n_steps_max = 300, loss_lim = nframes)\n",
    "    frame_ct_reco[frame_ct_reco < 0] = 0\n",
    "    frame_ct_reco = np.rint(frame_ct_reco)\n",
    "    # -------------------------------------------------------------------------------\n",
    "    \n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Apply the prior for each frame.\n",
    "    # Generate a 576x576 matrix of random numbers from a uniform distribution [0, 1)\n",
    "#     random_matrix = np.random.rand(*frame_ct_reco.shape)\n",
    "\n",
    "#     # Compare the random matrix to the probability matrix:\n",
    "#     #  - if the random number is greater than the conditional probability, \n",
    "#     #    the multi-count is said to have been due to the Landau\n",
    "#     #    tail, and therefore the count is set to 1\n",
    "#     single_electron_pixels = random_matrix > conditional_prob_batch\n",
    "\n",
    "#     # Set all counts > 1 that did not pass the probability game to 1 count.\n",
    "#     forced_single_electrons = (frame_ct_reco > 1) & single_electron_pixels\n",
    "#     n_single_elec = np.sum(forced_single_electrons)\n",
    "#     n_total = nbatch*frame_width*frame_width\n",
    "#     print(f\"{n_single_elec} of {n_total} ({n_single_elec/n_total*100:.4f}%) forced to single-electron counts\")\n",
    "#     frame_ct_reco[forced_single_electrons] = 1\n",
    "    # -------------------------------------------------------------------------------\n",
    "    \n",
    "    # Save the counted frames in the array\n",
    "    print(\"-- Saving frames...\")\n",
    "    frames_indices, frames_weights = frame_to_indices_weights(frame_ct_reco)\n",
    "    print(f\"Frame indices len = {len(frames_indices)} and weights = {len(frames_weights)}\")\n",
    "    update_counted_data_hdf5(counted_data_path, batch*nbatch, frames_indices, frames_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dfb8ee6-a725-4ca2-aa95-284d4c3c578d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying attributes for electron_events/frames\n",
      "Copying all data for electron_events/scan_positions\n",
      "Data and attributes have been copied successfully, with specific handling for /electron_events/frames attributes.\n"
     ]
    }
   ],
   "source": [
    "# Paths to the source and destination HDF5 files\n",
    "source_file_path = '/global/cfs/cdirs/ncemhub/distiller/counted/2024.05.09/FOURD_240509_1311_16228_00159.h5'\n",
    "destination_file_path = 'counted_halo_scan0000000159.h5'\n",
    "\n",
    "def copy_item(name, obj, dest_file):\n",
    "    \"\"\"Recursive function to copy items and attributes from source to destination, excluding dataset data for specific paths.\"\"\"\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        if name == 'electron_events/frames':\n",
    "            # Copy only attributes for the frames dataset\n",
    "            if name not in dest_file:\n",
    "                # If dataset doesn't exist, create an empty one just to hold attributes\n",
    "                print(f\"Copying dataset {name}\")\n",
    "                dest_file.create_dataset(name, shape=(0,), dtype=obj.dtype)\n",
    "            # Copy attributes\n",
    "            print(f\"Copying attributes for {name}\")\n",
    "            for attr_name, attr_value in obj.attrs.items():\n",
    "                dest_file[name].attrs[attr_name] = attr_value\n",
    "        elif name == 'electron_events/scan_positions':\n",
    "            # Overwrite or recreate scan_positions dataset with all frames\n",
    "            if name in dest_file:\n",
    "                del dest_file[name]  # Delete the existing dataset to recreate it\n",
    "            print(f\"Copying all data for {name}\")\n",
    "            dest_dataset = dest_file.create_dataset(name, data=obj[:], dtype=obj.dtype)\n",
    "            for attr_name, attr_value in obj.attrs.items():\n",
    "                dest_dataset.attrs[attr_name] = attr_value\n",
    "        else:\n",
    "            # Handle other datasets\n",
    "            if name not in dest_file or name == 'electron_events/scan_positions':\n",
    "                # Calculate the size to copy: minimum of dataset size or nframes\n",
    "                size_to_copy = min(obj.size, nframes)\n",
    "                # Copy the dataset with the limited size\n",
    "                dest_dataset = dest_file.create_dataset(name, data=obj[:size_to_copy], dtype=obj.dtype)\n",
    "                # Copy attributes for the dataset\n",
    "                for attr_name, attr_value in obj.attrs.items():\n",
    "                    dest_dataset.attrs[attr_name] = attr_value\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        # Create group if it doesn't exist\n",
    "        if name not in dest_file:\n",
    "            dest_file.create_group(name)\n",
    "        # Copy attributes for the group\n",
    "        for attr_name, attr_value in obj.attrs.items():\n",
    "            dest_file[name].attrs[attr_name] = attr_value\n",
    "\n",
    "# Open both the source and destination file\n",
    "with h5py.File(source_file_path, 'r') as source, h5py.File(destination_file_path, 'a') as dest:\n",
    "    # Walk through the source file and copy everything, applying specific rules to frames dataset\n",
    "    source.visititems(lambda name, obj: copy_item(name, obj, dest))\n",
    "\n",
    "print(\"Data and attributes have been copied successfully, with specific handling for /electron_events/frames attributes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320d8e22-68cb-4d0f-98d8-519dd7506ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dtype: object\n",
      "All data converted and written back successfully.\n"
     ]
    }
   ],
   "source": [
    "# Replace int32->int64\n",
    "destination_file_path = 'counted_halo_scan0000000159.h5'\n",
    "\n",
    "# Open the HDF5 file in read/write mode\n",
    "with h5py.File(destination_file_path, 'r+') as file:\n",
    "    # Access the dataset\n",
    "    dataset = file['/electron_events/frames']\n",
    "\n",
    "    # Check the type and structure\n",
    "    print(\"Data dtype:\", dataset.dtype)\n",
    "    #print(\"Data is a variable-length dataset:\", isinstance(dataset.dtype, h5py.vlen_dtype))\n",
    "\n",
    "    # Prepare for the new dataset creation by copying attributes and deleting the old dataset\n",
    "    old_attrs = dict(dataset.attrs)\n",
    "    del file['/electron_events/frames']\n",
    "    file.flush()  # Ensure the dataset is deleted before recreating\n",
    "\n",
    "    # Create a new dataset with variable-length uint32 arrays\n",
    "    # Note: Properly configure dtype for variable-length if necessary\n",
    "    dtype_uint32_vlen = h5py.special_dtype(vlen=np.dtype('uint32'))\n",
    "    frames_uint32 = file['/electron_events'].create_dataset('frames', shape=dataset.shape, dtype=dtype_uint32_vlen)\n",
    "\n",
    "    # Iterate over each array, convert and write back\n",
    "    for i in range(len(dataset)):\n",
    "        # Convert each array individually\n",
    "        array_uint32 = np.array(dataset[i], dtype=np.uint32)\n",
    "        frames_uint32[i] = array_uint32\n",
    "\n",
    "    # Reapply the saved attributes to the new dataset\n",
    "    for attr_name, attr_value in old_attrs.items():\n",
    "        frames_uint32.attrs[attr_name] = attr_value\n",
    "\n",
    "    print(\"All data converted and written back successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59949f6a-5262-4819-9b97-b9077e14e308",
   "metadata": {},
   "source": [
    "### Counting analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f176fd-d06d-4666-bd5c-b1f18eb97044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert linear indices to 2D (row, col) indices\n",
    "def linear_to_2d_indices(linear_indices, cols):\n",
    "    rows = linear_indices // cols\n",
    "    cols = linear_indices % cols\n",
    "    return rows, cols\n",
    "\n",
    "# Function to create a 2D image from the counted frame data\n",
    "def create_counted_image_weighted(frame_data, weight_data, frame_height, frame_width):\n",
    "    # Initialize a 2D array with zeros\n",
    "    image = np.zeros((frame_height, frame_width), dtype=np.int32)\n",
    "\n",
    "    # Convert linear indices to 2D indices\n",
    "    rows, cols = linear_to_2d_indices(frame_data, frame_width)\n",
    "\n",
    "    # Set the hit pixels\n",
    "    for r,c,w in zip(rows,cols,weight_data):\n",
    "        image[r, c] = w\n",
    "\n",
    "    return image\n",
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None:  # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask\n",
    "\n",
    "def calculate_center_of_mass(counted_image):\n",
    "    \"\"\"Calculate the center of mass of the counted image.\"\"\"\n",
    "    total_mass = np.sum(counted_image)\n",
    "    indices = np.indices(counted_image.shape)\n",
    "    mean_x = np.sum(indices[1] * counted_image) / total_mass\n",
    "    mean_y = np.sum(indices[0] * counted_image) / total_mass\n",
    "    sigma_x = np.sqrt(np.sum((indices[1] - mean_x)**2 * counted_image) / total_mass)\n",
    "    sigma_y = np.sqrt(np.sum((indices[0] - mean_y)**2 * counted_image) / total_mass)\n",
    "    return mean_x, mean_y, sigma_x, sigma_y\n",
    "\n",
    "def cartesian_to_polar_counts(image, center=None):\n",
    "    if center is None:\n",
    "        center = (image.shape[1] // 2, image.shape[0] // 2)  # Use the middle of the image\n",
    "    \n",
    "    y, x = np.indices(image.shape)\n",
    "    x = x - center[0]\n",
    "    y = y - center[1]\n",
    "    \n",
    "    # Convert Cartesian coordinates to polar\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    theta = np.arctan2(y, x)\n",
    "    \n",
    "    # Convert theta from radians to degrees\n",
    "    theta = np.degrees(theta)\n",
    "    theta[theta < 0] += 360  # Convert negative angles to positive angles\n",
    "    \n",
    "    return r, theta, image\n",
    "\n",
    "def plot_counts_vs_angle_binned(image, target_radius, center=None, delta_radius=5, num_bins=360):\n",
    "    r, theta, counts = cartesian_to_polar_counts(image, center)\n",
    "    \n",
    "    # Find indices where radius is close to target radius\n",
    "    mask = (r >= target_radius - delta_radius) & (r <= target_radius + delta_radius)\n",
    "    actual_radius = np.mean(r[mask])\n",
    "\n",
    "    # Bin the theta values\n",
    "    bins = np.linspace(0, 360, num_bins + 1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    digitized = np.digitize(theta[mask], bins) - 1  # Indices of the bins to which each value belongs\n",
    "\n",
    "    # Aggregate counts in each bin\n",
    "    binned_counts = np.zeros(num_bins)\n",
    "    np.add.at(binned_counts, digitized, counts[mask])  # Sum counts falling into each bin\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(bin_centers, binned_counts, marker='o', linestyle='-', color='b')\n",
    "    plt.title(f'Count Distribution Near Radius ~{actual_radius:.2f}')\n",
    "    plt.xlabel('Angle (degrees)')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bbd41-5c79-4ec7-87e0-60ed0a3fcaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path_0 = \"counted_halo_scan0000000159_10000.h5\"\n",
    "file_path_1 = \"counted_halo_scan0000000159_300.h5\"\n",
    "\n",
    "# Set up the figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(8, 2))\n",
    "\n",
    "iframe = 0\n",
    "with_weights = False\n",
    "\n",
    "\n",
    "with h5py.File(file_path_0, 'r') as f0:\n",
    "\n",
    "    frame_data_0 = np.array(f0['electron_events']['frames'][iframe])\n",
    "    if(with_weights):\n",
    "        weight_data_0 = np.array(f0['electron_events']['weights'][iframe])\n",
    "    else:\n",
    "        weight_data_0 = np.ones(len(f0['electron_events']['frames'][iframe]))\n",
    "\n",
    "    # Create a 2D image of the counted frame\n",
    "    counted_image_0 = create_counted_image_weighted(frame_data_0, weight_data_0, frame_width, frame_width)\n",
    "\n",
    "    # Plot the summed, counted image\n",
    "    ax0 = axs[0].imshow(counted_image_0, vmin=counted_image_0.min(), vmax=counted_image_0.max(), cmap='gray')\n",
    "    fig.colorbar(ax0, ax=axs[0])\n",
    "    \n",
    "with h5py.File(file_path_1, 'r') as f1:\n",
    "\n",
    "    frame_data_1 = np.array(f1['electron_events']['frames'][iframe])\n",
    "    if(with_weights):\n",
    "        weight_data_1 = np.array(f1['electron_events']['weights'][iframe])\n",
    "    else:\n",
    "        weight_data_1 = np.ones(len(f1['electron_events']['frames'][iframe]))\n",
    "\n",
    "    # Create a 2D image of the counted frame\n",
    "    counted_image_1 = create_counted_image_weighted(frame_data_1, weight_data_1, frame_width, frame_width)\n",
    "\n",
    "    # Plot the summed, counted image\n",
    "    ax1 = axs[1].imshow(counted_image_1, vmin=counted_image_1.min(), vmax=counted_image_1.max(), cmap='gray')\n",
    "    fig.colorbar(ax1, ax=axs[1])\n",
    "    \n",
    "counted_image_diff = counted_image_1 - counted_image_0\n",
    "ax2 = axs[2].imshow(counted_image_diff, vmin=counted_image_diff.min(), vmax=counted_image_diff.max(), cmap='gray')\n",
    "fig.colorbar(ax2, ax=axs[2])\n",
    "n0 = len(counted_image_0[np.nonzero(counted_image_0)])\n",
    "ndiff = len(counted_image_diff[np.nonzero(counted_image_diff)])\n",
    "print(f\"{ndiff}/{n0} = {ndiff/n0*100:.2f}% different counts\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a70cbe2-cdb2-4467-ae1a-2bf2d3c6a2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a706036-d861-4597-9bf7-9b7d69dfc4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nframes = 2064\n",
    "frame_width = 576\n",
    "with_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb415233-5d57-4715-8b51-e0b35cc0d22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f\"counted_halo_scan0000000159.h5\"\n",
    "#file_path = f\"/global/cfs/cdirs/ncemhub/distiller/counted/2023.09.18/data_scan{scan_no}_id{idbase[igroup]+i}_electrons.h5\"\n",
    "\n",
    "# Set up the figure\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "counted_electrons, counted_nelec = [], []\n",
    "with h5py.File(file_path, 'r') as f0:\n",
    "\n",
    "    summed_counted_image = np.zeros((frame_width, frame_width), dtype=np.int32)\n",
    "\n",
    "    # Create lists for CM computations.\n",
    "    mean_x_list, mean_y_list, sigma_x_list, sigma_y_list = [], [], [], []\n",
    "\n",
    "    nelec = 0\n",
    "    for iframe in range(nframes):\n",
    "        #if(iframe % int(nframes/10) == 0):\n",
    "        #    print(f\"Processing frame {iframe}/{nframes}\")\n",
    "        frame_data = np.array(f0['electron_events']['frames'][iframe])\n",
    "        if(with_weights):\n",
    "            weight_data = np.array(f0['electron_events']['weights'][iframe])\n",
    "        else:\n",
    "            weight_data = np.ones(len(f0['electron_events']['frames'][iframe]))\n",
    "\n",
    "        # Create a 2D image of the counted frame\n",
    "        counted_image = create_counted_image_weighted(frame_data, weight_data, frame_width, frame_width)\n",
    "\n",
    "        # Sum the counted images\n",
    "        summed_counted_image += counted_image\n",
    "        nelec += len(frame_data)\n",
    "\n",
    "    # Sum all the counts.\n",
    "    total_counts = np.sum(summed_counted_image)\n",
    "    print(f\"-- Total counts: {total_counts}\")\n",
    "    print(f\"-- Total nelec:  {nelec}\")\n",
    "    counted_electrons.append(total_counts)\n",
    "    counted_nelec.append(nelec)\n",
    "\n",
    "    # Plot the summed, counted image\n",
    "    imax = axs.imshow(summed_counted_image[250:280,265:295], vmin=summed_counted_image.min(), vmax=summed_counted_image.max(), cmap='gray')\n",
    "    #imax = axs.imshow(summed_counted_image, vmin=summed_counted_image.min(), vmax=summed_counted_image.max(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e863953-e5c9-4a5e-ba55-e8d522f0ac8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = f\"/global/cfs/cdirs/ncemhub/distiller/counted/2023.09.18/data_scan1531_id12284_electrons.h5\"\n",
    "with h5py.File(file_path, 'r') as f0:\n",
    "    frame_data = np.array(f0['electron_events']['frames'][2])\n",
    "    print(f0['electron_events']['frames'])\n",
    "    print(frame_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc65d56-f422-4b3f-ae2f-c531795a5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"counted_scan0000001532.h5\", 'r') as f0:\n",
    "    frame_data = np.array(f0['electron_events']['frames'][16505])\n",
    "    print(f0['electron_events']['frames'])\n",
    "    print(frame_data)\n",
    "    print(len(frame_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1eef0d",
   "metadata": {},
   "source": [
    "Create the initial prediction for the counted frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf1350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nreco = 0\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    \n",
    "    # Get the frame\n",
    "    frame_bls = np.array(data[nreco:nreco+nframes,:,:],dtype=np.float32) - baseline\n",
    "    \n",
    "    # Subtract the baseline\n",
    "    #frame_bls = (frame - baseline)\n",
    "    \n",
    "    # Get a \"prior\" counted frame\n",
    "    #frame_ct = (frame - baseline)\n",
    "    frame_ct = np.rint(frame_bls / A, out=np.zeros(frame_bls.shape,dtype=np.int16), casting='unsafe')\n",
    "    frame_ct[frame_ct < th_single_elec] = 0\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plotting on the ith subplot\n",
    "ax0 = axs[0].imshow(frame_bls[0], vmin=frame_bls[0].min(), vmax=frame_bls[0].max())\n",
    "fig.colorbar(ax0, ax=axs[0])\n",
    "ax1 = axs[1].imshow(frame_ct[0], vmin=frame_ct[0].min(), vmax=frame_ct[0].max())\n",
    "fig.colorbar(ax1, ax=axs[1])\n",
    "\n",
    "# Print nonzero prior values\n",
    "#print(\"Nonzero values in frame_ct:\")\n",
    "#print(frame_ct[np.nonzero(frame_ct)])\n",
    "#np.sum(frame_ct[np.nonzero(frame_ct)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3b511",
   "metadata": {},
   "source": [
    "Perform the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaa6ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_ct_reco, modeled_frame_reco = count_frame_pytorch(frame_bls, frame_ct, n_steps_max = 500, loss_lim = nframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710fc37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(modeled_frame_reco[0][100:150,100:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ct_reco[frame_ct_reco < 0] = 0\n",
    "frame_ct_reco = np.rint(frame_ct_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1106b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nframe = 0\n",
    "rng_start, rng_end = 150, 200\n",
    "\n",
    "# Set up the figure\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "frame_plt = frame_bls\n",
    "\n",
    "ax0 = axs[0].imshow(frame_plt[nframe,rng_start:rng_end,rng_start:rng_end], vmin=frame_plt[nframe,rng_start:rng_end,rng_start:rng_end].min(), vmax=frame_plt[nframe,rng_start:rng_end,rng_start:rng_end].max())\n",
    "fig.colorbar(ax0, ax=axs[0])\n",
    "axs[0].set_title(\"Frame (baseline subtracted)\")\n",
    "ax1 = axs[1].imshow(frame_ct_reco[nframe,rng_start:rng_end,rng_start:rng_end], vmin=frame_ct_reco[nframe,rng_start:rng_end,rng_start:rng_end].min(), vmax=frame_ct_reco[nframe,rng_start:rng_end,rng_start:rng_end].max())\n",
    "fig.colorbar(ax1, ax=axs[1])\n",
    "axs[1].set_title(\"Reconstructed counts\")\n",
    "ax2 = axs[2].imshow(frame_ct[nframe,rng_start:rng_end,rng_start:rng_end], vmin=frame_ct[nframe,rng_start:rng_end,rng_start:rng_end].min(), vmax=frame_ct[nframe,rng_start:rng_end,rng_start:rng_end].max())\n",
    "fig.colorbar(ax2, ax=axs[2])\n",
    "axs[2].set_title(\"Prior counts\")\n",
    "\n",
    "print(f\"Total counts reco = {np.sum(frame_ct_reco[nframe,rng_start:rng_end,rng_start:rng_end])}, prior = {np.sum(frame_ct[nframe,rng_start:rng_end,rng_start:rng_end])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a283e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(frame_ct_reco[np.nonzero(frame_ct_reco)])\n",
    "np.unique(frame_ct[np.nonzero(frame_ct)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad4af6-ad46-4bfc-a5d1-f25c4e47591a",
   "metadata": {},
   "source": [
    "# Get single-electron Gaussian profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef38f21-b460-4b09-8798-e3c971edf362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def randomized_scan_order(nrows, ncols):\n",
    "    indices = [(i, j) for i in range(1, nrows-1) for j in range(1, ncols-1)]\n",
    "    np.random.shuffle(indices)\n",
    "    return indices\n",
    "\n",
    "def extract_3x3_patches(original_frames, processed_frames):\n",
    "    \"\"\"\n",
    "    Extract 3x3 patches from original frames based on hits identified in processed frames.\n",
    "    \n",
    "    Args:\n",
    "    - original_frames: numpy array of original frames (non-baseline-subtracted, non-thresholded).\n",
    "    - processed_frames: numpy array of frames after baseline subtraction and thresholding.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array of 3x3 patches centered around identified hits.\n",
    "    \"\"\"\n",
    "    hit_patches = []\n",
    "    nrows, ncols = processed_frames.shape[1], processed_frames.shape[2]  # Frame dimensions\n",
    "\n",
    "    for frame_index in range(processed_frames.shape[0]):\n",
    "        processed_frame = processed_frames[frame_index]\n",
    "        original_frame = original_frames[frame_index]\n",
    "        \n",
    "        # Iterate through all pixels in the processed frame to find hits\n",
    "        indices = randomized_scan_order(nrows, ncols)\n",
    "        for i, j in indices:\n",
    "            if processed_frame[i, j] > 0:  # Center of a hit in processed frame\n",
    "                \n",
    "                # Extract the corresponding 3x3 patch from the original frame\n",
    "                patch = original_frame[i-1:i+2, j-1:j+2]\n",
    "                hit_patches.append(patch)\n",
    "\n",
    "    return np.array(hit_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02134c55-4a4d-4abd-ae1e-5c8cc1a66bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = \"/pscratch/sd/j/jrenner1\"\n",
    "#base_dir = \"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18\"\n",
    "scan_no = \"0159\" #1531\n",
    "nframes = 1000 #16512\n",
    "frame_width = 576\n",
    "baseline = 20.0\n",
    "th_single_elec = 10\n",
    "\n",
    "file_path = f\"{base_dir}/data_scan000000{scan_no}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e1bc2-230f-4f9c-9412-92835ddc7bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Histogram some frames to verify the baseline\n",
    "with h5py.File(file_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    print(\"Frames array shape:\",data.shape)\n",
    "    \n",
    "    data_hist = np.array(data[0:nframes,:,:],dtype=np.float32)\n",
    "    \n",
    "    plt.hist(data_hist.flatten(), bins=55, range=[0,150])\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"pixel value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f1ccf-e55a-4b66-85b8-1a5e48d5b583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Processing scan {scan_no}...\")\n",
    "with h5py.File(file_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    frames = data[0:nframes, :, :]\n",
    "    \n",
    "    # Subtract baseline only.\n",
    "    sub_frames = (frames - baseline)\n",
    "\n",
    "    # Subtract the baseline and threshold.\n",
    "    sub_frames_th = (frames - baseline)\n",
    "    sub_frames_th[sub_frames < th_single_elec] = 0\n",
    "\n",
    "    # Extract the 3x3 Gaussian pattern\n",
    "    hit_patches = extract_3x3_patches(sub_frames, sub_frames_th)\n",
    "    sum_patches = np.sum(hit_patches, axis=(1,2))\n",
    "    \n",
    "    avg_patch = np.mean(hit_patches, axis=0)\n",
    "\n",
    "    # Plot the averaged Gaussian\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    imax = ax.imshow(avg_patch, vmin=avg_patch.min(), vmax=avg_patch.max(), cmap='jet')\n",
    "    fig.colorbar(imax, ax=ax)\n",
    "    print(avg_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d930f-a650-4c5d-8bf3-00d7c5077a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the Landau distribution of patch summed charge\n",
    "sum_patches = np.sum(hit_patches, axis=(1,2))\n",
    "qmax = 300\n",
    "mpv = 19.8\n",
    "eta = 4.8\n",
    "A = 0.032 #0.0388\n",
    "x_landau = np.arange(0, qmax, 0.01)\n",
    "y_landau = pylandau.landau(x_landau, mpv, eta, A)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(sum_patches[sum_patches < qmax],bins=100,range=[0,qmax],density=True)\n",
    "plt.plot(x_landau,y_landau,color='red',linewidth=2,label=f\"L(mpv={mpv},$\\eta$={eta},A={A})\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('Patch charge',fontsize=14)\n",
    "plt.ylabel('Counts/bin',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8dfe5e-93e1-4501-95c0-b17e7ea7e200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The average 3x3 single-electron matrix\n",
    "matrix = np.array(avg_patch)\n",
    "\n",
    "# Gaussian function (assuming isotropic and centered at (1,1))\n",
    "def gaussian(x, y, A, sigma):\n",
    "    return A * np.exp(-((x-1)**2 + (y-1)**2) / (2*sigma**2))\n",
    "\n",
    "# Objective function to minimize (MSE)\n",
    "def objective(params):\n",
    "    A, sigma = params\n",
    "    predicted = np.array([[gaussian(x, y, A, sigma) for y in range(3)] for x in range(3)])\n",
    "    return np.mean((matrix - predicted)**2)\n",
    "\n",
    "# Initial guesses for A and sigma\n",
    "initial_guess = [35, 1.3]\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(objective, initial_guess, method='L-BFGS-B', bounds=[(0, None), (0, None)])\n",
    "\n",
    "# Optimized parameters\n",
    "A_opt, sigma_opt = result.x\n",
    "print(f\"Optimized A: {A_opt}, Optimized sigma: {sigma_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be956bef-4a80-4f00-a593-d2d62ec9e458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate the optimized Gaussian 3x3 matrix\n",
    "optimized_patch = np.array([[gaussian(x, y, A_opt, sigma_opt) for y in range(3)] for x in range(3)])\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original averaged patch\n",
    "ax0 = axs[0].imshow(avg_patch, cmap='viridis')\n",
    "fig.colorbar(ax0, ax=axs[0])\n",
    "axs[0].set_title('Original Avg. Patch')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Optimized Gaussian patch\n",
    "ax1 = axs[1].imshow(optimized_patch, cmap='viridis')\n",
    "fig.colorbar(ax1, ax=axs[1])\n",
    "axs[1].set_title('Optimized Gaussian Patch')\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Difference\n",
    "ax2 = axs[2].imshow(avg_patch - optimized_patch, cmap='viridis')\n",
    "fig.colorbar(ax2, ax=axs[2])\n",
    "axs[2].set_title('Difference (Original - Optimized)')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print optimized Gaussian\n",
    "print(optimized_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55cfea-8925-448f-a962-c1f4fc8bf0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at histogram of some frames\n",
    "nframes = 10\n",
    "file_path = f\"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan000000{scan_no}.h5\"\n",
    "with h5py.File(file_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    \n",
    "    data_hist = np.array(data[0:nframes,:,:],dtype=np.float32) - 20\n",
    "    \n",
    "    plt.hist(data_hist.flatten(), bins=100) #, range=[0,20])\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"baseline subtracted (-20) pixel value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496007e0-c81c-4193-b232-f581adcc41f6",
   "metadata": {},
   "source": [
    "# Attempt to count a single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c878da-0fc9-470a-ae14-b55260842eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = 31.3\n",
    "sigma = 1.3\n",
    "noise_sigma = 1.0\n",
    "\n",
    "def gaussian_5x5(A, sigma):\n",
    "    \"\"\"Generate a 5x5 Gaussian splash.\"\"\"\n",
    "    x = y = np.arange(0, 5) - 2  # Centered at 0, 0\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    gaussian = A * np.exp(- (X**2 + Y**2) / (2 * sigma**2))\n",
    "    return gaussian\n",
    "\n",
    "# Construct the 5x5 Gaussian\n",
    "gaus_elec = gaussian_5x5(A, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04d59a-362f-4bf1-b2b4-ee9efca30d8b",
   "metadata": {},
   "source": [
    "Obtain the frame to be counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46666752-43ed-43b8-afc7-00cfbf50d9b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nreco = 1500\n",
    "baseline = 20.0\n",
    "th_single_elec = 10\n",
    "\n",
    "#file_path = f\"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan000000{scan_no}.h5\"\n",
    "file_path = \"/pscratch/sd/j/jrenner1/data_scan0000000159.h5\"\n",
    "with h5py.File(file_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    \n",
    "    # Get the frame\n",
    "    frame = np.array(data[nreco,:,:],dtype=np.float32)\n",
    "    \n",
    "    # Get a \"prior\" counted frame\n",
    "    frame_ct = (frame - baseline)\n",
    "    frame_ct[frame_ct < th_single_elec] = 0\n",
    "    frame_ct = np.rint(frame_ct / A)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plotting on the ith subplot\n",
    "ax0 = axs[0].imshow(frame[260:280,270:290], vmin=frame.min(), vmax=frame.max())\n",
    "fig.colorbar(ax0, ax=axs[0])\n",
    "ax1 = axs[1].imshow(frame_ct[260:280,270:290], vmin=frame_ct.min(), vmax=frame_ct.max())\n",
    "fig.colorbar(ax1, ax=axs[1])\n",
    "\n",
    "# Print nonzero prior values\n",
    "print(\"Nonzero values in frame_ct:\")\n",
    "print(frame_ct[np.nonzero(frame_ct)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc594436-004b-403a-b6d9-adece757387a",
   "metadata": {},
   "source": [
    "Construct the modeled frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b244b97-5da8-439f-b48b-0e8e32c32e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_modeled_frame_from_counts(frame_ct, splash):\n",
    "    modeled_frame = np.zeros(frame_ct.shape)\n",
    "    nrows, ncols = frame_ct.shape\n",
    "    splash_size = splash.shape[0]  # Assuming splash is square\n",
    "\n",
    "    for x in range(nrows):\n",
    "        for y in range(ncols):\n",
    "            count = frame_ct[x, y]\n",
    "            if count > 0:\n",
    "                # Determine the indices for adding the splash\n",
    "                x_start = max(0, x - splash_size // 2)\n",
    "                x_end = min(nrows, x + splash_size // 2 + 1)\n",
    "                y_start = max(0, y - splash_size // 2)\n",
    "                y_end = min(ncols, y + splash_size // 2 + 1)\n",
    "                \n",
    "                # Adjust splash dimensions if near edges\n",
    "                splash_x_start = 0 - min(0, x - splash_size // 2)\n",
    "                splash_y_start = 0 - min(0, y - splash_size // 2)\n",
    "                splash_x_end = splash.shape[0] + min(0, nrows - (x + splash_size // 2 + 1))\n",
    "                splash_y_end = splash.shape[1] + min(0, ncols - (y + splash_size // 2 + 1))\n",
    "                #print(f\"x from {x_start} to {x_end}\")\n",
    "                #print(f\"splash from {splash_x_start} to {splash_x_end}\")\n",
    "\n",
    "                # Add the splash for each count\n",
    "                modeled_frame[x_start:x_end, y_start:y_end] += splash[splash_x_start:splash_x_end, splash_y_start:splash_y_end] * count\n",
    "\n",
    "    return modeled_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f5b1a-a172-4f02-bfef-8666d9468a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modeled_frame = construct_modeled_frame_from_counts(frame_ct, gaus_elec) + np.random.normal(0, noise_sigma, frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb195a64-5224-4989-a808-1ead547710d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(modeled_frame.flatten(),bins=100)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af2366-3e25-499e-9b47-193ca80b24c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8cb1b77-be0c-433c-b6c6-d3e6614b8f86",
   "metadata": {},
   "source": [
    "# Creation of averaged frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96418d-b000-4ce5-971a-8bd3d59aee52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#scan_nos = [1531, 1532, 1533, 1534] # 70 pA\n",
    "scan_nos = [1543, 1544, 1545, 1546] # 560 pA\n",
    "nframes = 16512\n",
    "frame_width = 576\n",
    "baselines = [20., 20., 20., 20.]\n",
    "th_single_elec = 30\n",
    "\n",
    "# Set up the figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, scan_no in enumerate(scan_nos):\n",
    "    print(f\"Processing scan {scan_no}...\")\n",
    "    file_path = f\"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan000000{scan_no}.h5\"\n",
    "    with h5py.File(file_path, 'r') as f0:\n",
    "        data = f0['frames']\n",
    "    \n",
    "        # Subtract the baseline and threshold.\n",
    "        sub_frames = (data[0:nframes, :, :] - baselines[i])\n",
    "        sub_frames[sub_frames < th_single_elec] = 0\n",
    "        \n",
    "        # Compute the sum of all frames.\n",
    "        avg_frame = np.average(sub_frames,axis=0)\n",
    "        \n",
    "        # Plotting on the ith subplot\n",
    "        imax = axs[i].imshow(avg_frame, vmin=avg_frame.min(), vmax=avg_frame.max(), cmap='gray')\n",
    "        fig.colorbar(imax, ax=axs[i])\n",
    "        axs[i].set_title(f'Scan No: {scan_no}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c033aa-ec8a-420d-8150-a3edb9139506",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b76fd7-03c0-4668-8aa0-4e920d3d5314",
   "metadata": {},
   "source": [
    "# Analysis of counted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b47f7-770f-4888-ab44-fa8d181fcfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /global/cfs/cdirs/ncemhub/distiller/counted/2023.09.18/\n",
    "# 1547 is first logo scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dae8e4-7fdf-4c9d-9d42-52c86fbec084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert linear indices to 2D (row, col) indices\n",
    "def linear_to_2d_indices(linear_indices, cols):\n",
    "    rows = linear_indices // cols\n",
    "    cols = linear_indices % cols\n",
    "    return rows, cols\n",
    "\n",
    "# Function to create a 2D image from the counted frame data\n",
    "def create_counted_image(frame_data, frame_height, frame_width):\n",
    "    # Initialize a 2D array with zeros\n",
    "    image = np.zeros((frame_height, frame_width), dtype=np.int32)\n",
    "\n",
    "    # Convert linear indices to 2D indices\n",
    "    rows, cols = linear_to_2d_indices(frame_data, frame_width)\n",
    "\n",
    "    # Set the hit pixels\n",
    "    image[rows, cols] = 1\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e6a2b-26c6-409d-847a-52f32f3731c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_groups = [[1531, 1532, 1533, 1534],  # 70 pA, 300 kV, 30 mrad\n",
    "               [1535, 1536, 1537, 1538],  # 140 pA\n",
    "               [1539, 1540, 1541, 1542],  # 280 pA\n",
    "               [1543, 1544, 1545, 1546]]  # 560 pA\n",
    "clength = [68, 54, 43, 34]  # in mm\n",
    "idbase = [12284, 12288, 12292, 12296]\n",
    "nframes = 1\n",
    "frame_width = 576\n",
    "\n",
    "# Set up the figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "counted_electrons = []\n",
    "igroup = 3\n",
    "for i, scan_no in enumerate(scan_groups[igroup]):\n",
    "    print(f\"Processing scan {scan_no}...\")\n",
    "    file_path = f\"/global/cfs/cdirs/ncemhub/distiller/counted/2023.09.18/data_scan{scan_no}_id{idbase[igroup]+i}_electrons.h5\"\n",
    "    with h5py.File(file_path, 'r') as f0:\n",
    "\n",
    "        summed_counted_image = np.zeros((frame_width, frame_width), dtype=np.int32)\n",
    "        nelec = 0\n",
    "        for iframe in range(nframes):\n",
    "            #if(iframe % int(nframes/10) == 0):\n",
    "            #    print(f\"Processing frame {iframe}/{nframes}\")\n",
    "            frame_data = np.array(f0['electron_events']['frames'][iframe])\n",
    "\n",
    "            # Create a 2D image of the counted frame\n",
    "            counted_image = create_counted_image(frame_data, frame_width, frame_width)\n",
    "\n",
    "            # Sum the counted images\n",
    "            summed_counted_image += counted_image\n",
    "            nelec += len(frame_data)\n",
    "            \n",
    "        # Sum all the counts.\n",
    "        print(f\"-- Total counts: {np.sum(summed_counted_image)}\")\n",
    "        print(f\"-- Total nelec:  {nelec}\")\n",
    "        counted_electrons.append(np.sum(summed_counted_image))\n",
    "\n",
    "        # Plot the summed, counted image\n",
    "        imax = axs[i].imshow(summed_counted_image, vmin=summed_counted_image.min(), vmax=summed_counted_image.max(), cmap='grey')\n",
    "        #imax = axs[i].imshow(summed_counted_image[200:250,200:250], vmin=summed_counted_image.min(), vmax=summed_counted_image.max(), cmap='grey')\n",
    "        fig.colorbar(imax, ax=axs[i])\n",
    "        axs[i].set_title(f'Scan No: {scan_no}')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Figure for the sum vs. camera length plot\n",
    "plt.plot(clength, counted_electrons, marker='o', linestyle='-')\n",
    "plt.xlabel('Camera length (mm)')\n",
    "plt.ylabel('Sum of all counted electrons')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf5e84-6bf8-4162-a2dc-ac5b0f25eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e6f6f70-60d1-44e4-a3ea-9c230eee3572",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c681ec3-79d9-47e0-8351-102e0a98ddea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc8c1a-fed9-4e2d-8e87-d7b60d33d0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8cbf907-82f3-4f9f-82c4-8350e97ee874",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb4559-22e7-4b2f-a05a-a448c6dc7551",
   "metadata": {},
   "source": [
    "# Analysis of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a893da-1103-4e9e-8cc6-d00088e97697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None:  # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacbe47-5274-4e7b-9de8-4de3951292d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scan_nos = [1531, 1532, 1533, 1534]\n",
    "clength = [60, 54, 43, 34]  # in mm\n",
    "center_pixels = [(280,280), (270,275), (280,288), (271,273)]\n",
    "radii = [280, 233, 185, 149]\n",
    "nframes = 16512\n",
    "frame_width = 576\n",
    "#baselines = [20., 20., 20., 20.]\n",
    "baselines = [20.52008230983527, 20.533552410368216, 20.529735138081396, 20.43255047238372]\n",
    "sums, means, medians, sigmas = [], [], [], []\n",
    "\n",
    "# Set up the figure\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, scan_no in enumerate(scan_nos):\n",
    "    print(f\"Processing scan {scan_no}...\")\n",
    "    file_path = f\"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan000000{scan_no}.h5\"\n",
    "    with h5py.File(file_path, 'r') as f0:\n",
    "        data = f0['frames']\n",
    "    \n",
    "        # Subtract the baseline and threshold.\n",
    "        sub_frames = (data[0:nframes, :, :] - baselines[i])\n",
    "        #sub_frames[sub_frames < 30] = 0\n",
    "        \n",
    "        # Compute the sum of all frames.\n",
    "        sum_frame = sub_frames.sum(axis=0)\n",
    "        \n",
    "        # Compute the baseline using the sum of all frames.\n",
    "        new_baseline = sum_frame[-100:,-100:].sum()/(100**2*nframes)\n",
    "        print(f\"-- new computed baseline = {new_baseline}\")\n",
    "\n",
    "        # Create the circular mask\n",
    "        mask = create_circular_mask(frame_width, frame_width, center=center_pixels[i], radius=radii[i])\n",
    "        \n",
    "        # Apply the mask and sum the pixel values\n",
    "        masked_sum = sum_frame * mask\n",
    "        circle_sum = masked_sum.sum()\n",
    "        sums.append(circle_sum)  # Store the sum of the circular area\n",
    "        #sums.append(sum_frame.sum())  # Store the sum of all pixels\n",
    "        \n",
    "        # Apply the mask to each frame and compute individual sums.\n",
    "        masked_individual_frames = sub_frames*mask\n",
    "        sum_individual_frames = masked_individual_frames.sum(axis=(1,2))\n",
    "        means.append(np.mean(sum_individual_frames))\n",
    "        medians.append(np.median(sum_individual_frames))\n",
    "        sigmas.append(np.std(sum_individual_frames))\n",
    "\n",
    "        # Plotting on the ith subplot\n",
    "        #imax = axs[i].imshow(sum_frame, vmin=sum_frame.min(), vmax=sum_frame.max())\n",
    "        imax = axs[i].imshow(masked_sum, vmin=sum_frame.min(), vmax=sum_frame.max())\n",
    "        fig.colorbar(imax, ax=axs[i])\n",
    "        axs[i].set_title(f'Scan No: {scan_no}')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Figure for the sum vs. camera length plot\n",
    "plt.plot(radii, sums, marker='o', linestyle='-')\n",
    "#plt.xlabel('Camera Length (mm)')\n",
    "plt.xlabel('Radius of integration area (pixels)')\n",
    "plt.ylabel('Sum of All Pixels')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # Figure for the means\n",
    "#plt.errorbar(radii, means, yerr=np.sqrt(means), marker='o', linestyle='-')\n",
    "plt.plot(radii, means, marker='o', linestyle='-', color='blue', label='Mean')\n",
    "plt.plot(radii, medians, marker='o', linestyle='-', color='green', label='Median')\n",
    "plt.legend()\n",
    "#plt.xlabel('Camera Length (mm)')\n",
    "plt.xlabel('Radius of integration area (pixels)')\n",
    "plt.ylabel('Mean/median pixel sum in each frame')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb147fa7-879e-4ff3-9bf3-b2d28ad86a1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0142e04-f0ec-4e8e-aee0-7901031aae57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc12d15-db30-4cd4-b3e4-f5b554f971a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"prior\"\n",
    "with h5py.File(dataset_path, 'r') as f0:\n",
    "    data = f0['frames']\n",
    "    \n",
    "    # Get all frames and subtract the baseline.\n",
    "    prior_bls = np.array(data[:,:,:],dtype=np.float32) - baseline\n",
    "    \n",
    "    # Compute the summed frame.\n",
    "    prior_frame = np.sum(prior_bls,axis=0)\n",
    "    \n",
    "    # Divide by the average electron amplitude.\n",
    "    prior_frame /= A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711498c-64d3-4f34-bee9-a518665f90b3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc602f3a-54ec-4bdf-9bc4-7880c449c3a2",
   "metadata": {},
   "source": [
    "### Test one scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c359a3-8fa9-489f-9be0-c6ff504e641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_no = 1533\n",
    "nframes = 16512\n",
    "file_path = f\"/global/cfs/cdirs/ncemhub/distiller/raw/2023.09.18/data_scan000000{scan_no}.h5\"\n",
    "f0 = h5py.File(file_path, 'r')\n",
    "data = f0['frames']\n",
    "\n",
    "initial_data = data[0:1000, :, :].sum(axis=0)\n",
    "fg1, axes = plt.subplots(2, 2)\n",
    "imax1 = axes[0,0].imshow(initial_data, vmin=initial_data[0:288,:].min(), vmax=initial_data[50:-50, 50:-50].max())\n",
    "plt.colorbar(imax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374a955-fe24-476a-865d-d3f16e3e02f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
